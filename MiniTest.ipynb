{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import time\n",
      "import io\n",
      "import sys\n",
      "import os\n",
      "import random\n",
      "import re\n",
      "import json\n",
      "import shutil\n",
      "from datetime import datetime\n",
      "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
      "from keras.layers.embeddings import Embedding\n",
      "from keras.layers.recurrent import LSTM\n",
      "from keras.layers.core import Dense, Dropout\n",
      "from keras.layers import Input\n",
      "from keras.models import Model, load_model\n",
      "import keras.regularizers\n",
      "import numpy as np\n",
      "\n",
      "from .utils import tobits, sobie_scores_to_char_ents, get_file\n",
      "from .corpusreader import CorpusReader\n",
      "\n",
      "charstr = \"abcdefghijklmonpqrstuvwxyzABCDEFGHIJKLMNOPQRTSUVWXYZ0123456789.,-[](){};:'\\\"^$%=/\\\\<>@_*+?! \"\n",
      "chard = {charstr[i]:i+1 for i in range(len(charstr))}\n",
      "charn = len(charstr) + 1\n",
      "\n",
      "defaultmodel = None\n",
      "\n",
      "def get_mini_model():\n",
      "    \"\"\"\n",
      "    Gets the default pre-trained minimalist model, loading if necessary.\n",
      "    \n",
      "    Returns:\n",
      "        An MiniModel\n",
      "    \"\"\"\n",
      "    global defaultmodel\n",
      "    if defaultmodel is not None: return defaultmodel\n",
      "    mm = MiniModel()\n",
      "    f = get_file(\"default_minimodel_0.0.1.h5\")\n",
      "    mm.load(f)\n",
      "    defaultmodel = mm\n",
      "    return defaultmodel\n",
      "\n",
      "def _char_to_num(c):\n",
      "    if c in chard: return chard[c]\n",
      "    return 0\n",
      "\n",
      "class MiniModel(object):\n",
      "    \"\"\"\n",
      "    A \"minimalist\" model for chemical named entity recognition - works character-by-character, does not use\n",
      "    rich features, does use multiple bidirectional LSTM layers.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        \"\"\"\n",
      "        Empty constructor - use train or load to populate this.\n",
      "        \"\"\"\n",
      "        pass\n",
      "        \n",
      "    def train(self, textfile, annotfile, runname):\n",
      "        \"\"\"\n",
      "        Train a new MiniModel.\n",
      "                \n",
      "        This produces one important file:\n",
      "        \n",
      "        minimodel_$RUNNAME.h5 - the keras model itself\n",
      "        \n",
      "        These consititute the trained model.\n",
      "        \n",
      "        It also produces several files named:\n",
      "        \n",
      "        epoch_$EPOCHNUM_$RUNAME.h5\n",
      "        \n",
      "        These are the keras models for each epoch (the auxilliary information doesn't change).\n",
      "        \n",
      "        Args:\n",
      "            textfile: the filename of the file containing the BioCreative training text - e.g. \"BioCreative V.5 training set.txt\"\n",
      "            annotfile: the filename of the containing the BioCreative training annotations - e.g. \"CEMP_BioCreative V.5 training set annot.tsv\"\n",
      "            runname: a string, part of the output filenames.\n",
      "        \"\"\"    \n",
      "        # Get training and test sequences\n",
      "        cr = CorpusReader(textfile, annotfile, charbychar=True)\n",
      "        train = cr.trainseqs\n",
      "        test = cr.testseqs\n",
      "\n",
      "        seqs = train+test\n",
      "        # \"wordn\" should be \"charn\" but we're using names lifted from the tradmodel.\n",
      "        # Anyway, characters to integers to use with embeddings\n",
      "        for seq in seqs:\n",
      "            seq[\"wordn\"] = [_char_to_num(i) for i in seq[\"tokens\"]]\n",
      "        \n",
      "        self.lablist = ['S-E', 'O', 'I-E', 'E-E', 'B-E']\n",
      "        self.labdict = {'O': 1, 'E-E': 3, 'B-E': 4, 'S-E': 0, 'I-E': 2}\n",
      "        \n",
      "        # convert SOBIE tags to numbers\n",
      "        for seq in seqs:\n",
      "            seq[\"bion\"] = [self.labdict[i] for i in seq[\"bio\"]]\n",
      "\n",
      "        # Gather together sequences by length\n",
      "        print(\"Make train dict at\", datetime.now(), file=sys.stderr)\n",
      "    \n",
      "        train_l_d = {}\n",
      "        for seq in train:\n",
      "            l = len(seq[\"tokens\"])\n",
      "            if l not in train_l_d: train_l_d[l] = []\n",
      "            train_l_d[l].append(seq)\n",
      "        sizes = list(train_l_d.keys())\n",
      "    \n",
      "        # Set up the keras model\n",
      "        il = Input(shape=(None, ), dtype='int32')\n",
      "        el = Embedding(charn, 200, name=\"embed\")(il)\n",
      "        bl1 = Bidirectional(LSTM(128, return_sequences=True, recurrent_dropout=0.5, dropout=0.5), merge_mode=\"concat\", name=\"lstm1\")(el)\n",
      "        bl2 = Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.5, dropout=0.5), merge_mode=\"concat\", name=\"lstm2\")(bl1)\n",
      "        bl3 = Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.5, dropout=0.5), merge_mode=\"concat\", name=\"lstm3\")(bl2)\n",
      "        dl = TimeDistributed(Dense(len(self.lablist), activation=\"softmax\"), name=\"output\")(bl3)\n",
      "        model = Model(inputs=il, outputs=dl)\n",
      "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
      "        self.model = model\n",
      "        \n",
      "        best_epoch = -1\n",
      "        best_f = 0.0\n",
      "    \n",
      "        # OK, start actually training\n",
      "        for epoch in range(30):\n",
      "            print(\"Epoch\", epoch, \"start at\", datetime.now(), file=sys.stderr)\n",
      "            # Train in batches of different sizes - randomize the order of sizes\n",
      "            # Except for the first few epochs - train on the smallest examples first\n",
      "            if epoch > 4: random.shuffle(sizes) # For unknown reasons we can't train on a single token (i.e. character)\n",
      "            for size in sizes:\n",
      "                if size == 1: continue\n",
      "                batch = train_l_d[size]\n",
      "                tx = np.array([seq[\"wordn\"] for seq in batch])\n",
      "                ty = np.array([[tobits(i, len(self.lablist)) for i in seq[\"bion\"]] for seq in batch])\n",
      "                # This trains in mini-batches\n",
      "                model.fit(tx, ty, verbose=0, epochs=1)\n",
      "            print(\"Trained at\", datetime.now(), file=sys.stderr)\n",
      "            model.save(\"epoch_%s_%s.h5\" % (epoch, runname))\n",
      "            # Evaluate\n",
      "            tp_all = 0\n",
      "            fp_all = 0\n",
      "            fn_all = 0\n",
      "            for i in range(len(test)):\n",
      "                \n",
      "                enttype = None\n",
      "                entstart = 0\n",
      "                ts = test[i]\n",
      "                ents = [(\"E\", i[2], i[3]) for i in ts[\"ents\"]]\n",
      "                mm = model.predict([np.array([ts[\"wordn\"]])])[0]\n",
      "                \n",
      "                pseq = {}\n",
      "                pseq[\"tokens\"] = ts[\"tokens\"]\n",
      "                pseq[\"tokstart\"] = ts[\"tokstart\"]\n",
      "                pseq[\"tokend\"] = ts[\"tokend\"]\n",
      "                pseq[\"tagfeat\"] = mm\n",
      "                \n",
      "                pents, pxe = sobie_scores_to_char_ents(pseq, 0.5, ts[\"ss\"])\n",
      "                \n",
      "                tp = 0\n",
      "                fp = 0\n",
      "                fn = 0\n",
      "                tofind = set(ents)\n",
      "                for ent in pents:\n",
      "                    if ent in tofind:\n",
      "                        tp += 1\n",
      "                    else:\n",
      "                        fp += 1\n",
      "                fn = len(tofind) - tp\n",
      "                tp_all += tp\n",
      "                fp_all += fp\n",
      "                fn_all += fn\n",
      "            f = (2*tp_all/(tp_all+tp_all+fp_all+fn_all))\n",
      "            print(\"TP\", tp_all, \"FP\", fp_all, \"FN\", fn_all, \"F\", f, \"Precision\", tp_all/(tp_all+fp_all), \"Recall\", tp_all/(tp_all+fn_all), file=sys.stderr)\n",
      "            if f > best_f:\n",
      "                print(\"Best so far\", file=sys.stderr)\n",
      "                best_f = f\n",
      "                best_epoch = epoch\n",
      "\n",
      "        # Pick the best model, and save it with a useful name        \n",
      "        if best_epoch > -1:\n",
      "            shutil.copyfile(\"epoch_%s_%s.h5\" % (best_epoch, runname), \"minimodel_%s.h5\" % runname)\n",
      "\n",
      "    def load(self, mfile):\n",
      "        \"\"\"\n",
      "        Load in model data.\n",
      "        \n",
      "        Args:\n",
      "            mfile: the filename of the .h5 file\n",
      "        \"\"\"    \n",
      "        self.lablist = ['S-E', 'O', 'I-E', 'E-E', 'B-E']\n",
      "        self.labdict = {'O': 1, 'E-E': 3, 'B-E': 4, 'S-E': 0, 'I-E': 2}\n",
      "        self.model = load_model(mfile)\n",
      "        print(\"Minimalist Model read at\", datetime.now(), file=sys.stderr)\n",
      "        \n",
      "    def process(self, str, threshold=0.5, domonly=True):\n",
      "        \"\"\"\n",
      "        Find named entities in a string.\n",
      "        \n",
      "        Entities are returned as tuples:\n",
      "        (start_charater_position, end_character_position, string, score, is_dominant)\n",
      "        \n",
      "        Entities are dominant if they are not partially or wholly overlapping with a higher-scoring entity.\n",
      "        \n",
      "        Args:\n",
      "            str: the string to find entities in.\n",
      "            threshold: the minimum score for entities.\n",
      "            domonly: if True, discard non-dominant entities.\n",
      "        \"\"\"\n",
      "        results = []\n",
      "        if len(str) == 0: return results\n",
      "        seq = {}\n",
      "        seq[\"tokens\"] = list(str)\n",
      "        seq[\"ss\"] = str\n",
      "        seq[\"tokstart\"] = [i for i in range(len(str))]\n",
      "        seq[\"tokend\"] = [i+1 for i in range(len(str))]\n",
      "        seq[\"wordn\"] = [_char_to_num(i) for i in seq[\"tokens\"]]\n",
      "        mm = self.model.predict([np.array([seq[\"wordn\"]])])[0]\n",
      "        seq[\"tagfeat\"] = mm\n",
      "        pents, pxe = sobie_scores_to_char_ents(seq, threshold, str)\n",
      "        if domonly:\n",
      "            pents = [i for i in pents if pxe[i][\"dom\"]]\n",
      "        for ent in pents:\n",
      "            results.append((ent[1], ent[2], str[ent[1]:ent[2]], pxe[ent][\"score\"], pxe[ent][\"dom\"]))\n",
      "        return results\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"chemlistem/minimodel.py\").read()\n",
    "ff = f.replace(\"\\t\", \"    \")\n",
    "print(ff)\n",
    "outf = open(\"chemlistem/neominimodel.py\", \"w\")\n",
    "outf.write(ff)\n",
    "outf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-10 15:27:13.708918\n",
      "Shufflesplit at 2018-05-10 15:27:13.709078\n",
      "Read annots at 2018-05-10 15:27:13.782480\n",
      "Read train seqs at 2018-05-10 15:27:14.129328\n",
      "Read test seqs at 2018-05-10 15:28:04.512449\n",
      "Corpus read at 2018-05-10 15:28:19.342064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make train dict at 2018-05-10 15:28:24.065793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-10 15:28:32.477172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 2018-05-10 15:28:32.477391\n",
      "392 2018-05-10 15:28:37.685004\n",
      "77 2018-05-10 15:28:38.202461\n",
      "431 2018-05-10 15:28:38.967881\n",
      "61 2018-05-10 15:28:39.544226\n",
      "464 2018-05-10 15:28:40.124108\n",
      "178 2018-05-10 15:28:40.661139\n",
      "288 2018-05-10 15:28:41.124827\n",
      "155 2018-05-10 15:28:41.489481\n",
      "255 2018-05-10 15:28:41.904567\n",
      "91 2018-05-10 15:28:42.238675\n",
      "249 2018-05-10 15:28:42.910632\n",
      "59 2018-05-10 15:28:43.238345\n",
      "183 2018-05-10 15:28:43.992816\n",
      "28 2018-05-10 15:28:44.245052\n",
      "820 2018-05-10 15:28:44.464227\n",
      "103 2018-05-10 15:28:45.244770\n",
      "582 2018-05-10 15:28:45.741785\n",
      "66 2018-05-10 15:28:46.270688\n",
      "884 2018-05-10 15:28:47.904246\n",
      "60 2018-05-10 15:28:48.574194\n",
      "792 2018-05-10 15:28:49.088745\n",
      "69 2018-05-10 15:28:49.692028\n",
      "719 2018-05-10 15:28:50.352161\n",
      "75 2018-05-10 15:28:50.928907\n",
      "755 2018-05-10 15:28:51.550484\n",
      "32 2018-05-10 15:28:52.146697\n",
      "362 2018-05-10 15:28:52.332993\n",
      "160 2018-05-10 15:28:52.669392\n",
      "109 2018-05-10 15:28:52.997028\n",
      "65 2018-05-10 15:28:53.358455\n",
      "896 2018-05-10 15:28:53.985728\n",
      "45 2018-05-10 15:28:54.656334\n",
      "616 2018-05-10 15:28:54.955296\n",
      "99 2018-05-10 15:28:55.462157\n",
      "646 2018-05-10 15:28:55.905356\n",
      "81 2018-05-10 15:28:56.426026\n",
      "675 2018-05-10 15:28:56.985794\n",
      "37 2018-05-10 15:28:57.530210\n",
      "434 2018-05-10 15:28:57.789428\n",
      "51 2018-05-10 15:28:58.176468\n",
      "880 2018-05-10 15:28:58.633076\n",
      "191 2018-05-10 15:28:59.292221\n",
      "376 2018-05-10 15:28:59.493775\n",
      "856 2018-05-10 15:28:59.851372\n",
      "101 2018-05-10 15:29:00.496460\n",
      "533 2018-05-10 15:29:00.842344\n",
      "85 2018-05-10 15:29:01.304741\n",
      "661 2018-05-10 15:29:02.828227\n",
      "78 2018-05-10 15:29:03.362372\n",
      "297 2018-05-10 15:29:04.000076\n",
      "102 2018-05-10 15:29:04.565503\n",
      "959 2018-05-10 15:29:05.022603\n",
      "46 2018-05-10 15:29:05.688268\n",
      "322 2018-05-10 15:29:06.049573\n",
      "154 2018-05-10 15:29:06.364681\n",
      "168 2018-05-10 15:29:06.528884\n",
      "134 2018-05-10 15:29:06.702558\n",
      "338 2018-05-10 15:29:06.990053\n",
      "74 2018-05-10 15:29:07.307756\n",
      "637 2018-05-10 15:29:07.919747\n",
      "64 2018-05-10 15:29:08.435884\n",
      "813 2018-05-10 15:29:09.053937\n",
      "42 2018-05-10 15:29:09.664464\n",
      "660 2018-05-10 15:29:10.001651\n",
      "33 2018-05-10 15:29:10.537730\n",
      "738 2018-05-10 15:29:10.774225\n",
      "30 2018-05-10 15:29:11.356843\n",
      "841 2018-05-10 15:29:11.533721\n",
      "743 2018-05-10 15:29:12.160023\n",
      "787 2018-05-10 15:29:12.725509\n",
      "208 2018-05-10 15:29:13.316195\n",
      "717 2018-05-10 15:29:13.519799\n",
      "501 2018-05-10 15:29:14.085464\n",
      "142 2018-05-10 15:29:14.524230\n",
      "413 2018-05-10 15:29:14.823512\n",
      "144 2018-05-10 15:29:15.208416\n",
      "82 2018-05-10 15:29:15.511228\n",
      "259 2018-05-10 15:29:16.076642\n",
      "135 2018-05-10 15:29:16.335019\n",
      "23 2018-05-10 15:29:16.618729\n",
      "443 2018-05-10 15:29:16.730251\n",
      "35 2018-05-10 15:29:18.163980\n",
      "433 2018-05-10 15:29:18.424671\n",
      "79 2018-05-10 15:29:18.845195\n",
      "508 2018-05-10 15:29:19.430195\n",
      "67 2018-05-10 15:29:19.904150\n",
      "1142 2018-05-10 15:29:20.583395\n",
      "98 2018-05-10 15:29:21.362159\n",
      "412 2018-05-10 15:29:21.807580\n",
      "47 2018-05-10 15:29:22.192060\n",
      "381 2018-05-10 15:29:22.618122\n",
      "108 2018-05-10 15:29:22.987178\n",
      "318 2018-05-10 15:29:23.351651\n",
      "34 2018-05-10 15:29:23.660032\n",
      "730 2018-05-10 15:29:23.907079\n",
      "159 2018-05-10 15:29:24.501692\n",
      "84 2018-05-10 15:29:24.672541\n",
      "883 2018-05-10 15:29:25.254128\n",
      "881 2018-05-10 15:29:25.902078\n",
      "107 2018-05-10 15:29:26.555093\n",
      "298 2018-05-10 15:29:26.909568\n",
      "38 2018-05-10 15:29:27.479034\n",
      "940 2018-05-10 15:29:27.740177\n",
      "39 2018-05-10 15:29:28.423896\n",
      "330 2018-05-10 15:29:28.692444\n",
      "632 2018-05-10 15:29:29.306487\n",
      "73 2018-05-10 15:29:29.817951\n",
      "127 2018-05-10 15:29:30.425764\n",
      "274 2018-05-10 15:29:30.704223\n",
      "25 2018-05-10 15:29:31.227670\n",
      "394 2018-05-10 15:29:31.385615\n",
      "817 2018-05-10 15:29:31.754940\n",
      "89 2018-05-10 15:29:32.362436\n",
      "257 2018-05-10 15:29:32.879220\n",
      "54 2018-05-10 15:29:33.140531\n",
      "331 2018-05-10 15:29:33.618567\n",
      "150 2018-05-10 15:29:33.935744\n",
      "1256 2018-05-10 15:29:34.095254\n",
      "48 2018-05-10 15:29:34.932384\n",
      "837 2018-05-10 15:29:35.414862\n",
      "909 2018-05-10 15:29:36.039693\n",
      "388 2018-05-10 15:29:37.753864\n",
      "365 2018-05-10 15:29:38.117779\n",
      "41 2018-05-10 15:29:38.474877\n",
      "371 2018-05-10 15:29:38.752584\n",
      "26 2018-05-10 15:29:39.444531\n",
      "536 2018-05-10 15:29:39.568185\n",
      "72 2018-05-10 15:29:40.031404\n",
      "736 2018-05-10 15:29:40.632509\n",
      "15 2018-05-10 15:29:41.216190\n",
      "63 2018-05-10 15:29:41.247646\n",
      "169 2018-05-10 15:29:41.864188\n",
      "575 2018-05-10 15:29:42.040748\n",
      "27 2018-05-10 15:29:42.526771\n",
      "94 2018-05-10 15:29:42.692677\n",
      "704 2018-05-10 15:29:43.124449\n",
      "293 2018-05-10 15:29:43.688423\n",
      "176 2018-05-10 15:29:43.981569\n",
      "545 2018-05-10 15:29:44.170654\n",
      "990 2018-05-10 15:29:44.645086\n",
      "780 2018-05-10 15:29:45.338304\n",
      "88 2018-05-10 15:29:45.935270\n",
      "370 2018-05-10 15:29:46.439111\n",
      "70 2018-05-10 15:29:46.784155\n",
      "956 2018-05-10 15:29:47.370938\n",
      "156 2018-05-10 15:29:48.070166\n",
      "18 2018-05-10 15:29:48.235906\n",
      "663 2018-05-10 15:29:48.270040\n",
      "657 2018-05-10 15:29:48.815623\n",
      "95 2018-05-10 15:29:49.352577\n",
      "1067 2018-05-10 15:29:49.780532\n",
      "244 2018-05-10 15:29:50.513728\n",
      "128 2018-05-10 15:29:50.753669\n",
      "119 2018-05-10 15:29:51.031315\n",
      "1001 2018-05-10 15:29:51.382449\n",
      "143 2018-05-10 15:29:52.099738\n",
      "1279 2018-05-10 15:29:52.398692\n",
      "258 2018-05-10 15:29:53.237249\n",
      "58 2018-05-10 15:29:53.485630\n",
      "251 2018-05-10 15:29:55.041361\n",
      "397 2018-05-10 15:29:55.297004\n",
      "181 2018-05-10 15:29:55.659455\n",
      "2082 2018-05-10 15:29:55.843649\n",
      "153 2018-05-10 15:29:56.594746\n",
      "750 2018-05-10 15:29:56.864963\n",
      "116 2018-05-10 15:29:57.467298\n",
      "1243 2018-05-10 15:29:57.848579\n",
      "727 2018-05-10 15:29:58.696560\n",
      "857 2018-05-10 15:29:59.260311\n",
      "122 2018-05-10 15:29:59.883019\n",
      "1100 2018-05-10 15:30:00.149938\n",
      "306 2018-05-10 15:30:00.916605\n",
      "507 2018-05-10 15:30:01.218947\n",
      "599 2018-05-10 15:30:01.673822\n",
      "324 2018-05-10 15:30:02.177083\n",
      "328 2018-05-10 15:30:02.485064\n",
      "928 2018-05-10 15:30:02.804247\n",
      "149 2018-05-10 15:30:03.482792\n",
      "396 2018-05-10 15:30:03.793526\n",
      "266 2018-05-10 15:30:04.164435\n",
      "426 2018-05-10 15:30:04.426097\n",
      "320 2018-05-10 15:30:04.815201\n",
      "332 2018-05-10 15:30:05.118945\n",
      "1200 2018-05-10 15:30:05.446079\n",
      "916 2018-05-10 15:30:06.234529\n",
      "19 2018-05-10 15:30:06.901564\n",
      "158 2018-05-10 15:30:06.969504\n",
      "44 2018-05-10 15:30:07.296143\n",
      "526 2018-05-10 15:30:07.645116\n",
      "979 2018-05-10 15:30:08.106453\n",
      "592 2018-05-10 15:30:08.795536\n",
      "550 2018-05-10 15:30:09.296235\n",
      "492 2018-05-10 15:30:09.773450\n",
      "586 2018-05-10 15:30:10.207208\n",
      "237 2018-05-10 15:30:10.701626\n",
      "437 2018-05-10 15:30:11.170153\n",
      "196 2018-05-10 15:30:11.571947\n",
      "551 2018-05-10 15:30:11.775183\n",
      "513 2018-05-10 15:30:12.251926\n",
      "206 2018-05-10 15:30:12.713332\n",
      "624 2018-05-10 15:30:12.924458\n",
      "110 2018-05-10 15:30:13.436602\n",
      "373 2018-05-10 15:30:14.817477\n",
      "104 2018-05-10 15:30:15.184606\n",
      "1125 2018-05-10 15:30:15.648269\n",
      "291 2018-05-10 15:30:16.408283\n",
      "290 2018-05-10 15:30:16.696525\n",
      "29 2018-05-10 15:30:16.983078\n",
      "199 2018-05-10 15:30:17.158421\n",
      "890 2018-05-10 15:30:17.367399\n",
      "83 2018-05-10 15:30:17.997364\n",
      "560 2018-05-10 15:30:18.481900\n",
      "126 2018-05-10 15:30:18.952155\n",
      "115 2018-05-10 15:30:19.226066\n",
      "626 2018-05-10 15:30:19.604179\n",
      "1189 2018-05-10 15:30:20.119933\n",
      "76 2018-05-10 15:30:20.950532\n",
      "202 2018-05-10 15:30:21.502215\n",
      "90 2018-05-10 15:30:21.906806\n",
      "400 2018-05-10 15:30:22.420174\n",
      "635 2018-05-10 15:30:22.797311\n",
      "264 2018-05-10 15:30:23.315331\n",
      "165 2018-05-10 15:30:23.576265\n",
      "262 2018-05-10 15:30:23.868158\n",
      "106 2018-05-10 15:30:24.132706\n",
      "250 2018-05-10 15:30:24.564963\n",
      "55 2018-05-10 15:30:24.813428\n",
      "123 2018-05-10 15:30:25.298611\n",
      "414 2018-05-10 15:30:25.566363\n",
      "50 2018-05-10 15:30:26.321940\n",
      "1032 2018-05-10 15:30:26.772409\n",
      "231 2018-05-10 15:30:27.506742\n",
      "978 2018-05-10 15:30:27.736283\n",
      "96 2018-05-10 15:30:28.433699\n",
      "974 2018-05-10 15:30:28.871131\n",
      "272 2018-05-10 15:30:29.563489\n",
      "448 2018-05-10 15:30:30.016036\n",
      "43 2018-05-10 15:30:30.417748\n",
      "601 2018-05-10 15:30:30.746251\n",
      "399 2018-05-10 15:30:31.244074\n",
      "213 2018-05-10 15:30:31.609578\n",
      "185 2018-05-10 15:30:31.827051\n",
      "843 2018-05-10 15:30:32.016439\n",
      "112 2018-05-10 15:30:32.667940\n",
      "669 2018-05-10 15:30:33.034654\n",
      "261 2018-05-10 15:30:34.573229\n",
      "380 2018-05-10 15:30:34.831352\n",
      "71 2018-05-10 15:30:35.188282\n",
      "1508 2018-05-10 15:30:35.784020\n",
      "222 2018-05-10 15:30:36.746327\n",
      "368 2018-05-10 15:30:37.129821\n",
      "263 2018-05-10 15:30:37.480030\n",
      "1026 2018-05-10 15:30:37.743408\n",
      "219 2018-05-10 15:30:38.455076\n",
      "137 2018-05-10 15:30:38.888012\n",
      "901 2018-05-10 15:30:39.040089\n",
      "352 2018-05-10 15:30:39.684792\n",
      "458 2018-05-10 15:30:40.023845\n",
      "1219 2018-05-10 15:30:40.434395\n",
      "1300 2018-05-10 15:30:41.243983\n",
      "68 2018-05-10 15:30:42.094866\n",
      "878 2018-05-10 15:30:42.677805\n",
      "247 2018-05-10 15:30:43.327455\n",
      "895 2018-05-10 15:30:43.576234\n",
      "234 2018-05-10 15:30:44.254966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194 2018-05-10 15:30:44.492236\n",
      "515 2018-05-10 15:30:44.688436\n",
      "387 2018-05-10 15:30:45.143784\n",
      "620 2018-05-10 15:30:45.506241\n",
      "958 2018-05-10 15:30:46.014795\n",
      "31 2018-05-10 15:30:46.717588\n",
      "307 2018-05-10 15:30:46.899500\n",
      "931 2018-05-10 15:30:47.198825\n",
      "20 2018-05-10 15:30:47.888102\n",
      "995 2018-05-10 15:30:47.960032\n",
      "164 2018-05-10 15:30:48.672430\n",
      "441 2018-05-10 15:30:48.842359\n",
      "145 2018-05-10 15:30:49.252579\n",
      "432 2018-05-10 15:30:49.511057\n",
      "1207 2018-05-10 15:30:49.912291\n",
      "235 2018-05-10 15:30:50.737192\n",
      "538 2018-05-10 15:30:50.971444\n",
      "611 2018-05-10 15:30:51.426213\n",
      "148 2018-05-10 15:30:51.928691\n",
      "36 2018-05-10 15:30:52.192432\n",
      "245 2018-05-10 15:30:52.442479\n",
      "1344 2018-05-10 15:30:52.691340\n",
      "654 2018-05-10 15:30:53.561947\n",
      "1015 2018-05-10 15:30:54.074131\n",
      "21 2018-05-10 15:30:54.789753\n",
      "581 2018-05-10 15:30:54.896366\n",
      "170 2018-05-10 15:30:55.377127\n",
      "1533 2018-05-10 15:30:55.557460\n",
      "14 2018-05-10 15:30:56.165990\n",
      "238 2018-05-10 15:30:56.196219\n",
      "846 2018-05-10 15:30:56.602592\n",
      "622 2018-05-10 15:30:57.248299\n",
      "763 2018-05-10 15:30:57.764002\n",
      "1077 2018-05-10 15:30:59.366566\n",
      "524 2018-05-10 15:31:00.104878\n",
      "1221 2018-05-10 15:31:00.551912\n",
      "190 2018-05-10 15:31:01.371654\n",
      "242 2018-05-10 15:31:01.563935\n",
      "56 2018-05-10 15:31:01.813420\n",
      "583 2018-05-10 15:31:02.380681\n",
      "429 2018-05-10 15:31:02.867427\n",
      "768 2018-05-10 15:31:03.275779\n",
      "2463 2018-05-10 15:31:03.869083\n",
      "49 2018-05-10 15:31:04.731840\n",
      "339 2018-05-10 15:31:05.114842\n",
      "311 2018-05-10 15:31:05.436903\n",
      "537 2018-05-10 15:31:05.735061\n",
      "52 2018-05-10 15:31:06.183852\n",
      "798 2018-05-10 15:31:06.652013\n",
      "535 2018-05-10 15:31:07.255134\n",
      "556 2018-05-10 15:31:07.714020\n",
      "405 2018-05-10 15:31:08.177057\n",
      "590 2018-05-10 15:31:08.552239\n",
      "1135 2018-05-10 15:31:09.076690\n",
      "100 2018-05-10 15:31:09.833124\n",
      "1056 2018-05-10 15:31:10.283342\n",
      "576 2018-05-10 15:31:11.043662\n",
      "270 2018-05-10 15:31:11.531509\n",
      "1045 2018-05-10 15:31:12.050964\n",
      "607 2018-05-10 15:31:12.773474\n",
      "772 2018-05-10 15:31:13.269249\n",
      "706 2018-05-10 15:31:13.859600\n",
      "1155 2018-05-10 15:31:14.414737\n",
      "80 2018-05-10 15:31:15.188084\n",
      "315 2018-05-10 15:31:16.755512\n",
      "791 2018-05-10 15:31:17.050336\n",
      "490 2018-05-10 15:31:17.663497\n",
      "563 2018-05-10 15:31:18.095277\n",
      "1085 2018-05-10 15:31:18.568836\n",
      "226 2018-05-10 15:31:19.317518\n",
      "552 2018-05-10 15:31:19.546658\n",
      "22 2018-05-10 15:31:20.016042\n",
      "867 2018-05-10 15:31:20.124209\n",
      "92 2018-05-10 15:31:20.757198\n",
      "1381 2018-05-10 15:31:21.178688\n",
      "62 2018-05-10 15:31:22.069746\n",
      "336 2018-05-10 15:31:22.604867\n",
      "505 2018-05-10 15:31:22.926614\n",
      "345 2018-05-10 15:31:23.363612\n",
      "411 2018-05-10 15:31:23.686086\n",
      "375 2018-05-10 15:31:24.064395\n",
      "341 2018-05-10 15:31:24.412641\n",
      "125 2018-05-10 15:31:24.738125\n",
      "893 2018-05-10 15:31:25.009428\n",
      "1393 2018-05-10 15:31:25.663267\n",
      "737 2018-05-10 15:31:26.551730\n",
      "121 2018-05-10 15:31:27.118834\n",
      "391 2018-05-10 15:31:27.386336\n",
      "571 2018-05-10 15:31:27.760867\n",
      "147 2018-05-10 15:31:28.246411\n",
      "830 2018-05-10 15:31:28.557223\n",
      "481 2018-05-10 15:31:29.170367\n",
      "462 2018-05-10 15:31:29.595698\n",
      "409 2018-05-10 15:31:30.014683\n",
      "598 2018-05-10 15:31:30.402863\n",
      "1029 2018-05-10 15:31:30.908526\n",
      "200 2018-05-10 15:31:31.613504\n",
      "1133 2018-05-10 15:31:31.822091\n",
      "377 2018-05-10 15:31:32.602768\n",
      "403 2018-05-10 15:31:32.964616\n",
      "239 2018-05-10 15:31:34.339268\n",
      "1604 2018-05-10 15:31:34.745354\n",
      "17 2018-05-10 15:31:35.370717\n",
      "541 2018-05-10 15:31:35.434812\n",
      "584 2018-05-10 15:31:35.907214\n",
      "114 2018-05-10 15:31:36.394825\n",
      "1236 2018-05-10 15:31:36.769774\n",
      "167 2018-05-10 15:31:37.596435\n",
      "765 2018-05-10 15:31:37.937282\n",
      "406 2018-05-10 15:31:38.531356\n",
      "105 2018-05-10 15:31:38.908229\n",
      "570 2018-05-10 15:31:39.262715\n",
      "227 2018-05-10 15:31:39.739663\n",
      "229 2018-05-10 15:31:40.184370\n",
      "649 2018-05-10 15:31:40.410955\n",
      "579 2018-05-10 15:31:40.929955\n",
      "848 2018-05-10 15:31:41.419023\n",
      "1404 2018-05-10 15:31:42.045181\n",
      "553 2018-05-10 15:31:42.607353\n",
      "617 2018-05-10 15:31:43.074855\n",
      "12 2018-05-10 15:31:43.582840\n",
      "483 2018-05-10 15:31:43.610953\n",
      "310 2018-05-10 15:31:44.035849\n",
      "302 2018-05-10 15:31:44.327478\n",
      "442 2018-05-10 15:31:44.897136\n",
      "589 2018-05-10 15:31:45.293847\n",
      "847 2018-05-10 15:31:45.790391\n",
      "87 2018-05-10 15:31:46.422491\n",
      "446 2018-05-10 15:31:46.919840\n",
      "682 2018-05-10 15:31:47.329213\n",
      "111 2018-05-10 15:31:47.874639\n",
      "680 2018-05-10 15:31:48.245964\n",
      "321 2018-05-10 15:31:48.800038\n",
      "1119 2018-05-10 15:31:49.113129\n",
      "401 2018-05-10 15:31:49.876056\n",
      "1037 2018-05-10 15:31:51.269937\n",
      "886 2018-05-10 15:31:52.004476\n",
      "284 2018-05-10 15:31:52.654981\n",
      "421 2018-05-10 15:31:52.931128\n",
      "93 2018-05-10 15:31:53.320429\n",
      "947 2018-05-10 15:31:53.852954\n",
      "57 2018-05-10 15:31:54.550833\n",
      "487 2018-05-10 15:31:55.049818\n",
      "301 2018-05-10 15:31:55.473748\n",
      "366 2018-05-10 15:31:56.042385\n",
      "491 2018-05-10 15:31:56.389443\n",
      "363 2018-05-10 15:31:56.818574\n",
      "517 2018-05-10 15:31:57.161026\n",
      "578 2018-05-10 15:31:57.612201\n",
      "436 2018-05-10 15:31:58.091013\n",
      "151 2018-05-10 15:31:58.483524\n",
      "152 2018-05-10 15:31:58.797094\n",
      "1109 2018-05-10 15:31:59.111455\n",
      "1073 2018-05-10 15:31:59.887988\n",
      "24 2018-05-10 15:32:00.615164\n",
      "243 2018-05-10 15:32:00.733176\n",
      "933 2018-05-10 15:32:00.975007\n",
      "971 2018-05-10 15:32:01.650434\n",
      "952 2018-05-10 15:32:02.338700\n",
      "1062 2018-05-10 15:32:03.027581\n",
      "1360 2018-05-10 15:32:03.770168\n",
      "699 2018-05-10 15:32:04.643807\n",
      "802 2018-05-10 15:32:05.195357\n",
      "120 2018-05-10 15:32:05.799304\n",
      "503 2018-05-10 15:32:06.065792\n",
      "430 2018-05-10 15:32:06.515125\n",
      "410 2018-05-10 15:32:06.909668\n",
      "996 2018-05-10 15:32:07.301172\n",
      "851 2018-05-10 15:32:08.017577\n",
      "1268 2018-05-10 15:32:08.640743\n",
      "698 2018-05-10 15:32:09.476149\n",
      "335 2018-05-10 15:32:10.033130\n",
      "350 2018-05-10 15:32:10.354462\n",
      "374 2018-05-10 15:32:10.682006\n",
      "831 2018-05-10 15:32:11.034962\n",
      "498 2018-05-10 15:32:11.655359\n",
      "1201 2018-05-10 15:32:12.091404\n",
      "456 2018-05-10 15:32:12.882876\n",
      "385 2018-05-10 15:32:14.282484\n",
      "659 2018-05-10 15:32:14.654685\n",
      "539 2018-05-10 15:32:15.179432\n",
      "519 2018-05-10 15:32:15.641647\n",
      "113 2018-05-10 15:32:16.096421\n",
      "450 2018-05-10 15:32:16.470945\n",
      "514 2018-05-10 15:32:16.883256\n",
      "1729 2018-05-10 15:32:17.336049\n",
      "862 2018-05-10 15:32:18.408862\n",
      "86 2018-05-10 15:32:19.052502\n",
      "1288 2018-05-10 15:32:19.546327\n",
      "233 2018-05-10 15:32:20.382646\n",
      "423 2018-05-10 15:32:20.618071\n",
      "192 2018-05-10 15:32:21.001417\n",
      "214 2018-05-10 15:32:21.201927\n",
      "485 2018-05-10 15:32:21.628009\n",
      "983 2018-05-10 15:32:22.061242\n",
      "689 2018-05-10 15:32:22.793009\n",
      "303 2018-05-10 15:32:23.346605\n",
      "679 2018-05-10 15:32:23.920187\n",
      "764 2018-05-10 15:32:24.458903\n",
      "671 2018-05-10 15:32:25.046569\n",
      "384 2018-05-10 15:32:25.576017\n",
      "1343 2018-05-10 15:32:25.943341\n",
      "790 2018-05-10 15:32:26.812588\n",
      "427 2018-05-10 15:32:27.432245\n",
      "762 2018-05-10 15:32:27.836136\n",
      "205 2018-05-10 15:32:28.424565\n",
      "528 2018-05-10 15:32:28.636677\n",
      "1011 2018-05-10 15:32:29.087770\n",
      "806 2018-05-10 15:32:29.816335\n",
      "131 2018-05-10 15:32:30.436764\n",
      "964 2018-05-10 15:32:30.719907\n",
      "404 2018-05-10 15:32:31.418166\n",
      "281 2018-05-10 15:32:31.801599\n",
      "580 2018-05-10 15:32:33.089774\n",
      "677 2018-05-10 15:32:33.582453\n",
      "171 2018-05-10 15:32:34.121053\n",
      "966 2018-05-10 15:32:34.468722\n",
      "603 2018-05-10 15:32:35.167744\n",
      "628 2018-05-10 15:32:35.665455\n",
      "845 2018-05-10 15:32:36.182712\n",
      "459 2018-05-10 15:32:36.813785\n",
      "746 2018-05-10 15:32:37.226941\n",
      "530 2018-05-10 15:32:37.819455\n",
      "914 2018-05-10 15:32:38.274573\n",
      "1150 2018-05-10 15:32:38.951501\n",
      "124 2018-05-10 15:32:39.732622\n",
      "1071 2018-05-10 15:32:40.004216\n",
      "224 2018-05-10 15:32:40.730046\n",
      "289 2018-05-10 15:32:40.956515\n",
      "186 2018-05-10 15:32:41.241814\n",
      "162 2018-05-10 15:32:41.436861\n",
      "1141 2018-05-10 15:32:41.768809\n",
      "527 2018-05-10 15:32:42.535559\n",
      "300 2018-05-10 15:32:42.987561\n",
      "692 2018-05-10 15:32:43.480627\n",
      "1473 2018-05-10 15:32:44.021271\n",
      "664 2018-05-10 15:32:44.599839\n",
      "471 2018-05-10 15:32:45.142364\n",
      "287 2018-05-10 15:32:45.560563\n",
      "221 2018-05-10 15:32:45.844860\n",
      "566 2018-05-10 15:32:46.069625\n",
      "859 2018-05-10 15:32:46.548241\n",
      "713 2018-05-10 15:32:47.201784\n",
      "1417 2018-05-10 15:32:47.769395\n",
      "454 2018-05-10 15:32:48.673189\n",
      "353 2018-05-10 15:32:49.086342\n",
      "139 2018-05-10 15:32:49.414978\n",
      "317 2018-05-10 15:32:49.704954\n",
      "1538 2018-05-10 15:32:50.005229\n",
      "470 2018-05-10 15:32:50.976098\n",
      "992 2018-05-10 15:32:51.402156\n",
      "688 2018-05-10 15:32:52.111392\n",
      "861 2018-05-10 15:32:52.659676\n",
      "189 2018-05-10 15:32:53.301266\n",
      "1173 2018-05-10 15:32:53.496297\n",
      "898 2018-05-10 15:32:54.292491\n",
      "280 2018-05-10 15:32:54.950242\n",
      "475 2018-05-10 15:32:55.229132\n",
      "873 2018-05-10 15:32:56.950686\n",
      "358 2018-05-10 15:32:57.608786\n",
      "486 2018-05-10 15:32:57.957954\n",
      "922 2018-05-10 15:32:58.390066\n",
      "509 2018-05-10 15:32:59.058109\n",
      "16 2018-05-10 15:32:59.503401\n",
      "489 2018-05-10 15:32:59.535664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 2018-05-10 15:32:59.983222\n",
      "793 2018-05-10 15:33:00.263725\n",
      "141 2018-05-10 15:33:00.874041\n",
      "546 2018-05-10 15:33:01.029302\n",
      "232 2018-05-10 15:33:01.498349\n",
      "292 2018-05-10 15:33:01.728607\n",
      "574 2018-05-10 15:33:02.016288\n",
      "1222 2018-05-10 15:33:02.509967\n",
      "814 2018-05-10 15:33:03.360624\n",
      "236 2018-05-10 15:33:03.984654\n",
      "215 2018-05-10 15:33:04.220739\n",
      "548 2018-05-10 15:33:04.441898\n",
      "899 2018-05-10 15:33:04.923503\n",
      "555 2018-05-10 15:33:05.593856\n",
      "1350 2018-05-10 15:33:06.069156\n",
      "500 2018-05-10 15:33:06.977372\n",
      "1390 2018-05-10 15:33:07.416704\n",
      "276 2018-05-10 15:33:07.979476\n",
      "808 2018-05-10 15:33:08.254856\n",
      "477 2018-05-10 15:33:08.880698\n",
      "132 2018-05-10 15:33:09.315569\n",
      "641 2018-05-10 15:33:09.601592\n",
      "117 2018-05-10 15:33:10.120317\n",
      "653 2018-05-10 15:33:10.377138\n",
      "349 2018-05-10 15:33:10.908849\n",
      "752 2018-05-10 15:33:11.240379\n",
      "998 2018-05-10 15:33:11.817365\n",
      "218 2018-05-10 15:33:12.521186\n",
      "210 2018-05-10 15:33:12.735256\n",
      "796 2018-05-10 15:33:13.095748\n",
      "482 2018-05-10 15:33:13.701770\n",
      "596 2018-05-10 15:33:14.131087\n",
      "821 2018-05-10 15:33:15.638295\n",
      "941 2018-05-10 15:33:16.261792\n",
      "1438 2018-05-10 15:33:16.948847\n",
      "343 2018-05-10 15:33:17.872861\n",
      "187 2018-05-10 15:33:18.191768\n",
      "435 2018-05-10 15:33:18.383983\n",
      "383 2018-05-10 15:33:18.794035\n",
      "188 2018-05-10 15:33:19.159668\n",
      "467 2018-05-10 15:33:19.357157\n",
      "372 2018-05-10 15:33:19.774842\n",
      "327 2018-05-10 15:33:20.125410\n",
      "676 2018-05-10 15:33:20.436356\n",
      "246 2018-05-10 15:33:20.964565\n",
      "618 2018-05-10 15:33:21.205758\n",
      "1208 2018-05-10 15:33:21.717353\n",
      "897 2018-05-10 15:33:22.557433\n",
      "844 2018-05-10 15:33:23.210091\n",
      "212 2018-05-10 15:33:23.847958\n",
      "382 2018-05-10 15:33:24.073392\n",
      "799 2018-05-10 15:33:24.777067\n",
      "751 2018-05-10 15:33:25.393802\n",
      "445 2018-05-10 15:33:25.966181\n",
      "918 2018-05-10 15:33:26.380900\n",
      "463 2018-05-10 15:33:27.058400\n",
      "97 2018-05-10 15:33:27.476366\n",
      "271 2018-05-10 15:33:28.022146\n",
      "823 2018-05-10 15:33:28.292544\n",
      "988 2018-05-10 15:33:28.929445\n",
      "389 2018-05-10 15:33:29.640171\n",
      "1116 2018-05-10 15:33:30.011749\n",
      "1076 2018-05-10 15:33:30.791186\n",
      "989 2018-05-10 15:33:31.544634\n",
      "447 2018-05-10 15:33:32.237750\n",
      "355 2018-05-10 15:33:32.651303\n",
      "53 2018-05-10 15:33:33.004076\n",
      "516 2018-05-10 15:33:34.390053\n",
      "417 2018-05-10 15:33:34.852070\n",
      "691 2018-05-10 15:33:35.247243\n",
      "4072 2018-05-10 15:33:35.800187\n",
      "184 2018-05-10 15:33:38.134668\n",
      "479 2018-05-10 15:33:38.327638\n",
      "587 2018-05-10 15:33:38.748229\n",
      "614 2018-05-10 15:33:39.253121\n",
      "1220 2018-05-10 15:33:39.771969\n",
      "1678 2018-05-10 15:33:40.576380\n",
      "166 2018-05-10 15:33:41.625851\n",
      "687 2018-05-10 15:33:41.797155\n",
      "129 2018-05-10 15:33:42.342929\n",
      "1106 2018-05-10 15:33:42.621021\n",
      "225 2018-05-10 15:33:43.380465\n",
      "917 2018-05-10 15:33:43.763969\n",
      "885 2018-05-10 15:33:44.428267\n",
      "268 2018-05-10 15:33:45.083721\n",
      "805 2018-05-10 15:33:45.348586\n",
      "334 2018-05-10 15:33:45.961880\n",
      "283 2018-05-10 15:33:46.288368\n",
      "609 2018-05-10 15:33:46.572429\n",
      "836 2018-05-10 15:33:47.061393\n",
      "294 2018-05-10 15:33:47.673592\n",
      "950 2018-05-10 15:33:48.240718\n",
      "1134 2018-05-10 15:33:48.929846\n",
      "812 2018-05-10 15:33:49.686541\n",
      "312 2018-05-10 15:33:50.285717\n",
      "286 2018-05-10 15:33:50.583610\n",
      "133 2018-05-10 15:33:51.063205\n",
      "1886 2018-05-10 15:33:51.209785\n",
      "395 2018-05-10 15:33:51.920701\n",
      "260 2018-05-10 15:33:52.283581\n",
      "712 2018-05-10 15:33:52.541545\n",
      "1110 2018-05-10 15:33:53.108537\n",
      "732 2018-05-10 15:33:53.885092\n",
      "326 2018-05-10 15:33:54.451907\n",
      "1118 2018-05-10 15:33:54.776951\n",
      "769 2018-05-10 15:33:55.527884\n",
      "697 2018-05-10 15:33:56.119990\n",
      "939 2018-05-10 15:33:56.672063\n",
      "201 2018-05-10 15:33:57.346238\n",
      "466 2018-05-10 15:33:57.545603\n",
      "460 2018-05-10 15:33:57.956177\n",
      "1023 2018-05-10 15:33:58.365053\n",
      "217 2018-05-10 15:33:59.077068\n",
      "651 2018-05-10 15:33:59.297126\n",
      "702 2018-05-10 15:33:59.826623\n",
      "1549 2018-05-10 15:34:00.377948\n",
      "1406 2018-05-10 15:34:01.352312\n",
      "642 2018-05-10 15:34:02.259299\n",
      "825 2018-05-10 15:34:02.786484\n",
      "230 2018-05-10 15:34:03.394708\n",
      "633 2018-05-10 15:34:03.630533\n",
      "177 2018-05-10 15:34:04.148054\n",
      "329 2018-05-10 15:34:04.330352\n",
      "1430 2018-05-10 15:34:05.661955\n",
      "390 2018-05-10 15:34:06.573285\n",
      "665 2018-05-10 15:34:06.942902\n",
      "639 2018-05-10 15:34:07.493511\n",
      "1477 2018-05-10 15:34:08.037016\n",
      "678 2018-05-10 15:34:08.978295\n",
      "223 2018-05-10 15:34:09.521903\n",
      "745 2018-05-10 15:34:09.958947\n",
      "402 2018-05-10 15:34:10.532968\n",
      "728 2018-05-10 15:34:10.908675\n",
      "1738 2018-05-10 15:34:11.484604\n",
      "754 2018-05-10 15:34:12.145768\n",
      "241 2018-05-10 15:34:12.721289\n",
      "1263 2018-05-10 15:34:12.954967\n",
      "1842 2018-05-10 15:34:13.786780\n",
      "1035 2018-05-10 15:34:14.476704\n",
      "943 2018-05-10 15:34:15.211260\n",
      "1215 2018-05-10 15:34:15.895666\n",
      "1437 2018-05-10 15:34:16.702941\n",
      "309 2018-05-10 15:34:17.618167\n",
      "779 2018-05-10 15:34:17.922526\n",
      "1539 2018-05-10 15:34:18.529721\n",
      "398 2018-05-10 15:34:19.505751\n",
      "378 2018-05-10 15:34:19.888824\n",
      "461 2018-05-10 15:34:20.241227\n",
      "572 2018-05-10 15:34:20.649958\n",
      "904 2018-05-10 15:34:21.129308\n",
      "655 2018-05-10 15:34:21.809148\n",
      "1212 2018-05-10 15:34:22.345354\n",
      "1165 2018-05-10 15:34:23.166387\n",
      "1224 2018-05-10 15:34:23.956315\n",
      "351 2018-05-10 15:34:24.778089\n",
      "1153 2018-05-10 15:34:25.107620\n",
      "379 2018-05-10 15:34:25.899102\n",
      "1285 2018-05-10 15:34:26.255532\n",
      "561 2018-05-10 15:34:27.099289\n",
      "488 2018-05-10 15:34:27.575391\n",
      "726 2018-05-10 15:34:28.003626\n",
      "209 2018-05-10 15:34:28.567853\n",
      "1241 2018-05-10 15:34:28.776712\n",
      "615 2018-05-10 15:34:29.591374\n",
      "803 2018-05-10 15:34:30.109411\n",
      "295 2018-05-10 15:34:31.738662\n",
      "340 2018-05-10 15:34:32.032936\n",
      "496 2018-05-10 15:34:32.361824\n",
      "1184 2018-05-10 15:34:32.798882\n",
      "801 2018-05-10 15:34:33.590040\n",
      "359 2018-05-10 15:34:34.203234\n",
      "951 2018-05-10 15:34:34.534913\n",
      "161 2018-05-10 15:34:35.225184\n",
      "647 2018-05-10 15:34:35.554964\n",
      "1595 2018-05-10 15:34:36.081782\n",
      "1251 2018-05-10 15:34:37.081504\n",
      "667 2018-05-10 15:34:37.923793\n",
      "1172 2018-05-10 15:34:38.459954\n",
      "1228 2018-05-10 15:34:39.261378\n",
      "468 2018-05-10 15:34:40.104187\n",
      "957 2018-05-10 15:34:40.518460\n",
      "444 2018-05-10 15:34:41.214908\n",
      "906 2018-05-10 15:34:41.615592\n",
      "118 2018-05-10 15:34:42.278045\n",
      "386 2018-05-10 15:34:42.538723\n",
      "356 2018-05-10 15:34:42.912899\n",
      "994 2018-05-10 15:34:43.241842\n",
      "1086 2018-05-10 15:34:43.971916\n",
      "337 2018-05-10 15:34:44.725441\n",
      "1186 2018-05-10 15:34:45.051529\n",
      "504 2018-05-10 15:34:45.857288\n",
      "729 2018-05-10 15:34:46.314122\n",
      "656 2018-05-10 15:34:46.878390\n",
      "296 2018-05-10 15:34:47.412471\n",
      "197 2018-05-10 15:34:47.701270\n",
      "638 2018-05-10 15:34:47.907033\n",
      "449 2018-05-10 15:34:48.414474\n",
      "1099 2018-05-10 15:34:48.841209\n",
      "438 2018-05-10 15:34:49.584302\n",
      "716 2018-05-10 15:34:49.975723\n",
      "1046 2018-05-10 15:34:50.545545\n",
      "1436 2018-05-10 15:34:51.272394\n",
      "888 2018-05-10 15:34:52.188291\n",
      "267 2018-05-10 15:34:52.836247\n",
      "1158 2018-05-10 15:34:53.346572\n",
      "715 2018-05-10 15:34:54.123371\n",
      "1004 2018-05-10 15:34:55.709425\n",
      "180 2018-05-10 15:34:56.424185\n",
      "1006 2018-05-10 15:34:56.786427\n",
      "1108 2018-05-10 15:34:57.505353\n",
      "484 2018-05-10 15:34:58.286901\n",
      "1273 2018-05-10 15:34:58.706205\n",
      "497 2018-05-10 15:34:59.543843\n",
      "452 2018-05-10 15:34:59.985630\n",
      "1082 2018-05-10 15:35:00.398987\n",
      "1096 2018-05-10 15:35:01.146564\n",
      "804 2018-05-10 15:35:01.897326\n",
      "319 2018-05-10 15:35:02.518653\n",
      "1154 2018-05-10 15:35:02.821963\n",
      "766 2018-05-10 15:35:03.603725\n",
      "308 2018-05-10 15:35:04.187305\n",
      "216 2018-05-10 15:35:04.488960\n",
      "4071 2018-05-10 15:35:04.701468\n",
      "686 2018-05-10 15:35:06.016343\n",
      "875 2018-05-10 15:35:06.559974\n",
      "364 2018-05-10 15:35:07.211965\n",
      "1442 2018-05-10 15:35:07.568051\n",
      "474 2018-05-10 15:35:08.148784\n",
      "594 2018-05-10 15:35:08.578385\n",
      "935 2018-05-10 15:35:09.075649\n",
      "453 2018-05-10 15:35:09.761152\n",
      "1382 2018-05-10 15:35:10.166750\n",
      "1650 2018-05-10 15:35:11.055394\n",
      "993 2018-05-10 15:35:12.083644\n",
      "1107 2018-05-10 15:35:12.801030\n",
      "510 2018-05-10 15:35:13.557819\n",
      "204 2018-05-10 15:35:14.006289\n",
      "986 2018-05-10 15:35:14.213566\n",
      "1137 2018-05-10 15:35:14.935348\n",
      "1058 2018-05-10 15:35:15.712279\n",
      "912 2018-05-10 15:35:16.464157\n",
      "973 2018-05-10 15:35:17.143079\n",
      "1069 2018-05-10 15:35:17.841264\n",
      "1043 2018-05-10 15:35:18.564142\n",
      "999 2018-05-10 15:35:19.296371\n",
      "499 2018-05-10 15:35:20.006953\n",
      "3037 2018-05-10 15:35:20.444873\n",
      "864 2018-05-10 15:35:21.467281\n",
      "695 2018-05-10 15:35:22.122678\n",
      "525 2018-05-10 15:35:22.674022\n",
      "757 2018-05-10 15:35:23.138269\n",
      "416 2018-05-10 15:35:23.732503\n",
      "863 2018-05-10 15:35:24.111017\n",
      "902 2018-05-10 15:35:24.755887\n",
      "1363 2018-05-10 15:35:25.416144\n",
      "473 2018-05-10 15:35:27.292075\n",
      "810 2018-05-10 15:35:27.716916\n",
      "858 2018-05-10 15:35:28.328302\n",
      "865 2018-05-10 15:35:28.981532\n",
      "613 2018-05-10 15:35:29.621140\n",
      "627 2018-05-10 15:35:30.125981\n",
      "650 2018-05-10 15:35:30.636313\n",
      "1596 2018-05-10 15:35:31.168322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670 2018-05-10 15:35:32.168987\n",
      "756 2018-05-10 15:35:32.698640\n",
      "872 2018-05-10 15:35:33.284891\n",
      "829 2018-05-10 15:35:33.928870\n",
      "685 2018-05-10 15:35:34.558716\n",
      "207 2018-05-10 15:35:35.112406\n",
      "520 2018-05-10 15:35:35.329706\n",
      "807 2018-05-10 15:35:35.792264\n",
      "735 2018-05-10 15:35:36.407305\n",
      "725 2018-05-10 15:35:36.989978\n",
      "1087 2018-05-10 15:35:37.551368\n",
      "252 2018-05-10 15:35:38.289802\n",
      "1373 2018-05-10 15:35:38.535672\n",
      "1080 2018-05-10 15:35:39.423164\n",
      "299 2018-05-10 15:35:40.166810\n",
      "834 2018-05-10 15:35:40.742571\n",
      "1264 2018-05-10 15:35:41.370635\n",
      "1104 2018-05-10 15:35:42.203039\n",
      "203 2018-05-10 15:35:42.964617\n",
      "256 2018-05-10 15:35:43.169663\n",
      "502 2018-05-10 15:35:43.421895\n",
      "558 2018-05-10 15:35:43.870030\n",
      "709 2018-05-10 15:35:44.357408\n",
      "195 2018-05-10 15:35:44.919559\n",
      "163 2018-05-10 15:35:45.117934\n",
      "850 2018-05-10 15:35:45.294361\n",
      "648 2018-05-10 15:35:45.945065\n",
      "1213 2018-05-10 15:35:46.476181\n",
      "636 2018-05-10 15:35:47.312945\n",
      "707 2018-05-10 15:35:47.837319\n",
      "360 2018-05-10 15:35:49.391650\n",
      "1160 2018-05-10 15:35:49.740090\n",
      "908 2018-05-10 15:35:50.511382\n",
      "774 2018-05-10 15:35:51.178922\n",
      "1196 2018-05-10 15:35:51.774378\n",
      "826 2018-05-10 15:35:52.568274\n",
      "457 2018-05-10 15:35:53.191386\n",
      "179 2018-05-10 15:35:53.600301\n",
      "668 2018-05-10 15:35:53.787857\n",
      "408 2018-05-10 15:35:54.334522\n",
      "967 2018-05-10 15:35:54.712689\n",
      "415 2018-05-10 15:35:55.430061\n",
      "770 2018-05-10 15:35:55.814730\n",
      "1659 2018-05-10 15:35:56.400275\n",
      "591 2018-05-10 15:35:57.438789\n",
      "465 2018-05-10 15:35:57.935308\n",
      "816 2018-05-10 15:35:58.345742\n",
      "1586 2018-05-10 15:35:58.978836\n",
      "788 2018-05-10 15:35:59.590898\n",
      "722 2018-05-10 15:36:00.186959\n",
      "136 2018-05-10 15:36:00.752163\n",
      "1036 2018-05-10 15:36:01.040062\n",
      "720 2018-05-10 15:36:01.752331\n",
      "984 2018-05-10 15:36:02.316950\n",
      "923 2018-05-10 15:36:03.017325\n",
      "759 2018-05-10 15:36:03.698260\n",
      "265 2018-05-10 15:36:04.273933\n",
      "173 2018-05-10 15:36:04.538997\n",
      "625 2018-05-10 15:36:04.721971\n",
      "279 2018-05-10 15:36:05.235755\n",
      "1317 2018-05-10 15:36:05.515309\n",
      "569 2018-05-10 15:36:06.401768\n",
      "476 2018-05-10 15:36:06.879508\n",
      "577 2018-05-10 15:36:07.300806\n",
      "1050 2018-05-10 15:36:07.785709\n",
      "568 2018-05-10 15:36:08.531297\n",
      "797 2018-05-10 15:36:08.995870\n",
      "172 2018-05-10 15:36:09.584528\n",
      "710 2018-05-10 15:36:09.768414\n",
      "674 2018-05-10 15:36:10.323309\n",
      "1423 2018-05-10 15:36:10.873369\n",
      "694 2018-05-10 15:36:11.792823\n",
      "506 2018-05-10 15:36:12.343671\n",
      "316 2018-05-10 15:36:12.792944\n",
      "1017 2018-05-10 15:36:13.104350\n",
      "557 2018-05-10 15:36:13.813697\n",
      "701 2018-05-10 15:36:14.286472\n",
      "868 2018-05-10 15:36:14.834650\n",
      "760 2018-05-10 15:36:15.481672\n",
      "985 2018-05-10 15:36:17.061869\n",
      "839 2018-05-10 15:36:17.763906\n",
      "198 2018-05-10 15:36:18.408747\n",
      "393 2018-05-10 15:36:18.752021\n",
      "157 2018-05-10 15:36:19.119490\n",
      "1030 2018-05-10 15:36:19.284788\n",
      "523 2018-05-10 15:36:20.000516\n",
      "795 2018-05-10 15:36:20.452396\n",
      "480 2018-05-10 15:36:21.053955\n",
      "522 2018-05-10 15:36:21.480417\n",
      "1217 2018-05-10 15:36:21.926476\n",
      "718 2018-05-10 15:36:22.736431\n",
      "439 2018-05-10 15:36:23.304912\n",
      "684 2018-05-10 15:36:23.700979\n",
      "1303 2018-05-10 15:36:24.251556\n",
      "731 2018-05-10 15:36:25.100001\n",
      "455 2018-05-10 15:36:25.666924\n",
      "344 2018-05-10 15:36:26.070056\n",
      "595 2018-05-10 15:36:26.395301\n",
      "934 2018-05-10 15:36:26.880754\n",
      "1097 2018-05-10 15:36:27.552821\n",
      "1712 2018-05-10 15:36:28.303904\n",
      "602 2018-05-10 15:36:28.960453\n",
      "786 2018-05-10 15:36:29.463062\n",
      "285 2018-05-10 15:36:30.067468\n",
      "146 2018-05-10 15:36:30.349915\n",
      "640 2018-05-10 15:36:30.654833\n",
      "1143 2018-05-10 15:36:31.181340\n",
      "811 2018-05-10 15:36:31.947099\n",
      "1084 2018-05-10 15:36:32.551287\n",
      "785 2018-05-10 15:36:33.282575\n",
      "748 2018-05-10 15:36:33.882917\n",
      "1504 2018-05-10 15:36:34.454974\n",
      "783 2018-05-10 15:36:35.416401\n",
      "1027 2018-05-10 15:36:36.022143\n",
      "693 2018-05-10 15:36:36.745396\n",
      "1070 2018-05-10 15:36:37.286796\n",
      "269 2018-05-10 15:36:38.029821\n",
      "612 2018-05-10 15:36:38.293913\n",
      "420 2018-05-10 15:36:38.788664\n",
      "953 2018-05-10 15:36:39.172229\n",
      "776 2018-05-10 15:36:39.866254\n",
      "278 2018-05-10 15:36:40.473996\n",
      "894 2018-05-10 15:36:40.755532\n",
      "425 2018-05-10 15:36:41.412127\n",
      "1276 2018-05-10 15:36:41.801334\n",
      "323 2018-05-10 15:36:42.657165\n",
      "1115 2018-05-10 15:36:42.968564\n",
      "1310 2018-05-10 15:36:43.748640\n",
      "540 2018-05-10 15:36:44.596531\n",
      "623 2018-05-10 15:36:45.061517\n",
      "775 2018-05-10 15:36:45.570386\n",
      "1497 2018-05-10 15:36:46.169285\n",
      "1226 2018-05-10 15:36:46.763876\n",
      "369 2018-05-10 15:36:47.595239\n",
      "588 2018-05-10 15:36:47.949008\n",
      "970 2018-05-10 15:36:48.438730\n",
      "714 2018-05-10 15:36:49.121086\n",
      "567 2018-05-10 15:36:49.674644\n",
      "987 2018-05-10 15:36:51.189831\n",
      "1339 2018-05-10 15:36:51.914871\n",
      "348 2018-05-10 15:36:52.800535\n",
      "874 2018-05-10 15:36:53.132029\n",
      "900 2018-05-10 15:36:53.784368\n",
      "422 2018-05-10 15:36:54.441823\n",
      "605 2018-05-10 15:36:54.833709\n",
      "304 2018-05-10 15:36:55.336529\n",
      "275 2018-05-10 15:36:55.927116\n",
      "1049 2018-05-10 15:36:56.196893\n",
      "305 2018-05-10 15:36:56.939004\n",
      "469 2018-05-10 15:36:57.527965\n",
      "963 2018-05-10 15:36:57.942294\n",
      "708 2018-05-10 15:36:58.625686\n",
      "1183 2018-05-10 15:36:59.189003\n",
      "619 2018-05-10 15:37:00.000076\n",
      "11 2018-05-10 15:37:00.503003\n",
      "367 2018-05-10 15:37:00.528799\n",
      "1271 2018-05-10 15:37:00.877031\n",
      "777 2018-05-10 15:37:01.730792\n",
      "254 2018-05-10 15:37:02.321252\n",
      "333 2018-05-10 15:37:02.578448\n",
      "313 2018-05-10 15:37:02.895911\n",
      "742 2018-05-10 15:37:03.199153\n",
      "354 2018-05-10 15:37:03.774795\n",
      "240 2018-05-10 15:37:04.109416\n",
      "869 2018-05-10 15:37:04.350644\n",
      "1000 2018-05-10 15:37:04.992442\n",
      "734 2018-05-10 15:37:05.715255\n",
      "1007 2018-05-10 15:37:06.291625\n",
      "253 2018-05-10 15:37:07.011423\n",
      "424 2018-05-10 15:37:07.259973\n",
      "597 2018-05-10 15:37:07.648501\n",
      "1568 2018-05-10 15:37:08.131403\n",
      "1105 2018-05-10 15:37:09.119296\n",
      "495 2018-05-10 15:37:09.875539\n",
      "1159 2018-05-10 15:37:11.297399\n",
      "658 2018-05-10 15:37:12.074546\n",
      "749 2018-05-10 15:37:12.596660\n",
      "741 2018-05-10 15:37:13.181238\n",
      "277 2018-05-10 15:37:13.750661\n",
      "938 2018-05-10 15:37:14.017479\n",
      "833 2018-05-10 15:37:14.706231\n",
      "1362 2018-05-10 15:37:15.319230\n",
      "1081 2018-05-10 15:37:16.201554\n",
      "877 2018-05-10 15:37:16.966943\n",
      "1199 2018-05-10 15:37:17.624789\n",
      "407 2018-05-10 15:37:18.433435\n",
      "1234 2018-05-10 15:37:18.821191\n",
      "1530 2018-05-10 15:37:19.633569\n",
      "1721 2018-05-10 15:37:20.600087\n",
      "174 2018-05-10 15:37:21.667530\n",
      "761 2018-05-10 15:37:21.842829\n",
      "652 2018-05-10 15:37:22.443240\n",
      "1120 2018-05-10 15:37:22.959756\n",
      "747 2018-05-10 15:37:23.737636\n",
      "1117 2018-05-10 15:37:24.310705\n",
      "211 2018-05-10 15:37:25.085874\n",
      "905 2018-05-10 15:37:25.303185\n",
      "1198 2018-05-10 15:37:25.977591\n",
      "876 2018-05-10 15:37:26.789182\n",
      "193 2018-05-10 15:37:27.441960\n",
      "220 2018-05-10 15:37:27.637667\n",
      "644 2018-05-10 15:37:27.862018\n",
      "1468 2018-05-10 15:37:28.393355\n",
      "781 2018-05-10 15:37:29.324340\n",
      "494 2018-05-10 15:37:29.922333\n",
      "1590 2018-05-10 15:37:30.356324\n",
      "273 2018-05-10 15:37:30.971710\n",
      "740 2018-05-10 15:37:31.235566\n",
      "440 2018-05-10 15:37:31.809322\n",
      "361 2018-05-10 15:37:32.221679\n",
      "960 2018-05-10 15:37:32.564382\n",
      "1057 2018-05-10 15:37:33.248852\n",
      "585 2018-05-10 15:37:33.993368\n",
      "1260 2018-05-10 15:37:34.482935\n",
      "1194 2018-05-10 15:37:34.997474\n",
      "357 2018-05-10 15:37:35.784065\n",
      "1038 2018-05-10 15:37:36.116071\n",
      "630 2018-05-10 15:37:36.824682\n",
      "532 2018-05-10 15:37:37.340347\n",
      "1413 2018-05-10 15:37:37.795258\n",
      "1075 2018-05-10 15:37:38.695405\n",
      "1101 2018-05-10 15:37:39.420153\n",
      "849 2018-05-10 15:37:40.189864\n",
      "2393 2018-05-10 15:37:40.823884\n",
      "1440 2018-05-10 15:37:41.677945\n",
      "314 2018-05-10 15:37:42.592995\n",
      "138 2018-05-10 15:37:42.895506\n",
      "282 2018-05-10 15:37:43.186694\n",
      "140 2018-05-10 15:37:43.466277\n",
      "871 2018-05-10 15:37:43.619007\n",
      "1293 2018-05-10 15:37:44.239384\n",
      "1299 2018-05-10 15:37:45.084767\n",
      "887 2018-05-10 15:37:45.929447\n",
      "621 2018-05-10 15:37:46.581720\n",
      "1396 2018-05-10 15:37:48.077982\n",
      "547 2018-05-10 15:37:48.981510\n",
      "1411 2018-05-10 15:37:49.448751\n",
      "1407 2018-05-10 15:37:50.350519\n",
      "977 2018-05-10 15:37:51.276795\n",
      "418 2018-05-10 15:37:51.995774\n",
      "346 2018-05-10 15:37:52.379709\n",
      "604 2018-05-10 15:37:52.701735\n",
      "1677 2018-05-10 15:37:53.196790\n",
      "1232 2018-05-10 15:37:53.845975\n",
      "549 2018-05-10 15:37:54.657207\n",
      "1663 2018-05-10 15:37:55.119892\n",
      "1324 2018-05-10 15:37:55.755318\n",
      "927 2018-05-10 15:37:56.611244\n",
      "1730 2018-05-10 15:37:57.289270\n",
      "1239 2018-05-10 15:37:57.946527\n",
      "1178 2018-05-10 15:37:58.776189\n",
      "1240 2018-05-10 15:37:59.579070\n",
      "543 2018-05-10 15:38:00.397833\n",
      "1378 2018-05-10 15:38:00.858798\n",
      "1214 2018-05-10 15:38:01.750925\n",
      "544 2018-05-10 15:38:02.554730\n",
      "565 2018-05-10 15:38:03.017921\n",
      "929 2018-05-10 15:38:03.493580\n",
      "835 2018-05-10 15:38:04.174621\n",
      "778 2018-05-10 15:38:04.810467\n",
      "542 2018-05-10 15:38:05.403258\n",
      "926 2018-05-10 15:38:05.879769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1237 2018-05-10 15:38:06.549810\n",
      "1177 2018-05-10 15:38:07.361173\n",
      "1131 2018-05-10 15:38:08.154875\n",
      "518 2018-05-10 15:38:08.906344\n",
      "512 2018-05-10 15:38:09.344458\n",
      "428 2018-05-10 15:38:09.781491\n",
      "1314 2018-05-10 15:38:10.174006\n",
      "891 2018-05-10 15:38:11.036946\n",
      "782 2018-05-10 15:38:11.689177\n",
      "1078 2018-05-10 15:38:12.282315\n",
      "1318 2018-05-10 15:38:13.015509\n",
      "1395 2018-05-10 15:38:13.870198\n",
      "1040 2018-05-10 15:38:14.442031\n",
      "932 2018-05-10 15:38:15.171285\n",
      "325 2018-05-10 15:38:15.855457\n",
      "1302 2018-05-10 15:38:16.174356\n",
      "955 2018-05-10 15:38:17.013553\n",
      "13 2018-05-10 15:38:17.704253\n",
      "913 2018-05-10 15:38:17.732869\n",
      "832 2018-05-10 15:38:18.402290\n",
      "1793 2018-05-10 15:38:19.025299\n",
      "824 2018-05-10 15:38:19.696379\n",
      "1364 2018-05-10 15:38:20.308815\n",
      "962 2018-05-10 15:38:21.203542\n",
      "643 2018-05-10 15:38:21.901423\n",
      "1229 2018-05-10 15:38:22.431515\n",
      "815 2018-05-10 15:38:23.269664\n",
      "1541 2018-05-10 15:38:23.882364\n",
      "924 2018-05-10 15:38:24.852595\n",
      "819 2018-05-10 15:38:25.519623\n",
      "1255 2018-05-10 15:38:26.136850\n",
      "1443 2018-05-10 15:38:26.974693\n",
      "773 2018-05-10 15:38:27.548941\n",
      "1400 2018-05-10 15:38:28.135015\n",
      "1225 2018-05-10 15:38:29.031130\n",
      "610 2018-05-10 15:38:29.840769\n",
      "593 2018-05-10 15:38:30.348106\n",
      "739 2018-05-10 15:38:30.832038\n",
      "1191 2018-05-10 15:38:31.403656\n",
      "1009 2018-05-10 15:38:32.217306\n",
      "1439 2018-05-10 15:38:32.911664\n",
      "1246 2018-05-10 15:38:33.837180\n",
      "1019 2018-05-10 15:38:34.682091\n",
      "1289 2018-05-10 15:38:35.379961\n",
      "771 2018-05-10 15:38:36.237971\n",
      "889 2018-05-10 15:38:36.825010\n",
      "1308 2018-05-10 15:38:37.470516\n",
      "954 2018-05-10 15:38:38.326886\n",
      "997 2018-05-10 15:38:39.020374\n",
      "1340 2018-05-10 15:38:40.723478\n",
      "1204 2018-05-10 15:38:41.594170\n",
      "662 2018-05-10 15:38:42.389231\n",
      "1936 2018-05-10 15:38:42.931441\n",
      "1122 2018-05-10 15:38:43.658781\n",
      "961 2018-05-10 15:38:44.432950\n",
      "182 2018-05-10 15:38:45.124126\n",
      "1348 2018-05-10 15:38:45.315518\n",
      "1366 2018-05-10 15:38:46.212454\n",
      "1102 2018-05-10 15:38:47.088992\n",
      "1281 2018-05-10 15:38:47.864048\n",
      "1419 2018-05-10 15:38:48.716760\n",
      "911 2018-05-10 15:38:49.630874\n",
      "915 2018-05-10 15:38:50.309291\n",
      "248 2018-05-10 15:38:50.959399\n",
      "672 2018-05-10 15:38:51.202987\n",
      "921 2018-05-10 15:38:51.742676\n",
      "1316 2018-05-10 15:38:52.417687\n",
      "818 2018-05-10 15:38:53.274006\n",
      "347 2018-05-10 15:38:53.875589\n",
      "945 2018-05-10 15:38:54.208343\n",
      "892 2018-05-10 15:38:54.890295\n",
      "1315 2018-05-10 15:38:55.547542\n",
      "1247 2018-05-10 15:38:56.080063\n",
      "472 2018-05-10 15:38:56.898528\n",
      "342 2018-05-10 15:38:57.327024\n",
      "1091 2018-05-10 15:38:57.663899\n",
      "534 2018-05-10 15:38:58.436889\n",
      "1472 2018-05-10 15:38:58.911766\n",
      "949 2018-05-10 15:38:59.843420\n",
      "1195 2018-05-10 15:39:00.533409\n",
      "705 2018-05-10 15:39:01.316667\n",
      "946 2018-05-10 15:39:01.882175\n",
      "972 2018-05-10 15:39:02.585807\n",
      "1790 2018-05-10 15:39:03.299869\n",
      "1259 2018-05-10 15:39:03.972310\n",
      "1068 2018-05-10 15:39:04.811950\n",
      "1359 2018-05-10 15:39:05.538052\n",
      "631 2018-05-10 15:39:06.411432\n",
      "451 2018-05-10 15:39:06.922407\n",
      "711 2018-05-10 15:39:07.330072\n",
      "794 2018-05-10 15:39:07.889785\n",
      "975 2018-05-10 15:39:08.475246\n",
      "919 2018-05-10 15:39:09.151197\n",
      "1174 2018-05-10 15:39:09.821173\n",
      "1347 2018-05-10 15:39:10.613767\n",
      "1401 2018-05-10 15:39:11.486157\n",
      "521 2018-05-10 15:39:12.386438\n",
      "1072 2018-05-10 15:39:12.829546\n",
      "1012 2018-05-10 15:39:13.571702\n",
      "1486 2018-05-10 15:39:14.297504\n",
      "1051 2018-05-10 15:39:15.243079\n",
      "1602 2018-05-10 15:39:15.983078\n",
      "1387 2018-05-10 15:39:16.600767\n",
      "634 2018-05-10 15:39:17.487280\n",
      "2048 2018-05-10 15:39:18.004696\n",
      "228 2018-05-10 15:39:19.730569\n",
      "1008 2018-05-10 15:39:19.960992\n",
      "1003 2018-05-10 15:39:20.694172\n",
      "1648 2018-05-10 15:39:21.412223\n",
      "1351 2018-05-10 15:39:22.435900\n",
      "1261 2018-05-10 15:39:23.308447\n",
      "1094 2018-05-10 15:39:24.135447\n",
      "1161 2018-05-10 15:39:24.885761\n",
      "1294 2018-05-10 15:39:25.692955\n",
      "942 2018-05-10 15:39:26.536499\n",
      "529 2018-05-10 15:39:27.223469\n",
      "1432 2018-05-10 15:39:27.673926\n",
      "419 2018-05-10 15:39:28.253808\n",
      "1245 2018-05-10 15:39:28.641243\n",
      "758 2018-05-10 15:39:29.454431\n",
      "1866 2018-05-10 15:39:30.039577\n",
      "1065 2018-05-10 15:39:30.729053\n",
      "1185 2018-05-10 15:39:31.457586\n",
      "554 2018-05-10 15:39:32.237846\n",
      "2139 2018-05-10 15:39:32.710500\n",
      "1718 2018-05-10 15:39:33.473357\n",
      "1476 2018-05-10 15:39:34.126032\n",
      "559 2018-05-10 15:39:35.061462\n",
      "606 2018-05-10 15:39:35.538531\n",
      "1466 2018-05-10 15:39:36.040417\n",
      "573 2018-05-10 15:39:36.973687\n",
      "1055 2018-05-10 15:39:37.463620\n",
      "721 2018-05-10 15:39:38.219079\n",
      "1291 2018-05-10 15:39:38.776748\n",
      "1262 2018-05-10 15:39:39.615692\n",
      "969 2018-05-10 15:39:40.435030\n",
      "1061 2018-05-10 15:39:41.131336\n",
      "511 2018-05-10 15:39:41.870860\n",
      "1722 2018-05-10 15:39:42.319209\n",
      "683 2018-05-10 15:39:42.981972\n",
      "724 2018-05-10 15:39:43.510045\n",
      "1361 2018-05-10 15:39:44.071681\n",
      "948 2018-05-10 15:39:44.974849\n",
      "608 2018-05-10 15:39:45.658452\n",
      "1397 2018-05-10 15:39:46.164573\n",
      "968 2018-05-10 15:39:47.059496\n",
      "703 2018-05-10 15:39:47.750663\n",
      "723 2018-05-10 15:39:48.310772\n",
      "690 2018-05-10 15:39:48.890142\n",
      "1152 2018-05-10 15:39:49.445962\n",
      "1399 2018-05-10 15:39:50.250714\n",
      "1218 2018-05-10 15:39:51.144848\n",
      "925 2018-05-10 15:39:51.963924\n",
      "493 2018-05-10 15:39:52.639030\n",
      "980 2018-05-10 15:39:53.090894\n",
      "744 2018-05-10 15:39:53.790937\n",
      "629 2018-05-10 15:39:54.360789\n",
      "1090 2018-05-10 15:39:54.881990\n",
      "1706 2018-05-10 15:39:55.349915\n",
      "1375 2018-05-10 15:39:56.005344\n",
      "1149 2018-05-10 15:39:56.885823\n",
      "1352 2018-05-10 15:39:57.656569\n",
      "1206 2018-05-10 15:39:58.524348\n",
      "822 2018-05-10 15:39:59.342585\n",
      "1431 2018-05-10 15:40:00.963711\n",
      "1582 2018-05-10 15:40:01.878361\n",
      "1337 2018-05-10 15:40:02.878867\n",
      "1031 2018-05-10 15:40:03.744006\n",
      "1095 2018-05-10 15:40:04.448846\n",
      "1163 2018-05-10 15:40:05.209268\n",
      "842 2018-05-10 15:40:05.977204\n",
      "645 2018-05-10 15:40:06.614982\n",
      "1022 2018-05-10 15:40:07.139560\n",
      "1093 2018-05-10 15:40:07.865800\n",
      "1079 2018-05-10 15:40:08.622224\n",
      "838 2018-05-10 15:40:09.384549\n",
      "1447 2018-05-10 15:40:10.008457\n",
      "1313 2018-05-10 15:40:10.935439\n",
      "1025 2018-05-10 15:40:11.790784\n",
      "700 2018-05-10 15:40:12.523240\n",
      "853 2018-05-10 15:40:13.072691\n",
      "789 2018-05-10 15:40:13.711115\n",
      "1458 2018-05-10 15:40:14.307669\n",
      "1304 2018-05-10 15:40:15.232798\n",
      "1128 2018-05-10 15:40:16.087105\n",
      "531 2018-05-10 15:40:16.848790\n",
      "1113 2018-05-10 15:40:17.305898\n",
      "1626 2018-05-10 15:40:18.060517\n",
      "1063 2018-05-10 15:40:19.080289\n",
      "944 2018-05-10 15:40:19.815298\n",
      "1257 2018-05-10 15:40:20.504606\n",
      "1312 2018-05-10 15:40:21.331391\n",
      "673 2018-05-10 15:40:21.864572\n",
      "1270 2018-05-10 15:40:22.391958\n",
      "1112 2018-05-10 15:40:23.222079\n",
      "1048 2018-05-10 15:40:23.980014\n",
      "767 2018-05-10 15:40:24.709847\n",
      "930 2018-05-10 15:40:25.285639\n",
      "1254 2018-05-10 15:40:25.967983\n",
      "1034 2018-05-10 15:40:26.826871\n",
      "1016 2018-05-10 15:40:27.550230\n",
      "600 2018-05-10 15:40:28.289508\n",
      "1044 2018-05-10 15:40:28.789664\n",
      "1238 2018-05-10 15:40:29.506121\n",
      "1033 2018-05-10 15:40:30.319937\n",
      "907 2018-05-10 15:40:31.044496\n",
      "1244 2018-05-10 15:40:31.699944\n",
      "1724 2018-05-10 15:40:32.218456\n",
      "1326 2018-05-10 15:40:32.872229\n",
      "562 2018-05-10 15:40:33.733545\n",
      "1715 2018-05-10 15:40:34.208624\n",
      "1355 2018-05-10 15:40:35.269113\n",
      "1018 2018-05-10 15:40:36.147728\n",
      "1010 2018-05-10 15:40:36.860830\n",
      "1242 2018-05-10 15:40:37.564302\n",
      "1274 2018-05-10 15:40:38.419763\n",
      "1130 2018-05-10 15:40:39.255268\n",
      "1233 2018-05-10 15:40:40.024330\n",
      "1709 2018-05-10 15:40:40.834515\n",
      "855 2018-05-10 15:40:41.482248\n",
      "9 2018-05-10 15:40:42.131736\n",
      "681 2018-05-10 15:40:42.155377\n",
      "1187 2018-05-10 15:40:42.711268\n",
      "1414 2018-05-10 15:40:43.500728\n",
      "1182 2018-05-10 15:40:44.411223\n",
      "903 2018-05-10 15:40:45.203437\n",
      "1403 2018-05-10 15:40:46.888332\n",
      "1042 2018-05-10 15:40:47.786618\n",
      "1139 2018-05-10 15:40:48.515407\n",
      "1248 2018-05-10 15:40:49.310055\n",
      "564 2018-05-10 15:40:50.132165\n",
      "1306 2018-05-10 15:40:50.608627\n",
      "1421 2018-05-10 15:40:51.455049\n",
      "1597 2018-05-10 15:40:52.359732\n",
      "1651 2018-05-10 15:40:53.364033\n",
      "1661 2018-05-10 15:40:53.998933\n",
      "1546 2018-05-10 15:40:55.035870\n",
      "1894 2018-05-10 15:40:55.636402\n",
      "879 2018-05-10 15:40:56.339591\n",
      "1405 2018-05-10 15:40:56.986312\n",
      "1021 2018-05-10 15:40:57.558250\n",
      "981 2018-05-10 15:40:58.257765\n",
      "1124 2018-05-10 15:40:58.964520\n",
      "1662 2018-05-10 15:40:59.753555\n",
      "1005 2018-05-10 15:41:00.388078\n",
      "175 2018-05-10 15:41:01.100712\n",
      "666 2018-05-10 15:41:01.287267\n",
      "1024 2018-05-10 15:41:01.814012\n",
      "1197 2018-05-10 15:41:02.537070\n",
      "478 2018-05-10 15:41:03.358029\n",
      "1164 2018-05-10 15:41:03.777536\n",
      "1129 2018-05-10 15:41:04.554980\n",
      "1111 2018-05-10 15:41:05.314169\n",
      "800 2018-05-10 15:41:06.074870\n",
      "1292 2018-05-10 15:41:06.674521\n",
      "1140 2018-05-10 15:41:07.514130\n",
      "1190 2018-05-10 15:41:07.995163\n",
      "1253 2018-05-10 15:41:08.785524\n",
      "1607 2018-05-10 15:41:09.619128\n",
      "1319 2018-05-10 15:41:10.626283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066 2018-05-10 15:41:11.477312\n",
      "1418 2018-05-10 15:41:11.935732\n",
      "1041 2018-05-10 15:41:12.502662\n",
      "866 2018-05-10 15:41:13.223041\n",
      "1151 2018-05-10 15:41:13.862454\n",
      "1020 2018-05-10 15:41:14.633063\n",
      "1945 2018-05-10 15:41:15.329707\n",
      "1216 2018-05-10 15:41:16.043660\n",
      "1897 2018-05-10 15:41:16.850505\n",
      "1563 2018-05-10 15:41:18.007215\n",
      "1052 2018-05-10 15:41:18.983048\n",
      "1203 2018-05-10 15:41:19.712189\n",
      "1209 2018-05-10 15:41:20.514410\n",
      "1169 2018-05-10 15:41:21.320722\n",
      "910 2018-05-10 15:41:22.105358\n",
      "1028 2018-05-10 15:41:22.771405\n",
      "1329 2018-05-10 15:41:23.502324\n",
      "1394 2018-05-10 15:41:24.382430\n",
      "1014 2018-05-10 15:41:24.945534\n",
      "1448 2018-05-10 15:41:25.636145\n",
      "1450 2018-05-10 15:41:26.206491\n",
      "1740 2018-05-10 15:41:26.779458\n",
      "852 2018-05-10 15:41:27.853881\n",
      "870 2018-05-10 15:41:28.489561\n",
      "1833 2018-05-10 15:41:29.137059\n",
      "860 2018-05-10 15:41:29.818174\n",
      "1353 2018-05-10 15:41:30.465827\n",
      "753 2018-05-10 15:41:31.022938\n",
      "1083 2018-05-10 15:41:31.631279\n",
      "733 2018-05-10 15:41:32.368879\n",
      "1249 2018-05-10 15:41:32.939736\n",
      "1053 2018-05-10 15:41:33.761969\n",
      "1188 2018-05-10 15:41:34.488471\n",
      "1060 2018-05-10 15:41:35.290983\n",
      "1277 2018-05-10 15:41:36.028329\n",
      "1064 2018-05-10 15:41:36.879744\n",
      "937 2018-05-10 15:41:37.647667\n",
      "828 2018-05-10 15:41:38.335630\n",
      "854 2018-05-10 15:41:38.948955\n",
      "1349 2018-05-10 15:41:39.582484\n",
      "1575 2018-05-10 15:41:40.452362\n",
      "1168 2018-05-10 15:41:41.444888\n",
      "1328 2018-05-10 15:41:42.247585\n",
      "1518 2018-05-10 15:41:43.107595\n",
      "920 2018-05-10 15:41:44.073379\n",
      "965 2018-05-10 15:41:44.721312\n",
      "1346 2018-05-10 15:41:45.400415\n",
      "1367 2018-05-10 15:41:46.271394\n",
      "784 2018-05-10 15:41:47.148705\n",
      "1123 2018-05-10 15:41:47.737799\n",
      "1088 2018-05-10 15:41:48.493139\n",
      "1330 2018-05-10 15:41:49.245691\n",
      "1039 2018-05-10 15:41:50.106213\n",
      "1456 2018-05-10 15:41:50.837634\n",
      "1307 2018-05-10 15:41:51.765737\n",
      "1606 2018-05-10 15:41:52.618286\n",
      "991 2018-05-10 15:41:53.627307\n",
      "1499 2018-05-10 15:41:54.338273\n",
      "1544 2018-05-10 15:41:54.928504\n",
      "1002 2018-05-10 15:41:55.899352\n",
      "1267 2018-05-10 15:41:56.602005\n",
      "1211 2018-05-10 15:41:57.430752\n",
      "1384 2018-05-10 15:41:58.249274\n",
      "1777 2018-05-10 15:41:58.809940\n",
      "1121 2018-05-10 15:41:59.472136\n",
      "1147 2018-05-10 15:42:00.235479\n",
      "1357 2018-05-10 15:42:00.728360\n",
      "1059 2018-05-10 15:42:01.272738\n",
      "1641 2018-05-10 15:42:01.992899\n",
      "1230 2018-05-10 15:42:02.630792\n",
      "1365 2018-05-10 15:42:03.444890\n",
      "2323 2018-05-10 15:42:04.343588\n",
      "1386 2018-05-10 15:42:05.177038\n",
      "696 2018-05-10 15:42:05.736427\n",
      "10 2018-05-10 15:42:06.276143\n",
      "1132 2018-05-10 15:42:06.299656\n",
      "1676 2018-05-10 15:42:07.053702\n",
      "1323 2018-05-10 15:42:07.696543\n",
      "1146 2018-05-10 15:42:08.554247\n",
      "1192 2018-05-10 15:42:09.315251\n",
      "1054 2018-05-10 15:42:10.121786\n",
      "1156 2018-05-10 15:42:10.857033\n",
      "1272 2018-05-10 15:42:11.633275\n",
      "1977 2018-05-10 15:42:12.161222\n",
      "1202 2018-05-10 15:42:12.888547\n",
      "1717 2018-05-10 15:42:13.681089\n",
      "1761 2018-05-10 15:42:15.347630\n",
      "1098 2018-05-10 15:42:16.008099\n",
      "1210 2018-05-10 15:42:16.744443\n",
      "1505 2018-05-10 15:42:17.545102\n",
      "8 2018-05-10 15:42:18.496253\n",
      "1269 2018-05-10 15:42:18.518667\n",
      "2395 2018-05-10 15:42:19.355725\n",
      "1127 2018-05-10 15:42:20.205760\n",
      "1374 2018-05-10 15:42:20.974176\n",
      "1501 2018-05-10 15:42:21.860360\n",
      "936 2018-05-10 15:42:22.808293\n",
      "1175 2018-05-10 15:42:23.485985\n",
      "1284 2018-05-10 15:42:24.277715\n",
      "1492 2018-05-10 15:42:25.118130\n",
      "1138 2018-05-10 15:42:26.061528\n",
      "1103 2018-05-10 15:42:26.815335\n",
      "1593 2018-05-10 15:42:27.560753\n",
      "1416 2018-05-10 15:42:28.557710\n",
      "1633 2018-05-10 15:42:29.461850\n",
      "1422 2018-05-10 15:42:30.479256\n",
      "1425 2018-05-10 15:42:31.387178\n",
      "1952 2018-05-10 15:42:31.950640\n",
      "2453 2018-05-10 15:42:32.660040\n",
      "1333 2018-05-10 15:42:33.532737\n",
      "1679 2018-05-10 15:42:34.396580\n",
      "1377 2018-05-10 15:42:35.441132\n",
      "1309 2018-05-10 15:42:36.002453\n",
      "1628 2018-05-10 15:42:36.857133\n",
      "1521 2018-05-10 15:42:37.481975\n",
      "1265 2018-05-10 15:42:38.074709\n",
      "1519 2018-05-10 15:42:38.896707\n",
      "1114 2018-05-10 15:42:39.863498\n",
      "1815 2018-05-10 15:42:40.604914\n",
      "1474 2018-05-10 15:42:41.278665\n",
      "1527 2018-05-10 15:42:42.217252\n",
      "1725 2018-05-10 15:42:42.814923\n",
      "1398 2018-05-10 15:42:43.883096\n",
      "1471 2018-05-10 15:42:44.774426\n",
      "1615 2018-05-10 15:42:45.707857\n",
      "1227 2018-05-10 15:42:46.717120\n",
      "1179 2018-05-10 15:42:47.528191\n",
      "1176 2018-05-10 15:42:48.306427\n",
      "1514 2018-05-10 15:42:49.115683\n",
      "4132 2018-05-10 15:42:50.068589\n",
      "1126 2018-05-10 15:42:51.422859\n",
      "1181 2018-05-10 15:42:52.203719\n",
      "1371 2018-05-10 15:42:52.696415\n",
      "1275 2018-05-10 15:42:53.574561\n",
      "1282 2018-05-10 15:42:54.404025\n",
      "1420 2018-05-10 15:42:55.235644\n",
      "1460 2018-05-10 15:42:55.805118\n",
      "2348 2018-05-10 15:42:56.731333\n",
      "1645 2018-05-10 15:42:57.556217\n",
      "1295 2018-05-10 15:42:58.577944\n",
      "1092 2018-05-10 15:42:59.419684\n",
      "1444 2018-05-10 15:43:00.156991\n",
      "1451 2018-05-10 15:43:00.725340\n",
      "1449 2018-05-10 15:43:01.306675\n",
      "1435 2018-05-10 15:43:02.229238\n",
      "1495 2018-05-10 15:43:02.815201\n",
      "1520 2018-05-10 15:43:03.763943\n",
      "1484 2018-05-10 15:43:04.722293\n",
      "1283 2018-05-10 15:43:05.308687\n",
      "2143 2018-05-10 15:43:06.143181\n",
      "1558 2018-05-10 15:43:06.914686\n",
      "1338 2018-05-10 15:43:07.889819\n",
      "1162 2018-05-10 15:43:08.752312\n",
      "1806 2018-05-10 15:43:09.521684\n",
      "1552 2018-05-10 15:43:10.196631\n",
      "1726 2018-05-10 15:43:10.797319\n",
      "1305 2018-05-10 15:43:11.444921\n",
      "2283 2018-05-10 15:43:12.289540\n",
      "1489 2018-05-10 15:43:13.102702\n",
      "1632 2018-05-10 15:43:13.689259\n",
      "1467 2018-05-10 15:43:14.321903\n",
      "1144 2018-05-10 15:43:14.905593\n",
      "1483 2018-05-10 15:43:15.677711\n",
      "1493 2018-05-10 15:43:16.616764\n",
      "1426 2018-05-10 15:43:17.561435\n",
      "1167 2018-05-10 15:43:18.469252\n",
      "882 2018-05-10 15:43:19.249808\n",
      "1829 2018-05-10 15:43:19.882841\n",
      "1457 2018-05-10 15:43:20.563948\n",
      "1665 2018-05-10 15:43:21.490329\n",
      "1550 2018-05-10 15:43:22.525479\n",
      "1013 2018-05-10 15:43:23.129058\n",
      "1301 2018-05-10 15:43:23.823951\n",
      "1157 2018-05-10 15:43:24.672299\n",
      "3260 2018-05-10 15:43:25.439581\n",
      "976 2018-05-10 15:43:26.527843\n",
      "1148 2018-05-10 15:43:27.219215\n",
      "2598 2018-05-10 15:43:27.982776\n",
      "1487 2018-05-10 15:43:28.895478\n",
      "1223 2018-05-10 15:43:29.846172\n",
      "1657 2018-05-10 15:43:30.657596\n",
      "1171 2018-05-10 15:43:31.298963\n",
      "1193 2018-05-10 15:43:32.091414\n",
      "1074 2018-05-10 15:43:32.881681\n",
      "1903 2018-05-10 15:43:33.611125\n",
      "1235 2018-05-10 15:43:34.772827\n",
      "2656 2018-05-10 15:43:35.601561\n",
      "1809 2018-05-10 15:43:36.525651\n",
      "1569 2018-05-10 15:43:37.205576\n",
      "1409 2018-05-10 15:43:37.817579\n",
      "1601 2018-05-10 15:43:38.376684\n",
      "1296 2018-05-10 15:43:38.991118\n",
      "1368 2018-05-10 15:43:39.833757\n",
      "1402 2018-05-10 15:43:40.711466\n",
      "1760 2018-05-10 15:43:41.606836\n",
      "2479 2018-05-10 15:43:42.279988\n",
      "1482 2018-05-10 15:43:43.165085\n",
      "1379 2018-05-10 15:43:43.754339\n",
      "3458 2018-05-10 15:43:44.637682\n",
      "1655 2018-05-10 15:43:45.793890\n",
      "1579 2018-05-10 15:43:46.426170\n",
      "1532 2018-05-10 15:43:47.048372\n",
      "1136 2018-05-10 15:43:47.648194\n",
      "840 2018-05-10 15:43:48.405413\n",
      "982 2018-05-10 15:43:49.026008\n",
      "1618 2018-05-10 15:43:49.705997\n",
      "1452 2018-05-10 15:43:50.330360\n",
      "1949 2018-05-10 15:43:51.260045\n",
      "1358 2018-05-10 15:43:51.972800\n",
      "2837 2018-05-10 15:43:52.852786\n",
      "1298 2018-05-10 15:43:53.825612\n",
      "1696 2018-05-10 15:43:54.675320\n",
      "1266 2018-05-10 15:43:55.727721\n",
      "1383 2018-05-10 15:43:56.551952\n",
      "1335 2018-05-10 15:43:57.439590\n",
      "4660 2018-05-10 15:43:58.299496\n",
      "1642 2018-05-10 15:43:59.790796\n",
      "1180 2018-05-10 15:44:00.424485\n",
      "1516 2018-05-10 15:44:00.917339\n",
      "1089 2018-05-10 15:44:01.537126\n",
      "1647 2018-05-10 15:44:02.290499\n",
      "1759 2018-05-10 15:44:02.919900\n",
      "1297 2018-05-10 15:44:03.580881\n",
      "1756 2018-05-10 15:44:04.426956\n",
      "1841 2018-05-10 15:44:05.092018\n",
      "2405 2018-05-10 15:44:05.792334\n",
      "2142 2018-05-10 15:44:06.654717\n",
      "827 2018-05-10 15:44:07.432108\n",
      "1673 2018-05-10 15:44:07.821701\n",
      "1325 2018-05-10 15:44:08.462128\n",
      "809 2018-05-10 15:44:09.319850\n",
      "1445 2018-05-10 15:44:09.914585\n",
      "1320 2018-05-10 15:44:10.487104\n",
      "1321 2018-05-10 15:44:11.338620\n",
      "2081 2018-05-10 15:44:11.878001\n",
      "1817 2018-05-10 15:44:13.130183\n",
      "5 2018-05-10 15:44:13.809984\n",
      "1322 2018-05-10 15:44:13.822751\n",
      "1510 2018-05-10 15:44:14.370623\n",
      "1548 2018-05-10 15:44:14.970972\n",
      "1370 2018-05-10 15:44:15.943233\n",
      "1385 2018-05-10 15:44:16.494023\n",
      "3135 2018-05-10 15:44:17.389501\n",
      "1231 2018-05-10 15:44:18.458220\n",
      "2413 2018-05-10 15:44:19.262931\n",
      "1555 2018-05-10 15:44:20.111563\n",
      "1865 2018-05-10 15:44:20.721964\n",
      "1455 2018-05-10 15:44:21.414347\n",
      "6 2018-05-10 15:44:21.989789\n",
      "7 2018-05-10 15:44:22.002639\n",
      "1311 2018-05-10 15:44:22.015692\n",
      "1258 2018-05-10 15:44:22.862775\n",
      "1205 2018-05-10 15:44:23.377016\n",
      "1410 2018-05-10 15:44:24.171060\n",
      "1526 2018-05-10 15:44:25.070340\n",
      "1616 2018-05-10 15:44:26.031717\n",
      "1610 2018-05-10 15:44:26.665529\n",
      "1585 2018-05-10 15:44:27.299412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1459 2018-05-10 15:44:27.910278\n",
      "1408 2018-05-10 15:44:28.834587\n",
      "1840 2018-05-10 15:44:29.731783\n",
      "1280 2018-05-10 15:44:30.426368\n",
      "1644 2018-05-10 15:44:31.256558\n",
      "1635 2018-05-10 15:44:31.884119\n",
      "1502 2018-05-10 15:44:32.511216\n",
      "1540 2018-05-10 15:44:33.101195\n",
      "1591 2018-05-10 15:44:33.705387\n",
      "1697 2018-05-10 15:44:34.322526\n",
      "1170 2018-05-10 15:44:34.964451\n",
      "1814 2018-05-10 15:44:35.739719\n",
      "1332 2018-05-10 15:44:36.421198\n",
      "1564 2018-05-10 15:44:37.279876\n",
      "2038 2018-05-10 15:44:37.884935\n",
      "1749 2018-05-10 15:44:38.642167\n",
      "1619 2018-05-10 15:44:39.315481\n",
      "1391 2018-05-10 15:44:39.952392\n",
      "1424 2018-05-10 15:44:40.514523\n",
      "1464 2018-05-10 15:44:41.421089\n",
      "1470 2018-05-10 15:44:41.999120\n",
      "1345 2018-05-10 15:44:42.576482\n",
      "1737 2018-05-10 15:44:43.122994\n",
      "1855 2018-05-10 15:44:43.782979\n",
      "1638 2018-05-10 15:44:44.472958\n",
      "1509 2018-05-10 15:44:45.099282\n",
      "1734 2018-05-10 15:44:45.691709\n",
      "1428 2018-05-10 15:44:46.347423\n",
      "1166 2018-05-10 15:44:46.926210\n",
      "1485 2018-05-10 15:44:47.765280\n",
      "1369 2018-05-10 15:44:48.352365\n",
      "1461 2018-05-10 15:44:49.227215\n",
      "1636 2018-05-10 15:44:49.813532\n",
      "1491 2018-05-10 15:44:50.445490\n",
      "1341 2018-05-10 15:44:51.034511\n",
      "1573 2018-05-10 15:44:51.587902\n",
      "1735 2018-05-10 15:44:52.208085\n",
      "1769 2018-05-10 15:44:52.869600\n",
      "1592 2018-05-10 15:44:53.534134\n",
      "1560 2018-05-10 15:44:54.148559\n",
      "1652 2018-05-10 15:44:54.734522\n",
      "1145 2018-05-10 15:44:55.370025\n",
      "1290 2018-05-10 15:44:55.854766\n",
      "1488 2018-05-10 15:44:56.689668\n",
      "1287 2018-05-10 15:44:57.274850\n",
      "1342 2018-05-10 15:44:57.809232\n",
      "1699 2018-05-10 15:44:58.352428\n",
      "1547 2018-05-10 15:44:58.996570\n",
      "1490 2018-05-10 15:44:59.595365\n",
      "1551 2018-05-10 15:45:00.181892\n",
      "1462 2018-05-10 15:45:00.790551\n",
      "1713 2018-05-10 15:45:01.373032\n",
      "2655 2018-05-10 15:45:02.021068\n",
      "1680 2018-05-10 15:45:02.940592\n",
      "1954 2018-05-10 15:45:03.593419\n",
      "1415 2018-05-10 15:45:04.313356\n",
      "1250 2018-05-10 15:45:04.881176\n",
      "1047 2018-05-10 15:45:05.403062\n",
      "1788 2018-05-10 15:45:05.862686\n",
      "1523 2018-05-10 15:45:06.541768\n",
      "1685 2018-05-10 15:45:07.156845\n",
      "3 2018-05-10 15:45:07.799371\n",
      "1500 2018-05-10 15:45:07.811497\n",
      "1334 2018-05-10 15:45:08.399907\n",
      "1531 2018-05-10 15:45:08.941417\n",
      "1354 2018-05-10 15:45:09.536416\n",
      "2489 2018-05-10 15:45:10.079936\n",
      "2028 2018-05-10 15:45:10.949560\n",
      "1714 2018-05-10 15:45:11.680350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-10 15:45:12.332078\n",
      "TP 7956 FP 1527 FN 11373 F 0.5522698875468555 Precision 0.8389750079088896 Recall 0.41160949868073876\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-10 16:00:17.883151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 2018-05-10 16:00:17.883638\n",
      "392 2018-05-10 16:00:18.263901\n",
      "77 2018-05-10 16:00:18.780843\n",
      "431 2018-05-10 16:00:19.546133\n",
      "61 2018-05-10 16:00:20.124870\n",
      "464 2018-05-10 16:00:20.761736\n",
      "178 2018-05-10 16:00:21.347919\n",
      "288 2018-05-10 16:00:21.822005\n",
      "155 2018-05-10 16:00:22.192226\n",
      "255 2018-05-10 16:00:22.612927\n",
      "91 2018-05-10 16:00:22.954089\n",
      "249 2018-05-10 16:00:23.638492\n",
      "59 2018-05-10 16:00:23.970351\n",
      "183 2018-05-10 16:00:24.734396\n",
      "28 2018-05-10 16:00:24.989045\n",
      "820 2018-05-10 16:00:25.210717\n",
      "103 2018-05-10 16:00:26.006185\n",
      "582 2018-05-10 16:00:27.496536\n",
      "66 2018-05-10 16:00:28.093379\n",
      "884 2018-05-10 16:00:28.744318\n",
      "60 2018-05-10 16:00:29.486594\n",
      "792 2018-05-10 16:00:30.025377\n",
      "69 2018-05-10 16:00:30.641998\n",
      "719 2018-05-10 16:00:31.320431\n",
      "75 2018-05-10 16:00:31.898167\n",
      "755 2018-05-10 16:00:32.517113\n",
      "32 2018-05-10 16:00:33.113319\n",
      "362 2018-05-10 16:00:33.300170\n",
      "160 2018-05-10 16:00:33.634209\n",
      "109 2018-05-10 16:00:33.965378\n",
      "65 2018-05-10 16:00:34.329949\n",
      "896 2018-05-10 16:00:34.962602\n",
      "45 2018-05-10 16:00:35.628669\n",
      "616 2018-05-10 16:00:35.926754\n",
      "99 2018-05-10 16:00:36.435674\n",
      "646 2018-05-10 16:00:36.881809\n",
      "81 2018-05-10 16:00:37.405688\n",
      "675 2018-05-10 16:00:37.966328\n",
      "37 2018-05-10 16:00:38.511732\n",
      "434 2018-05-10 16:00:38.770007\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e9d031d4530e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneominimodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneominimodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNeoMiniModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"neominitest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trained at\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch_%s_%s.h5\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from chemlistem import neominimodel\n",
    "nmm = neominimodel.NeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"neominitest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-10 16:21:04.876284\n",
      "Shufflesplit at 2018-05-10 16:21:04.877018\n",
      "Read annots at 2018-05-10 16:21:04.937804\n",
      "Read train seqs at 2018-05-10 16:21:05.267537\n",
      "Read test seqs at 2018-05-10 16:21:54.959123\n",
      "Corpus read at 2018-05-10 16:22:09.583687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-10 16:22:13.548343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-10 16:22:22.737312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "0 2018-05-10 16:22:22.737539\n",
      "200 2018-05-10 16:23:32.211228\n",
      "400 2018-05-10 16:24:40.548319\n",
      "600 2018-05-10 16:25:48.933128\n",
      "800 2018-05-10 16:27:02.732689\n",
      "1000 2018-05-10 16:28:14.259063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-10 16:28:26.996452\n",
      "TP 9882 FP 2929 FN 9447 F 0.614934660858743 Precision 0.7713683553196472 Recall 0.5112525221170262\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-10 16:44:18.382449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "0 2018-05-10 16:44:18.382609\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e9d031d4530e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneominimodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneominimodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNeoMiniModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"neominitest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0;31m#for size in sizes:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m#    if size == 1: continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from chemlistem import neominimodel\n",
    "nmm = neominimodel.NeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"neominitest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-11 10:39:18.202844\n",
      "Shufflesplit at 2018-05-11 10:39:18.202965\n",
      "Read annots at 2018-05-11 10:39:18.337370\n",
      "Read train seqs at 2018-05-11 10:39:18.619443\n",
      "Read test seqs at 2018-05-11 10:40:02.680193\n",
      "Corpus read at 2018-05-11 10:40:15.544944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-11 10:40:19.794347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-11 10:40:28.286850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "0 2018-05-11 10:40:28.287641\n",
      "200 2018-05-11 10:41:39.954173\n",
      "400 2018-05-11 10:42:44.690717\n",
      "600 2018-05-11 10:43:50.749109\n",
      "800 2018-05-11 10:45:02.719514\n",
      "1000 2018-05-11 10:46:10.774773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 10:46:23.927338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2018-05-11 10:46:24.099776\n",
      "1 2018-05-11 10:46:24.735605\n",
      "2 2018-05-11 10:46:24.929435\n",
      "3 2018-05-11 10:46:25.003205\n",
      "4 2018-05-11 10:46:25.646121\n",
      "5 2018-05-11 10:46:26.615669\n",
      "6 2018-05-11 10:46:27.245090\n",
      "7 2018-05-11 10:46:27.356868\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 17 is out of bounds for axis 0 with size 17",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e9d031d4530e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneominimodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneominimodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNeoMiniModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"neominitest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname)\u001b[0m\n\u001b[1;32m    186\u001b[0m                     \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0ments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"E\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ents\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                     \u001b[0mmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmmb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmmb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmmb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmmb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 17 is out of bounds for axis 0 with size 17"
     ]
    }
   ],
   "source": [
    "from chemlistem import neominimodel\n",
    "nmm = neominimodel.NeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"neominitest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-11 11:00:55.034691\n",
      "Shufflesplit at 2018-05-11 11:00:55.034849\n",
      "Read annots at 2018-05-11 11:00:55.096370\n",
      "Read train seqs at 2018-05-11 11:00:55.383040\n",
      "Read test seqs at 2018-05-11 11:01:39.345319\n",
      "Corpus read at 2018-05-11 11:01:52.190308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-11 11:01:56.588899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-11 11:02:03.959890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "0 2018-05-11 11:02:03.960794\n",
      "200 2018-05-11 11:03:11.432994\n",
      "400 2018-05-11 11:04:16.189312\n",
      "600 2018-05-11 11:05:22.343987\n",
      "800 2018-05-11 11:06:34.358138\n",
      "1000 2018-05-11 11:07:42.829047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 11:07:56.112599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2018-05-11 11:07:56.289550\n",
      "200 2018-05-11 11:09:51.334771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP 9908 FP 3250 FN 9421 F 0.6099670637485763 Precision 0.7530019759841922 Recall 0.5125976511976822\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-11 11:10:25.204407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "0 2018-05-11 11:10:25.204517\n",
      "200 2018-05-11 11:11:27.318059\n",
      "400 2018-05-11 11:12:33.011347\n",
      "600 2018-05-11 11:13:39.284597\n",
      "800 2018-05-11 11:14:50.826514\n",
      "1000 2018-05-11 11:16:00.126605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 11:16:12.584614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2018-05-11 11:16:12.618421\n",
      "200 2018-05-11 11:17:58.924946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP 13559 FP 3084 FN 5770 F 0.7538641165350829 Precision 0.8146968695547677 Recall 0.7014848155621087\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-11 11:18:32.346069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "0 2018-05-11 11:18:32.346178\n",
      "200 2018-05-11 11:19:36.025505\n",
      "400 2018-05-11 11:20:41.225869\n",
      "600 2018-05-11 11:21:48.354975\n",
      "800 2018-05-11 11:22:59.842624\n",
      "1000 2018-05-11 11:24:09.315354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 11:24:21.795097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2018-05-11 11:24:21.829254\n",
      "200 2018-05-11 11:26:03.682353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP 14904 FP 2842 FN 4425 F 0.8039919082939987 Precision 0.8398512340809197 Recall 0.7710693776191215\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-11 11:26:36.591785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "0 2018-05-11 11:26:36.591896\n",
      "200 2018-05-11 11:27:39.783011\n",
      "400 2018-05-11 11:28:44.750866\n",
      "600 2018-05-11 11:29:50.922068\n",
      "800 2018-05-11 11:31:02.305291\n",
      "1000 2018-05-11 11:32:11.302584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 11:32:23.687539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2018-05-11 11:32:23.719692\n",
      "200 2018-05-11 11:34:04.803256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP 15369 FP 2685 FN 3960 F 0.8222454056656769 Precision 0.851279494848787 Recall 0.7951264938693156\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-11 11:34:37.549489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "0 2018-05-11 11:34:37.549735\n",
      "200 2018-05-11 11:35:40.587698\n",
      "400 2018-05-11 11:36:45.515603\n",
      "600 2018-05-11 11:37:52.656747\n",
      "800 2018-05-11 11:39:03.733968\n",
      "1000 2018-05-11 11:40:12.512789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 11:40:24.959022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2018-05-11 11:40:24.991295\n",
      "200 2018-05-11 11:42:06.274467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP 15681 FP 2692 FN 3648 F 0.8318391597262744 Precision 0.853480650955206 Recall 0.8112680428371877\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-11 11:42:38.951022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "0 2018-05-11 11:42:38.951125\n",
      "200 2018-05-11 11:43:41.999809\n",
      "400 2018-05-11 11:44:46.780219\n",
      "600 2018-05-11 11:45:53.587128\n",
      "800 2018-05-11 11:47:04.669363\n",
      "1000 2018-05-11 11:48:13.354300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 11:48:25.711197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2018-05-11 11:48:25.744730\n",
      "200 2018-05-11 11:50:08.403610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP 15899 FP 2785 FN 3430 F 0.8365033015021177 Precision 0.8509419824448726 Recall 0.8225464328211496\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-11 11:50:41.060815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "0 2018-05-11 11:50:41.060917\n",
      "200 2018-05-11 11:51:44.000046\n",
      "400 2018-05-11 11:52:48.654148\n",
      "600 2018-05-11 11:53:55.546248\n",
      "800 2018-05-11 11:55:06.862342\n",
      "1000 2018-05-11 11:56:16.012577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 11:56:28.422213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2018-05-11 11:56:28.454995\n",
      "200 2018-05-11 11:58:09.999273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP 15718 FP 2631 FN 3611 F 0.8343330325388821 Precision 0.8566134394244918 Recall 0.8131822649904289\n",
      "Epoch 7 start at 2018-05-11 11:58:42.825929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "0 2018-05-11 11:58:42.826039\n",
      "200 2018-05-11 11:59:45.673545\n",
      "400 2018-05-11 12:00:50.424525\n",
      "600 2018-05-11 12:01:56.656060\n",
      "800 2018-05-11 12:03:07.956364\n",
      "1000 2018-05-11 12:04:16.983684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 12:04:29.417818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2018-05-11 12:04:29.449719\n",
      "200 2018-05-11 12:06:11.498079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP 15711 FP 2759 FN 3618 F 0.8312918331172783 Precision 0.8506226312939903 Recall 0.8128201148533292\n",
      "Epoch 8 start at 2018-05-11 12:06:44.431207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "0 2018-05-11 12:06:44.431313\n",
      "200 2018-05-11 12:07:47.507518\n",
      "400 2018-05-11 12:08:52.341965\n",
      "600 2018-05-11 12:09:59.212001\n",
      "800 2018-05-11 12:11:10.373282\n",
      "1000 2018-05-11 12:12:19.394747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 12:12:31.797027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2018-05-11 12:12:31.828774\n",
      "200 2018-05-11 12:14:15.234202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP 15792 FP 2878 FN 3537 F 0.8311797678886287 Precision 0.8458489555436529 Recall 0.8170107092969113\n",
      "Epoch 9 start at 2018-05-11 12:14:48.234583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "0 2018-05-11 12:14:48.235575\n",
      "200 2018-05-11 12:15:51.187262\n",
      "400 2018-05-11 12:16:56.037907\n",
      "600 2018-05-11 12:18:02.725228\n",
      "800 2018-05-11 12:19:13.613909\n",
      "1000 2018-05-11 12:20:22.321518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 12:20:34.738304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2018-05-11 12:20:34.768843\n",
      "200 2018-05-11 12:22:16.485725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP 15941 FP 2980 FN 3388 F 0.8335163398692811 Precision 0.8425030389514296 Recall 0.8247193336437477\n",
      "Epoch 10 start at 2018-05-11 12:22:49.550860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "0 2018-05-11 12:22:49.550967\n",
      "200 2018-05-11 12:23:52.374213\n",
      "400 2018-05-11 12:24:56.740579\n",
      "600 2018-05-11 12:26:03.241397\n",
      "800 2018-05-11 12:27:14.252423\n",
      "1000 2018-05-11 12:28:23.173676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 12:28:35.586779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2018-05-11 12:28:35.619350\n",
      "200 2018-05-11 12:30:18.967330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP 16268 FP 3125 FN 3061 F 0.8402458550694696 Precision 0.8388593822513278 Recall 0.8416369186196906\n",
      "Best so far\n",
      "Epoch 11 start at 2018-05-11 12:30:52.177181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "0 2018-05-11 12:30:52.177286\n",
      "200 2018-05-11 12:31:55.045281\n",
      "400 2018-05-11 12:32:59.527186\n",
      "600 2018-05-11 12:34:05.576861\n",
      "800 2018-05-11 12:35:16.629956\n",
      "1000 2018-05-11 12:36:25.227688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 12:36:37.595983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2018-05-11 12:36:37.627906\n",
      "200 2018-05-11 12:38:18.337650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP 16158 FP 2799 FN 3171 F 0.8440683278482997 Precision 0.8523500553885108 Recall 0.8359459878938382\n",
      "Best so far\n",
      "Epoch 12 start at 2018-05-11 12:38:51.142754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "0 2018-05-11 12:38:51.142858\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e9d031d4530e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneominimodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneominimodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNeoMiniModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"neominitest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;31m#if n % 200 == 0: print(n, datetime.now())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;31m#if n % 200 == 0: print(n, datetime.now())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;31m#if n % 200 == 0: print(n, datetime.now())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chemlistem/chemlistem/utils.py\u001b[0m in \u001b[0;36mtobits\u001b[0;34m(bit, bits)\u001b[0m\n\u001b[1;32m     16\u001b[0m \t\"\"\"\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbit\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msobie_scores_to_char_ents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from chemlistem import neominimodel\n",
    "nmm = neominimodel.NeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"neominitest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-11 12:40:09.124107\n",
      "Shufflesplit at 2018-05-11 12:40:09.124237\n",
      "Read annots at 2018-05-11 12:40:09.183135\n",
      "Read train seqs at 2018-05-11 12:40:09.487854\n",
      "Read test seqs at 2018-05-11 12:40:54.183401\n",
      "Corpus read at 2018-05-11 12:41:07.284350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-11 12:41:11.630894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-11 12:41:19.219113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 12:45:06.424171\n",
      "TP 4035 FP 1525 FN 15294 F 0.3242396239302503 Precision 0.7257194244604317 Recall 0.20875368617103834\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-11 12:46:58.598874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 12:50:41.138015\n",
      "TP 9057 FP 1584 FN 10272 F 0.6044044044044043 Precision 0.851141809980265 Recall 0.4685705416731336\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-11 12:52:40.955562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 12:56:22.929494\n",
      "TP 12284 FP 2082 FN 7045 F 0.7291289508829203 Precision 0.8550744814144507 Recall 0.635521754876093\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-11 12:58:24.618367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 13:02:05.557004\n",
      "TP 13821 FP 2335 FN 5508 F 0.7789770325489643 Precision 0.855471651398861 Recall 0.7150395778364116\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-11 13:04:07.867392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 13:07:48.690700\n",
      "TP 15327 FP 2815 FN 4002 F 0.8180726428437992 Precision 0.8448351890640503 Recall 0.7929535930467174\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-11 13:09:52.021733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 13:13:31.965299\n",
      "TP 15877 FP 3224 FN 3452 F 0.8262815508717148 Precision 0.8312130254960474 Recall 0.8214082466759791\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-11 13:15:36.246846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 13:19:15.798742\n",
      "TP 15551 FP 2844 FN 3778 F 0.8244618810306437 Precision 0.8453927697743953 Recall 0.8045423974339077\n",
      "Epoch 7 start at 2018-05-11 13:21:19.762436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-11 13:24:59.467035\n",
      "TP 16251 FP 3412 FN 3078 F 0.8335556011489537 Precision 0.8264761226669379 Recall 0.840757411143877\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-11 13:27:04.593028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4bf2ed23bd29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneominimodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneominimodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNeoMiniModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"neominitest_b64\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0;31m#model.fit(tx, ty, verbose=0, epochs=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from chemlistem import neominimodel\n",
    "nmm = neominimodel.NeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"neominitest_b64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-11 13:27:52.653601\n",
      "Shufflesplit at 2018-05-11 13:27:52.653735\n",
      "Read annots at 2018-05-11 13:27:52.711691\n",
      "Read train seqs at 2018-05-11 13:27:53.015696\n",
      "Read test seqs at 2018-05-11 13:28:37.650202\n",
      "Corpus read at 2018-05-11 13:28:50.421318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-11 13:28:54.747538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-11 13:29:02.111354\n",
      "Trained at 2018-05-11 13:39:19.280285\n",
      "TP 11698 FP 2759 FN 7631 F 0.6924761735630143 Precision 0.8091581932627793 Recall 0.6052046148274614\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-11 13:42:03.979993\n",
      "Trained at 2018-05-11 13:52:16.680499\n",
      "TP 14595 FP 2721 FN 4734 F 0.7965616045845272 Precision 0.8428620928620929 Recall 0.7550830358528636\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-11 13:55:00.284151\n",
      "Trained at 2018-05-11 14:05:10.867302\n",
      "TP 15377 FP 2491 FN 3952 F 0.8267871064870823 Precision 0.8605887620326841 Recall 0.7955403797402866\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-11 14:07:55.610889\n",
      "Trained at 2018-05-11 14:18:05.438245\n",
      "TP 15759 FP 2653 FN 3570 F 0.8351130070745343 Precision 0.855909189658918 Recall 0.8153034300791556\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-11 14:20:50.516455\n",
      "Trained at 2018-05-11 14:31:01.028486\n",
      "TP 15811 FP 2524 FN 3518 F 0.8395815632965166 Precision 0.8623397872920644 Recall 0.8179936882404677\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-11 14:33:46.202128\n",
      "Trained at 2018-05-11 14:43:57.433214\n",
      "TP 16135 FP 2760 FN 3194 F 0.8442339891167853 Precision 0.8539296110082032 Recall 0.8347560660147965\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-11 14:46:42.529023\n",
      "Trained at 2018-05-11 14:56:54.157276\n",
      "TP 16224 FP 2836 FN 3105 F 0.8452421266508635 Precision 0.8512067156348374 Recall 0.8393605463293496\n",
      "Best so far\n",
      "Epoch 7 start at 2018-05-11 14:59:41.294436\n",
      "Trained at 2018-05-11 15:09:52.536402\n",
      "TP 15737 FP 2404 FN 3592 F 0.8399786495863357 Precision 0.867482498208478 Recall 0.8141652439339851\n",
      "Epoch 8 start at 2018-05-11 15:12:37.085457\n",
      "Trained at 2018-05-11 15:22:47.674269\n",
      "TP 16014 FP 2774 FN 3315 F 0.8402550043287772 Precision 0.8523525654673195 Recall 0.8284960422163589\n",
      "Epoch 9 start at 2018-05-11 15:25:33.316524\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a5c4be7c6739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneominimodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneominimodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNeoMiniModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"neominitest_b16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;31m#if n % 200 == 0: print(n, datetime.now())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;31m#if n % 200 == 0: print(n, datetime.now())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;31m#if n % 200 == 0: print(n, datetime.now())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chemlistem/chemlistem/utils.py\u001b[0m in \u001b[0;36mtobits\u001b[0;34m(bit, bits)\u001b[0m\n\u001b[1;32m     16\u001b[0m \t\"\"\"\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbit\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msobie_scores_to_char_ents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chemlistem/chemlistem/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m \t\"\"\"\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbit\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msobie_scores_to_char_ents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from chemlistem import neominimodel\n",
    "nmm = neominimodel.NeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"neominitest_b16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-11 15:28:37.686241\n",
      "Shufflesplit at 2018-05-11 15:28:37.686934\n",
      "Read annots at 2018-05-11 15:28:37.743845\n",
      "Read train seqs at 2018-05-11 15:28:38.041546\n",
      "Read test seqs at 2018-05-11 15:29:22.229916\n",
      "Corpus read at 2018-05-11 15:29:35.062100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-11 15:29:39.503997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-11 15:29:46.964705\n",
      "Trained at 2018-05-11 15:35:40.115471\n",
      "TP 10043 FP 3763 FN 9286 F 0.6061868115285951 Precision 0.7274373460814139 Recall 0.5195819752703192\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-11 15:38:04.018415\n",
      "Trained at 2018-05-11 15:43:52.502654\n",
      "TP 13860 FP 3642 FN 5469 F 0.7526268632402052 Precision 0.7919094960575934 Recall 0.7170572714573956\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-11 15:46:07.945984\n",
      "Trained at 2018-05-11 15:51:57.076832\n",
      "TP 15431 FP 3860 FN 3898 F 0.7991196271361989 Precision 0.7999066922399046 Recall 0.7983341093693413\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-11 15:54:13.359297\n",
      "Trained at 2018-05-11 16:00:00.734968\n",
      "TP 15837 FP 3275 FN 3492 F 0.8239639967742775 Precision 0.8286416910841357 Recall 0.8193388173211237\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-11 16:02:12.948641\n",
      "Trained at 2018-05-11 16:08:00.514942\n",
      "TP 16174 FP 3317 FN 3155 F 0.8332818134981969 Precision 0.8298188907700991 Recall 0.8367737596357804\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-11 16:10:12.813889\n",
      "Trained at 2018-05-11 16:15:59.851429\n",
      "TP 16426 FP 3276 FN 2903 F 0.8416899387666213 Precision 0.8337224647243935 Recall 0.8498111645713694\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-11 16:18:11.807339\n",
      "Trained at 2018-05-11 16:23:59.021998\n",
      "TP 16554 FP 3281 FN 2775 F 0.8453681952813809 Precision 0.8345853289639527 Recall 0.8564333385069067\n",
      "Best so far\n",
      "Epoch 7 start at 2018-05-11 16:26:10.954592\n",
      "Trained at 2018-05-11 16:31:57.416584\n",
      "TP 16491 FP 3203 FN 2838 F 0.8451938600312636 Precision 0.8373616329846654 Recall 0.8531739872730094\n",
      "Epoch 8 start at 2018-05-11 16:34:07.361116\n",
      "Trained at 2018-05-11 16:39:54.224806\n",
      "TP 16597 FP 3198 FN 2732 F 0.8484306308148452 Precision 0.8384440515281637 Recall 0.8586579750633763\n",
      "Best so far\n",
      "Epoch 9 start at 2018-05-11 16:42:04.646085\n",
      "Trained at 2018-05-11 16:47:51.396284\n",
      "TP 16701 FP 3194 FN 2628 F 0.851570467060983 Precision 0.8394571500376979 Recall 0.8640384913860003\n",
      "Best so far\n",
      "Epoch 10 start at 2018-05-11 16:50:04.779567\n",
      "Trained at 2018-05-11 16:55:51.359378\n"
     ]
    }
   ],
   "source": [
    "from chemlistem import neominimodel\n",
    "nmm = neominimodel.NeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"neominitest_b32d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-14 09:35:46.972188\n",
      "Shufflesplit at 2018-05-14 09:35:46.972824\n",
      "Read annots at 2018-05-14 09:35:47.102003\n",
      "Read train seqs at 2018-05-14 09:35:47.424255\n",
      "Read test seqs at 2018-05-14 09:36:36.193916\n",
      "Corpus read at 2018-05-14 09:36:50.545362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-14 09:36:55.253794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-14 09:37:05.804037\n",
      "Trained at 2018-05-14 09:43:15.231686\n",
      "TP 9695 FP 3555 FN 9634 F 0.5951686669326867 Precision 0.7316981132075472 Recall 0.5015779398830772\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-14 09:45:34.924286\n",
      "Trained at 2018-05-14 09:51:34.804651\n",
      "TP 13840 FP 3605 FN 5489 F 0.7527057159949965 Precision 0.7933505302378905 Recall 0.716022556779968\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-14 09:53:51.230889\n",
      "Trained at 2018-05-14 09:59:50.573654\n",
      "TP 15264 FP 3642 FN 4065 F 0.7984307571596705 Precision 0.8073627419866709 Recall 0.7896942418128201\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-14 10:02:05.810787\n",
      "Trained at 2018-05-14 10:08:03.865639\n",
      "TP 16061 FP 3626 FN 3268 F 0.8233032602009432 Precision 0.8158175445725606 Recall 0.8309276217083139\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-14 10:10:16.892052\n",
      "Trained at 2018-05-14 10:16:15.975877\n",
      "TP 16288 FP 3209 FN 3041 F 0.839025395353629 Precision 0.8354105759860492 Recall 0.8426716332971184\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-14 10:18:28.982794\n",
      "Trained at 2018-05-14 10:24:26.326328\n",
      "TP 16333 FP 3167 FN 2996 F 0.841278425918772 Precision 0.8375897435897436 Recall 0.8449997413213306\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-14 10:26:39.771361\n",
      "Trained at 2018-05-14 10:32:37.593770\n",
      "TP 16494 FP 3137 FN 2835 F 0.8467145790554415 Precision 0.8402017217665937 Recall 0.8533291944746236\n",
      "Best so far\n",
      "Epoch 7 start at 2018-05-14 10:34:49.547965\n",
      "Trained at 2018-05-14 10:40:48.183708\n",
      "TP 16529 FP 3176 FN 2800 F 0.8469027002100733 Precision 0.8388226338492768 Recall 0.8551399451601222\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-14 10:42:59.854304\n",
      "Trained at 2018-05-14 10:48:57.952613\n",
      "TP 16373 FP 2856 FN 2956 F 0.8492660407697494 Precision 0.8514743356388788 Recall 0.847069170676186\n",
      "Best so far\n",
      "Epoch 9 start at 2018-05-14 10:51:11.656113\n",
      "Trained at 2018-05-14 10:57:09.446006\n",
      "TP 16777 FP 3153 FN 2552 F 0.8546830026236022 Precision 0.8417962870045158 Recall 0.8679704071602256\n",
      "Best so far\n",
      "Epoch 10 start at 2018-05-14 10:59:22.533968\n",
      "Trained at 2018-05-14 11:05:21.047200\n",
      "TP 16716 FP 3174 FN 2613 F 0.8524439684846631 Precision 0.840422322775264 Recall 0.864814527394071\n",
      "Epoch 11 start at 2018-05-14 11:07:33.657291\n",
      "Trained at 2018-05-14 11:13:31.739176\n",
      "TP 16364 FP 2881 FN 2965 F 0.8484471405609997 Precision 0.8502987789036113 Recall 0.8466035490713436\n",
      "Epoch 12 start at 2018-05-14 11:15:45.001336\n",
      "Trained at 2018-05-14 11:21:44.362478\n",
      "TP 16751 FP 3051 FN 2578 F 0.8561498556131967 Precision 0.8459246540753459 Recall 0.8666252780795696\n",
      "Best so far\n",
      "Epoch 13 start at 2018-05-14 11:23:56.777777\n",
      "Trained at 2018-05-14 11:29:55.678788\n",
      "TP 16635 FP 3085 FN 2694 F 0.8520064534303056 Precision 0.8435598377281948 Recall 0.8606239329504889\n",
      "Epoch 14 start at 2018-05-14 11:32:10.036511\n",
      "Trained at 2018-05-14 11:38:08.833746\n",
      "TP 16385 FP 2743 FN 2944 F 0.8521205502249265 Precision 0.8565976578837307 Recall 0.8476899994826427\n",
      "Epoch 15 start at 2018-05-14 11:40:22.164113\n",
      "Trained at 2018-05-14 11:46:19.951124\n",
      "TP 16522 FP 2816 FN 2807 F 0.8545788398375876 Precision 0.8543799772468714 Recall 0.8547777950230224\n",
      "Epoch 16 start at 2018-05-14 11:48:32.818724\n",
      "Trained at 2018-05-14 11:54:31.296908\n",
      "TP 16764 FP 2997 FN 2565 F 0.8577129700690713 Precision 0.8483376347350843 Recall 0.8672978426198976\n",
      "Best so far\n",
      "Epoch 17 start at 2018-05-14 11:56:44.357755\n",
      "Trained at 2018-05-14 12:02:43.944751\n",
      "TP 16715 FP 2986 FN 2614 F 0.8565206251601333 Precision 0.8484340896401198 Recall 0.8647627916601996\n",
      "Epoch 18 start at 2018-05-14 12:05:01.505503\n",
      "Trained at 2018-05-14 12:10:59.624646\n",
      "TP 16620 FP 2897 FN 2709 F 0.8556865571744838 Precision 0.8515653020443715 Recall 0.8598478969424181\n",
      "Epoch 19 start at 2018-05-14 12:13:14.036145\n",
      "Trained at 2018-05-14 12:19:12.277991\n",
      "TP 16849 FP 3164 FN 2480 F 0.8565400843881856 Precision 0.8419027632039174 Recall 0.8716953799989653\n",
      "Epoch 20 start at 2018-05-14 12:21:26.595279\n",
      "Trained at 2018-05-14 12:27:25.064328\n",
      "TP 16542 FP 2796 FN 2787 F 0.8556133136783304 Precision 0.8554142103630158 Recall 0.8558125097004501\n",
      "Epoch 21 start at 2018-05-14 12:29:42.304005\n",
      "Trained at 2018-05-14 12:35:40.909851\n",
      "TP 16837 FP 2961 FN 2492 F 0.8606333222582871 Precision 0.8504394383271038 Recall 0.8710745511925087\n",
      "Best so far\n",
      "Epoch 22 start at 2018-05-14 12:37:54.313029\n",
      "Trained at 2018-05-14 12:43:53.418129\n",
      "TP 16748 FP 3054 FN 2581 F 0.8559965244946461 Precision 0.8457731542268457 Recall 0.8664700708779554\n",
      "Epoch 23 start at 2018-05-14 12:46:05.538554\n",
      "Trained at 2018-05-14 12:52:03.549061\n",
      "TP 16680 FP 2818 FN 2649 F 0.8591959203646947 Precision 0.8554723561390912 Recall 0.8629520409747012\n",
      "Epoch 24 start at 2018-05-14 12:54:16.489029\n",
      "Trained at 2018-05-14 13:00:15.088093\n",
      "TP 16663 FP 2929 FN 2666 F 0.856247270111251 Precision 0.8505002041649653 Recall 0.8620725334988877\n",
      "Epoch 25 start at 2018-05-14 13:02:28.387986\n",
      "Trained at 2018-05-14 13:08:26.575176\n",
      "TP 16651 FP 2969 FN 2678 F 0.8550155331330714 Precision 0.8486748216106015 Recall 0.861451704692431\n",
      "Epoch 26 start at 2018-05-14 13:10:40.475410\n",
      "Trained at 2018-05-14 13:16:38.833136\n",
      "TP 16702 FP 2902 FN 2627 F 0.8579867978321732 Precision 0.8519689859212406 Recall 0.8640902271198717\n",
      "Epoch 27 start at 2018-05-14 13:18:52.134454\n",
      "Trained at 2018-05-14 13:24:51.404406\n",
      "TP 16619 FP 3006 FN 2710 F 0.8532628228166556 Precision 0.846828025477707 Recall 0.8597961612085467\n",
      "Epoch 28 start at 2018-05-14 13:27:06.387733\n",
      "Trained at 2018-05-14 13:33:04.803656\n",
      "TP 16891 FP 3190 FN 2438 F 0.8571936056838366 Precision 0.8411433693541158 Recall 0.8738682808215634\n",
      "Epoch 29 start at 2018-05-14 13:35:17.219736\n",
      "Trained at 2018-05-14 13:41:15.664088\n",
      "TP 16693 FP 2941 FN 2636 F 0.8568642045017067 Precision 0.8502088214322094 Recall 0.8636246055150292\n"
     ]
    }
   ],
   "source": [
    "# Baseline for un-monkeyed-with, proper batches, CUDNNLSTM and dropout only\n",
    "\n",
    "from chemlistem import neominimodel\n",
    "nmm = neominimodel.NeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"neominitest_b32d\")\n",
    "# Run time a little longer than 4h\n",
    "# Best - TP 16837 FP 2961 FN 2492 F 0.8606333222582871 Precision 0.8504394383271038 Recall 0.8710745511925087\n",
    "# c.f. 0.8664 from paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-14 16:28:37.882404\n",
      "Shufflesplit at 2018-05-14 16:28:37.882539\n",
      "Read annots at 2018-05-14 16:28:37.949109\n",
      "Read train seqs at 2018-05-14 16:28:38.277620\n",
      "Read test seqs at 2018-05-14 16:29:26.898678\n",
      "Corpus read at 2018-05-14 16:29:41.191807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-14 16:29:45.916537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "0 [9.021686, 4.51123, 4.5104556]\n",
      "100 [5.6181917, 2.653276, 2.9649155]\n",
      "200 [5.3153033, 2.3907044, 2.924599]\n",
      "300 [5.6636024, 2.374536, 3.2890663]\n",
      "400 [4.9849772, 2.0310314, 2.9539459]\n",
      "500 [4.8945336, 1.8819913, 3.0125422]\n",
      "600 [4.696561, 1.6785787, 3.017982]\n",
      "700 [4.4414797, 1.6110613, 2.8304186]\n",
      "800 [4.3171806, 1.44188, 2.8753004]\n",
      "900 [4.457017, 1.5382507, 2.9187663]\n",
      "1000 [4.2775145, 1.3193585, 2.958156]\n",
      "1100 [4.1448436, 1.2733483, 2.8714952]\n",
      "1200 [4.1494985, 1.217874, 2.9316244]\n",
      "1300 [4.2999015, 1.2928488, 3.0070527]\n",
      "1400 [4.2066345, 1.2824017, 2.924233]\n",
      "1500 [4.0160017, 1.1157148, 2.900287]\n",
      "1600 [3.9208903, 1.1563586, 2.7645316]\n",
      "1700 [4.100525, 1.2481909, 2.8523343]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-14 16:32:06.340244\n",
      "Trained at 2018-05-14 16:38:06.600352\n",
      "TP 12511 FP 3445 FN 6818 F 0.7091398611307921 Precision 0.7840937578340437 Recall 0.6472657664648973\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-14 16:40:21.676709\n",
      "Trained at 2018-05-14 16:46:18.897553\n",
      "TP 14270 FP 2865 FN 5059 F 0.7826897762176394 Precision 0.8327983659177123 Recall 0.7382689223446635\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-14 16:48:35.969440\n",
      "Trained at 2018-05-14 16:54:32.747610\n",
      "TP 15900 FP 3170 FN 3429 F 0.8281465663168311 Precision 0.8337703198741478 Recall 0.822598168555021\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-14 16:56:47.958004\n",
      "Trained at 2018-05-14 17:02:42.835447\n",
      "TP 15818 FP 2804 FN 3511 F 0.8336012226291797 Precision 0.8494254108044249 Recall 0.8183558383775674\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-14 17:04:57.893609\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-060fd7729ae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneominimodel_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneominimodel_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPredNeoMiniModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"neominitest_b32d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../catted_pattitle_blah.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel_pred.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname, unsupfile)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;31m#if n % 200 == 0: print(n, datetime.now())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel_pred.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;31m#if n % 200 == 0: print(n, datetime.now())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel_pred.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;31m#if n % 200 == 0: print(n, datetime.now())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chemlistem/chemlistem/utils.py\u001b[0m in \u001b[0;36mtobits\u001b[0;34m(bit, bits)\u001b[0m\n\u001b[1;32m     16\u001b[0m \t\"\"\"\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbit\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msobie_scores_to_char_ents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chemlistem/chemlistem/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m \t\"\"\"\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbit\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msobie_scores_to_char_ents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from chemlistem import neominimodel_pred\n",
    "nmm = neominimodel_pred.PredNeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"neominitest_b32d\", \"../catted_pattitle_blah.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-15 09:43:28.865591\n",
      "Shufflesplit at 2018-05-15 09:43:28.866113\n",
      "Read annots at 2018-05-15 09:43:28.924591\n",
      "Read train seqs at 2018-05-15 09:43:29.222409\n",
      "Read test seqs at 2018-05-15 09:44:13.812264\n",
      "Corpus read at 2018-05-15 09:44:26.718834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-15 09:44:31.042135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "0 9.021065 [9.021065, 4.5111785, 4.5098867]\n",
      "100 6.2756853 [5.774021, 2.7059596, 3.0680618]\n",
      "200 5.665429 [5.549868, 2.4758081, 3.07406]\n",
      "300 5.3840265 [5.468137, 2.3081405, 3.1599963]\n",
      "400 5.173281 [5.136941, 2.038292, 3.0986493]\n",
      "500 4.9933786 [4.930211, 1.9313505, 2.9988604]\n",
      "600 4.8721633 [5.4334126, 2.1985137, 3.2348988]\n",
      "700 4.762485 [4.6634383, 1.6351056, 3.0283327]\n",
      "800 4.671583 [4.6038914, 1.6060369, 2.9978547]\n",
      "900 4.6480513 [4.6801987, 1.6331882, 3.0470107]\n",
      "1000 4.5405307 [4.453121, 1.4139895, 3.0391319]\n",
      "1100 4.4944363 [4.5566235, 1.5598145, 2.996809]\n",
      "1200 4.4622974 [4.433712, 1.4461931, 2.9875188]\n",
      "1300 4.396305 [4.4584017, 1.4063821, 3.0520198]\n",
      "1400 4.415447 [4.082164, 1.1856389, 2.8965251]\n",
      "1500 4.317213 [4.184849, 1.3314238, 2.853425]\n",
      "1600 4.325774 [4.4012794, 1.4197993, 2.9814804]\n",
      "1700 4.2421017 [4.341096, 1.3236942, 3.0174017]\n",
      "1800 4.293669 [4.090337, 1.2202241, 2.870113]\n",
      "1900 4.219013 [3.8784199, 1.0594864, 2.8189335]\n",
      "2000 4.2541256 [4.0416517, 1.1718516, 2.8698003]\n",
      "2100 4.190452 [3.926553, 1.1305349, 2.7960181]\n",
      "2200 4.1966047 [3.9610379, 1.0843172, 2.8767207]\n",
      "2300 4.2039304 [4.1638207, 1.1510397, 3.012781]\n",
      "2400 4.128075 [4.144702, 1.1999545, 2.9447474]\n",
      "2500 4.131003 [4.3488855, 1.3008229, 3.0480626]\n",
      "2600 4.120268 [3.9033394, 1.0579022, 2.8454373]\n",
      "2700 4.093625 [3.900841, 0.94844407, 2.9523969]\n",
      "2800 4.115778 [4.074785, 1.1317534, 2.9430318]\n",
      "2900 4.117872 [4.644645, 1.5180416, 3.1266036]\n",
      "3000 4.0817966 [4.3252645, 1.3253955, 2.9998689]\n",
      "3100 4.088181 [4.2262745, 1.216263, 3.0100112]\n",
      "3200 4.1035323 [3.902578, 0.98180985, 2.9207683]\n",
      "3300 4.094934 [4.287848, 1.2666415, 3.0212064]\n",
      "3400 4.010211 [3.9293768, 1.031584, 2.8977928]\n",
      "3500 4.030976 [4.0064926, 0.99927974, 3.007213]\n",
      "3600 4.0127287 [3.694519, 0.9474183, 2.7471008]\n",
      "3700 4.050807 [4.1750193, 1.1510273, 3.0239918]\n",
      "3800 4.0055346 [4.5481114, 1.4084481, 3.1396632]\n",
      "3900 3.990906 [3.9854593, 1.1797959, 2.8056633]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-15 09:59:35.460940\n",
      "Trained at 2018-05-15 10:05:24.112048\n",
      "TP 13586 FP 3875 FN 5743 F 0.7385702636586029 Precision 0.7780768569955901 Recall 0.7028816803766361\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-15 10:07:33.298985\n",
      "Trained at 2018-05-15 10:13:16.921157\n",
      "TP 15226 FP 2988 FN 4103 F 0.8111232453453373 Precision 0.8359503678489074 Recall 0.7877282839257075\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-15 10:15:23.800673\n",
      "Trained at 2018-05-15 10:21:06.662717\n",
      "TP 15825 FP 2983 FN 3504 F 0.8299027191441383 Precision 0.8413972777541472 Recall 0.818717988514667\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-15 10:23:13.320361\n",
      "Trained at 2018-05-15 10:28:57.552856\n",
      "TP 16375 FP 3129 FN 2954 F 0.8433548785826488 Precision 0.8395713699753896 Recall 0.8471726421439288\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-15 10:31:04.689496\n",
      "Trained at 2018-05-15 10:36:48.083901\n",
      "TP 16398 FP 2999 FN 2931 F 0.8468729019263543 Precision 0.8453884621333196 Recall 0.8483625640229706\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-15 10:38:55.082931\n",
      "Trained at 2018-05-15 10:44:38.764682\n",
      "TP 16735 FP 3376 FN 2594 F 0.848630831643002 Precision 0.8321316692357417 Recall 0.8657975063376274\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-15 10:46:45.124277\n",
      "Trained at 2018-05-15 10:52:28.460127\n",
      "TP 16718 FP 3087 FN 2611 F 0.8543977104308275 Precision 0.8441302701338046 Recall 0.8649179988618139\n",
      "Best so far\n",
      "Epoch 7 start at 2018-05-15 10:54:35.077645\n",
      "Trained at 2018-05-15 11:00:18.392125\n",
      "TP 16701 FP 3084 FN 2628 F 0.8539653321061512 Precision 0.8441243366186505 Recall 0.8640384913860003\n",
      "Epoch 8 start at 2018-05-15 11:02:23.206494\n",
      "Trained at 2018-05-15 11:08:06.155592\n",
      "TP 16575 FP 2952 FN 2754 F 0.8531500926497838 Precision 0.848824704255646 Recall 0.8575197889182058\n",
      "Epoch 9 start at 2018-05-15 11:10:11.593474\n",
      "Trained at 2018-05-15 11:15:54.857698\n",
      "TP 16838 FP 3097 FN 2491 F 0.857681336593317 Precision 0.8446450965638325 Recall 0.8711262869263801\n",
      "Best so far\n",
      "Epoch 10 start at 2018-05-15 11:18:00.974234\n",
      "Trained at 2018-05-15 11:23:44.148066\n",
      "TP 16739 FP 2949 FN 2590 F 0.858036240613066 Precision 0.8502133279154815 Recall 0.8660044492731129\n",
      "Best so far\n",
      "Epoch 11 start at 2018-05-15 11:25:50.575472\n",
      "Trained at 2018-05-15 11:31:33.861708\n",
      "TP 16940 FP 3225 FN 2389 F 0.857851825593761 Precision 0.8400694272253906 Recall 0.8764033317812613\n",
      "Epoch 12 start at 2018-05-15 11:33:39.255930\n",
      "Trained at 2018-05-15 11:39:22.575052\n",
      "TP 16717 FP 3013 FN 2612 F 0.8559870964438414 Precision 0.8472883933096806 Recall 0.8648662631279425\n",
      "Epoch 13 start at 2018-05-15 11:41:30.442007\n",
      "Trained at 2018-05-15 11:47:14.620721\n",
      "TP 17028 FP 3239 FN 2301 F 0.8600868774623699 Precision 0.8401835496126708 Recall 0.8809560763619432\n",
      "Best so far\n",
      "Epoch 14 start at 2018-05-15 11:49:22.499675\n",
      "Trained at 2018-05-15 11:55:06.189650\n",
      "TP 16781 FP 3097 FN 2548 F 0.8560206085647971 Precision 0.8441996176677734 Recall 0.8681773500957111\n",
      "Epoch 15 start at 2018-05-15 11:57:12.162577\n",
      "Trained at 2018-05-15 12:02:55.998104\n",
      "TP 16855 FP 3085 FN 2474 F 0.858437953602078 Precision 0.8452858575727181 Recall 0.8720057944021936\n",
      "Epoch 16 start at 2018-05-15 12:05:03.392447\n",
      "Trained at 2018-05-15 12:10:46.828342\n",
      "TP 16955 FP 3195 FN 2374 F 0.8589376630613744 Precision 0.841439205955335 Recall 0.8771793677893321\n",
      "Epoch 17 start at 2018-05-15 12:12:54.101430\n",
      "Trained at 2018-05-15 12:18:37.051385\n",
      "TP 16911 FP 3143 FN 2418 F 0.8587969428433588 Precision 0.8432731624613543 Recall 0.8749029954989912\n",
      "Epoch 18 start at 2018-05-15 12:20:46.557713\n",
      "Trained at 2018-05-15 12:26:30.155948\n",
      "TP 16851 FP 3053 FN 2478 F 0.8590217419009507 Precision 0.8466137459807074 Recall 0.8717988514667081\n",
      "Epoch 19 start at 2018-05-15 12:28:38.902133\n",
      "Trained at 2018-05-15 12:34:21.992497\n",
      "TP 16875 FP 3130 FN 2454 F 0.8580363044694158 Precision 0.8435391152211947 Recall 0.8730405090796213\n",
      "Epoch 20 start at 2018-05-15 12:36:29.025862\n",
      "Trained at 2018-05-15 12:42:12.714694\n",
      "TP 16954 FP 3272 FN 2375 F 0.8572367589432436 Precision 0.8382280233362999 Recall 0.8771276320554607\n",
      "Epoch 21 start at 2018-05-15 12:44:20.817283\n",
      "Trained at 2018-05-15 12:50:04.311397\n",
      "TP 16620 FP 2857 FN 2709 F 0.8565685718703293 Precision 0.8533141654258869 Recall 0.8598478969424181\n",
      "Epoch 22 start at 2018-05-15 12:52:10.424521\n",
      "Trained at 2018-05-15 12:57:53.884628\n",
      "TP 16784 FP 2980 FN 2545 F 0.8586703501905713 Precision 0.8492208055049585 Recall 0.8683325572973253\n",
      "Epoch 23 start at 2018-05-15 12:59:59.913659\n",
      "Trained at 2018-05-15 13:05:43.320981\n",
      "TP 16849 FP 3094 FN 2480 F 0.8580668160521491 Precision 0.8448578448578449 Recall 0.8716953799989653\n",
      "Epoch 24 start at 2018-05-15 13:07:51.415545\n",
      "Trained at 2018-05-15 13:13:34.466245\n",
      "TP 16753 FP 2872 FN 2576 F 0.8601427324536634 Precision 0.853656050955414 Recall 0.8667287495473124\n",
      "Best so far\n",
      "Epoch 25 start at 2018-05-15 13:15:41.638354\n",
      "Trained at 2018-05-15 13:21:24.782850\n",
      "TP 17007 FP 3206 FN 2322 F 0.8601992817763391 Precision 0.8413892049671003 Recall 0.8798696259506441\n",
      "Best so far\n",
      "Epoch 26 start at 2018-05-15 13:23:33.397749\n",
      "Trained at 2018-05-15 13:29:16.343437\n",
      "TP 16836 FP 2984 FN 2493 F 0.8600985976653299 Precision 0.8494450050454087 Recall 0.8710228154586372\n",
      "Epoch 27 start at 2018-05-15 13:31:23.816492\n",
      "Trained at 2018-05-15 13:37:07.024425\n",
      "TP 16913 FP 2953 FN 2416 F 0.8630182421227197 Precision 0.8513540722843048 Recall 0.8750064669667339\n",
      "Best so far\n",
      "Epoch 28 start at 2018-05-15 13:39:14.413094\n",
      "Trained at 2018-05-15 13:44:57.454744\n",
      "TP 16782 FP 2949 FN 2547 F 0.8592933947772657 Precision 0.8505397597688916 Recall 0.8682290858295825\n",
      "Epoch 29 start at 2018-05-15 13:47:05.274229\n",
      "Trained at 2018-05-15 13:52:48.270218\n",
      "TP 17050 FP 3190 FN 2279 F 0.8617857413631883 Precision 0.842391304347826 Recall 0.8820942625071136\n"
     ]
    }
   ],
   "source": [
    "from chemlistem import neominimodel_pred\n",
    "nmm = neominimodel_pred.PredNeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"neominitest_b32d\", \"../catted_patabs_a61k31a61_noct_s.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-15 14:55:31.378455\n",
      "Shufflesplit at 2018-05-15 14:55:31.378911\n",
      "Read annots at 2018-05-15 14:55:31.437333\n",
      "Read train seqs at 2018-05-15 14:55:31.730812\n",
      "Read test seqs at 2018-05-15 14:56:15.730888\n",
      "Corpus read at 2018-05-15 14:56:28.546270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-15 14:56:33.081658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "0 9.022087 [9.022087, 4.5104046, 4.5116825] 2018-05-15 14:56:44.650918\n",
      "100 6.200254 [5.47212, 2.725273, 2.746847] 2018-05-15 14:57:08.049252\n",
      "200 5.1722646 [4.9315195, 2.4677782, 2.4637415] 2018-05-15 14:57:28.527680\n",
      "300 4.6170373 [4.608303, 2.2947712, 2.3135319] 2018-05-15 14:57:52.862187\n",
      "400 4.1950974 [3.9944856, 1.9863813, 2.0081043] 2018-05-15 14:58:16.560860\n",
      "500 3.8625333 [3.8028307, 1.8747596, 1.9280713] 2018-05-15 14:58:37.425545\n",
      "600 3.6146808 [4.297882, 2.1377115, 2.1601708] 2018-05-15 14:59:02.017796\n",
      "700 3.4166389 [3.1680627, 1.5664179, 1.6016446] 2018-05-15 14:59:21.294046\n",
      "800 3.24423 [3.138722, 1.5503459, 1.588376] 2018-05-15 14:59:40.779033\n",
      "900 3.1808655 [3.1726155, 1.5786971, 1.5939186] 2018-05-15 15:00:04.071251\n",
      "1000 3.0100718 [2.7530885, 1.3606968, 1.3923918] 2018-05-15 15:00:26.010550\n",
      "1100 2.9352415 [3.072249, 1.521148, 1.5511011] 2018-05-15 15:00:48.623571\n",
      "1200 2.8672483 [2.7974706, 1.3781765, 1.4192941] 2018-05-15 15:01:12.492976\n",
      "1300 2.7740195 [2.7669003, 1.368804, 1.3980963] 2018-05-15 15:01:36.070383\n",
      "1400 2.7883117 [2.3638554, 1.1693475, 1.1945077] 2018-05-15 15:02:02.651801\n",
      "1500 2.6589713 [2.6360464, 1.3045418, 1.3315046] 2018-05-15 15:02:23.804284\n",
      "1600 2.6512785 [2.821667, 1.3930455, 1.4286214] 2018-05-15 15:02:46.327153\n",
      "1700 2.5372958 [2.592628, 1.2840023, 1.3086257] 2018-05-15 15:03:04.919551\n",
      "1800 2.5822744 [2.4338498, 1.1950119, 1.2388378] 2018-05-15 15:03:30.661967\n",
      "1900 2.4804604 [2.0831666, 1.0305498, 1.0526168] 2018-05-15 15:03:51.811725\n",
      "2000 2.5329952 [2.3532972, 1.1559465, 1.1973507] 2018-05-15 15:04:18.497982\n",
      "2100 2.4296966 [2.2962642, 1.1102287, 1.1860355] 2018-05-15 15:04:38.488578\n",
      "2200 2.4553733 [2.1480541, 1.0579278, 1.0901263] 2018-05-15 15:05:03.286452\n",
      "2300 2.431075 [2.2616575, 1.1302419, 1.1314156] 2018-05-15 15:05:30.036210\n",
      "2400 2.3312454 [2.3818386, 1.198797, 1.1830416] 2018-05-15 15:05:51.272511\n",
      "2500 2.3369977 [2.5509918, 1.2677836, 1.2832081] 2018-05-15 15:06:12.230931\n",
      "2600 2.3297465 [2.1160066, 1.051137, 1.0648696] 2018-05-15 15:06:34.764534\n",
      "2700 2.2856312 [1.8731273, 0.9280038, 0.94512355] 2018-05-15 15:06:53.113056\n",
      "2800 2.3133674 [2.247322, 1.1185377, 1.1287844] 2018-05-15 15:07:17.527603\n",
      "2900 2.2918603 [2.9911718, 1.4920421, 1.4991297] 2018-05-15 15:07:44.984187\n",
      "3000 2.243118 [2.6522887, 1.3241209, 1.3281678] 2018-05-15 15:08:07.628795\n",
      "3100 2.2385583 [2.4227273, 1.1969424, 1.2257849] 2018-05-15 15:08:32.832888\n",
      "3200 2.265207 [1.9546986, 0.9603228, 0.99437577] 2018-05-15 15:09:00.066485\n",
      "3300 2.247604 [2.5006366, 1.2381289, 1.2625077] 2018-05-15 15:09:28.322747\n",
      "3400 2.1491575 [2.0254674, 0.99389744, 1.0315701] 2018-05-15 15:09:46.102431\n",
      "3500 2.1566408 [1.9686131, 0.98037916, 0.9882339] 2018-05-15 15:10:07.210310\n",
      "3600 2.1407654 [1.8862476, 0.9212409, 0.9650067] 2018-05-15 15:10:27.375250\n",
      "3700 2.191361 [2.2653534, 1.1286159, 1.1367376] 2018-05-15 15:10:53.017776\n",
      "3800 2.149945 [2.7965658, 1.3893051, 1.4072607] 2018-05-15 15:11:12.455817\n",
      "3900 2.1514425 [2.4196572, 1.1839573, 1.2357] 2018-05-15 15:11:32.412424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-15 15:11:38.028171\n",
      "Trained at 2018-05-15 15:17:25.730787\n",
      "TP 14371 FP 2919 FN 4958 F 0.784893088287501 Precision 0.831174089068826 Recall 0.7434942314656734\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-15 15:19:35.785330\n",
      "Trained at 2018-05-15 15:25:19.645663\n",
      "TP 15405 FP 2679 FN 3924 F 0.8235105444631545 Precision 0.8518579960185799 Recall 0.7969889802886854\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-15 15:27:32.153696\n",
      "Trained at 2018-05-15 15:33:15.393557\n",
      "TP 16029 FP 2877 FN 3300 F 0.8384464495880738 Precision 0.8478260869565217 Recall 0.8292720782244296\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-15 15:35:28.288571\n",
      "Trained at 2018-05-15 15:41:12.132199\n",
      "TP 16522 FP 2954 FN 2807 F 0.8515397500322124 Precision 0.8483261449989731 Recall 0.8547777950230224\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-15 15:43:25.047432\n",
      "Trained at 2018-05-15 15:49:09.634324\n",
      "TP 16382 FP 2867 FN 2947 F 0.8492923427860438 Precision 0.8510571977765079 Recall 0.8475347922810285\n",
      "Epoch 5 start at 2018-05-15 15:51:21.432198\n",
      "Trained at 2018-05-15 15:57:05.576940\n",
      "TP 16383 FP 2772 FN 2946 F 0.8514187714374805 Precision 0.8552858261550509 Recall 0.8475865280148999\n",
      "Epoch 6 start at 2018-05-15 15:59:17.428225\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-34afd23606b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneominimodel_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneominimodel_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPredNeoMiniModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../catted_patabs_a61k31a61_noct_s.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel_pred.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname, unsupfile)\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0;31m#model.fit(tx, ty, verbose=0, epochs=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get things the right way round this time\n",
    "from chemlistem import neominimodel_pred\n",
    "nmm = neominimodel_pred.PredNeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patabs_a61k31a61_noct_s.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-16 09:25:24.596086\n",
      "Shufflesplit at 2018-05-16 09:25:24.596210\n",
      "Read annots at 2018-05-16 09:25:24.664987\n",
      "Read train seqs at 2018-05-16 09:25:24.887979\n",
      "Read test seqs at 2018-05-16 09:26:13.588163\n",
      "Corpus read at 2018-05-16 09:26:26.842850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-16 09:26:30.401141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.022741 [9.022741, 4.512315, 4.510426] 2018-05-16 09:26:34.922747\n",
      "100 6.2145805 [5.481135, 2.737206, 2.743929] 2018-05-16 09:26:57.728049\n",
      "200 5.1951075 [4.955016, 2.4803019, 2.474714] 2018-05-16 09:27:19.929318\n",
      "300 4.6383314 [4.5986476, 2.2940598, 2.3045878] 2018-05-16 09:27:43.294883\n",
      "400 4.203955 [3.9969354, 1.9906096, 2.0063257] 2018-05-16 09:28:07.807775\n",
      "500 3.8588233 [3.7862008, 1.8801621, 1.9060386] 2018-05-16 09:28:28.508182\n",
      "600 3.6076376 [4.30256, 2.1570134, 2.1455464] 2018-05-16 09:28:51.023391\n",
      "700 3.4110937 [3.1637669, 1.5725343, 1.5912325] 2018-05-16 09:29:10.443423\n",
      "800 3.2414455 [3.138119, 1.5575473, 1.5805717] 2018-05-16 09:29:30.341046\n",
      "900 3.1797736 [3.1881905, 1.5908809, 1.5973096] 2018-05-16 09:29:55.332490\n",
      "1000 3.0115092 [2.7543464, 1.368304, 1.3860422] 2018-05-16 09:30:16.496965\n",
      "1100 2.9400196 [3.07547, 1.5327297, 1.5427403] 2018-05-16 09:30:40.878831\n",
      "1200 2.873942 [2.7835755, 1.3891454, 1.3944302] 2018-05-16 09:31:04.355937\n",
      "1300 2.7800174 [2.7694237, 1.3692958, 1.4001279] 2018-05-16 09:31:28.924253\n",
      "1400 2.7973151 [2.3693244, 1.177578, 1.1917465] 2018-05-16 09:31:56.510439\n",
      "1500 2.6623611 [2.6374876, 1.3114378, 1.3260498] 2018-05-16 09:32:17.074426\n",
      "1600 2.6603518 [2.8445778, 1.4083956, 1.4361821] 2018-05-16 09:32:40.580851\n",
      "1700 2.5413356 [2.6147635, 1.3034387, 1.3113248] 2018-05-16 09:32:59.390952\n",
      "1800 2.5823133 [2.427526, 1.2061251, 1.221401] 2018-05-16 09:33:26.154770\n",
      "1900 2.4856248 [2.1093998, 1.0459288, 1.0634708] 2018-05-16 09:33:47.710394\n",
      "2000 2.534858 [2.350825, 1.1631584, 1.1876667] 2018-05-16 09:34:14.546052\n",
      "2100 2.4313035 [2.318956, 1.1247842, 1.1941715] 2018-05-16 09:34:34.812376\n",
      "2200 2.456722 [2.1435933, 1.0677757, 1.0758176] 2018-05-16 09:35:00.442715\n",
      "2300 2.4346483 [2.2684484, 1.1266698, 1.1417787] 2018-05-16 09:35:28.277351\n",
      "2400 2.3342674 [2.3743129, 1.1715678, 1.2027452] 2018-05-16 09:35:49.117104\n",
      "2500 2.337249 [2.5688927, 1.2739449, 1.2949479] 2018-05-16 09:36:11.941364\n",
      "2600 2.3311105 [2.0985866, 1.0404167, 1.05817] 2018-05-16 09:36:33.848393\n",
      "2700 2.2854202 [1.8800774, 0.92603374, 0.95404357] 2018-05-16 09:36:53.913651\n",
      "2800 2.3184295 [2.2788665, 1.1248732, 1.1539934] 2018-05-16 09:37:17.662979\n",
      "2900 2.2939003 [2.9995341, 1.5001109, 1.4994234] 2018-05-16 09:37:47.836279\n",
      "3000 2.2484043 [2.660515, 1.3269323, 1.3335828] 2018-05-16 09:38:10.256633\n",
      "3100 2.2429776 [2.44473, 1.212646, 1.232084] 2018-05-16 09:38:36.413981\n",
      "3200 2.26934 [1.9182767, 0.9480072, 0.97026944] 2018-05-16 09:39:03.896256\n",
      "3300 2.2496054 [2.5059428, 1.2422862, 1.2636566] 2018-05-16 09:39:33.214794\n",
      "3400 2.1554978 [2.035573, 1.0050989, 1.0304742] 2018-05-16 09:39:51.177645\n",
      "3500 2.1595051 [1.9825106, 0.98728853, 0.99522203] 2018-05-16 09:40:14.126100\n",
      "3600 2.145428 [1.891855, 0.9250677, 0.9667872] 2018-05-16 09:40:33.654041\n",
      "3700 2.1971712 [2.2791624, 1.137367, 1.1417954] 2018-05-16 09:41:00.189430\n",
      "3800 2.1557035 [2.791061, 1.3919034, 1.3991574] 2018-05-16 09:41:19.891191\n",
      "3900 2.1574385 [2.416996, 1.1592064, 1.2577897] 2018-05-16 09:41:40.182435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-16 09:41:46.679975\n",
      "Trained at 2018-05-16 09:47:44.048170\n",
      "TP 14877 FP 3041 FN 4452 F 0.7988294359277257 Precision 0.8302823975890167 Recall 0.7696725128045941\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-16 09:49:56.867321\n",
      "Trained at 2018-05-16 09:55:49.547577\n",
      "TP 15612 FP 2771 FN 3717 F 0.8279592702588036 Precision 0.8492629059457107 Recall 0.807698277200062\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-16 09:58:01.898365\n",
      "Trained at 2018-05-16 10:03:56.176803\n",
      "TP 16318 FP 3090 FN 3011 F 0.842502000671193 Precision 0.8407873042044518 Recall 0.8442237053132599\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-16 10:06:08.885771\n",
      "Trained at 2018-05-16 10:12:00.815185\n",
      "TP 16413 FP 2984 FN 2916 F 0.8476475752724268 Precision 0.8461617775944734 Recall 0.8491386000310415\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-16 10:14:13.538293\n",
      "Trained at 2018-05-16 10:20:06.043472\n",
      "TP 16637 FP 3084 FN 2692 F 0.8520870678617157 Precision 0.8436184777648192 Recall 0.8607274044182317\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-16 10:22:19.455938\n",
      "Trained at 2018-05-16 10:28:10.035433\n",
      "TP 16734 FP 3110 FN 2595 F 0.854363975186991 Precision 0.843277565007055 Recall 0.865745770603756\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-16 10:30:21.350218\n",
      "Trained at 2018-05-16 10:36:14.053296\n",
      "TP 16827 FP 3168 FN 2502 F 0.8558132438205676 Precision 0.8415603900975244 Recall 0.8705571938537948\n",
      "Best so far\n",
      "Epoch 7 start at 2018-05-16 10:38:27.137073\n",
      "Trained at 2018-05-16 10:44:18.884263\n",
      "TP 16811 FP 3020 FN 2518 F 0.8585801838610827 Precision 0.8477131763400736 Recall 0.8697294221118527\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-16 10:46:30.576590\n",
      "Trained at 2018-05-16 10:52:23.702831\n",
      "TP 16791 FP 3080 FN 2538 F 0.8566836734693878 Precision 0.8450002516229681 Recall 0.868694707434425\n",
      "Epoch 9 start at 2018-05-16 10:54:37.446120\n",
      "Trained at 2018-05-16 11:00:29.844180\n",
      "TP 17049 FP 3189 FN 2280 F 0.8617787550231253 Precision 0.8424251408241921 Recall 0.8820425267732422\n",
      "Best so far\n",
      "Epoch 10 start at 2018-05-16 11:02:39.854244\n",
      "Trained at 2018-05-16 11:08:31.955672\n",
      "TP 16584 FP 2862 FN 2745 F 0.8553965183752418 Precision 0.8528232027152114 Recall 0.8579854105230482\n",
      "Epoch 11 start at 2018-05-16 11:10:45.308528\n",
      "Trained at 2018-05-16 11:16:38.176378\n",
      "TP 16927 FP 3115 FN 2402 F 0.8598714790073912 Precision 0.8445763895818781 Recall 0.8757307672409333\n",
      "Epoch 12 start at 2018-05-16 11:18:50.962221\n",
      "Trained at 2018-05-16 11:24:42.444902\n",
      "TP 16864 FP 3028 FN 2465 F 0.8599474771168507 Precision 0.8477780012065151 Recall 0.872471416007036\n",
      "Epoch 13 start at 2018-05-16 11:26:56.383597\n",
      "Trained at 2018-05-16 11:32:49.930621\n",
      "TP 16838 FP 2982 FN 2491 F 0.8602007714117857 Precision 0.8495459132189708 Recall 0.8711262869263801\n",
      "Epoch 14 start at 2018-05-16 11:35:02.850716\n",
      "Trained at 2018-05-16 11:40:56.164703\n",
      "TP 16849 FP 3072 FN 2480 F 0.8585477707006369 Precision 0.8457908739521108 Recall 0.8716953799989653\n",
      "Epoch 15 start at 2018-05-16 11:43:10.982280\n",
      "Trained at 2018-05-16 11:49:02.779253\n",
      "TP 17034 FP 3338 FN 2295 F 0.8581144051787108 Precision 0.8361476536422541 Recall 0.8812664907651715\n",
      "Epoch 16 start at 2018-05-16 11:51:15.692035\n",
      "Trained at 2018-05-16 11:57:08.484273\n",
      "TP 16721 FP 3003 FN 2608 F 0.8563234578649528 Precision 0.8477489353072399 Recall 0.865073206063428\n",
      "Epoch 17 start at 2018-05-16 11:59:23.570654\n",
      "Trained at 2018-05-16 12:05:14.859865\n",
      "TP 17127 FP 3384 FN 2202 F 0.859789156626506 Precision 0.8350153576129882 Recall 0.8860779140152103\n",
      "Epoch 18 start at 2018-05-16 12:07:28.926787\n",
      "Trained at 2018-05-16 12:13:22.389885\n",
      "TP 17099 FP 3365 FN 2230 F 0.8593973814489986 Precision 0.8355648944487881 Recall 0.8846293134668115\n",
      "Epoch 19 start at 2018-05-16 12:15:37.058752\n",
      "Trained at 2018-05-16 12:21:30.204172\n",
      "TP 16963 FP 2922 FN 2366 F 0.8651502014586627 Precision 0.8530550666331406 Recall 0.8775932536603032\n",
      "Best so far\n",
      "Epoch 20 start at 2018-05-16 12:23:40.607243\n",
      "Trained at 2018-05-16 12:29:31.758625\n",
      "TP 16927 FP 3177 FN 2402 F 0.8585195141125453 Precision 0.8419717469160366 Recall 0.8757307672409333\n",
      "Epoch 21 start at 2018-05-16 12:31:42.691690\n",
      "Trained at 2018-05-16 12:37:36.059581\n",
      "TP 16965 FP 3273 FN 2364 F 0.8575327924785806 Precision 0.8382745330566261 Recall 0.877696725128046\n",
      "Epoch 22 start at 2018-05-16 12:39:49.646360\n",
      "Trained at 2018-05-16 12:45:41.566362\n",
      "TP 16947 FP 3175 FN 2382 F 0.859141720108489 Precision 0.8422125037272636 Recall 0.876765481918361\n",
      "Epoch 23 start at 2018-05-16 12:47:53.286073\n",
      "Trained at 2018-05-16 12:53:46.345866\n",
      "TP 17024 FP 3231 FN 2305 F 0.8601455133387227 Precision 0.8404838311528018 Recall 0.8807491334264577\n",
      "Epoch 24 start at 2018-05-16 12:55:56.110479\n",
      "Trained at 2018-05-16 13:01:48.994760\n",
      "TP 16970 FP 3202 FN 2359 F 0.8592187539555961 Precision 0.8412651199682728 Recall 0.8779554037974029\n",
      "Epoch 25 start at 2018-05-16 13:04:00.944073\n",
      "Trained at 2018-05-16 13:09:52.620940\n",
      "TP 17013 FP 3121 FN 2316 F 0.8622253756683476 Precision 0.8449885765372007 Recall 0.8801800403538724\n",
      "Epoch 26 start at 2018-05-16 13:12:05.620395\n",
      "Trained at 2018-05-16 13:17:58.249938\n",
      "TP 16979 FP 3214 FN 2350 F 0.8592176509285967 Precision 0.8408359332441935 Recall 0.8784210254022453\n",
      "Epoch 27 start at 2018-05-16 13:20:09.389699\n",
      "Trained at 2018-05-16 13:26:00.738058\n",
      "TP 17012 FP 3255 FN 2317 F 0.8592787150217194 Precision 0.8393940889130113 Recall 0.880128304620001\n",
      "Epoch 28 start at 2018-05-16 13:28:12.528291\n",
      "Trained at 2018-05-16 13:34:05.761418\n",
      "TP 17000 FP 3288 FN 2329 F 0.8582174319105434 Precision 0.8379337539432177 Recall 0.8795074758135444\n",
      "Epoch 29 start at 2018-05-16 13:36:17.961063\n",
      "Trained at 2018-05-16 13:42:11.042488\n",
      "TP 16971 FP 3302 FN 2358 F 0.8570779253573052 Precision 0.8371232673999901 Recall 0.8780071395312743\n"
     ]
    }
   ],
   "source": [
    "# get things the right way round this time - see if we can get a full run\n",
    "from chemlistem import neominimodel_pred\n",
    "nmm = neominimodel_pred.PredNeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patabs_a61k31a61_noct_s.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-17 10:22:23.582415\n",
      "Shufflesplit at 2018-05-17 10:22:23.582526\n",
      "Read annots at 2018-05-17 10:22:23.639061\n",
      "Read train seqs at 2018-05-17 10:22:23.917396\n",
      "Read test seqs at 2018-05-17 10:23:06.897746\n",
      "Corpus read at 2018-05-17 10:23:19.476993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-17 10:23:24.092882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "3931 batches of unsupervised\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-17 10:23:32.738636\n",
      "Trained at 2018-05-17 10:33:28.463708\n",
      "TP 11913 FP 3269 FN 7416 F 0.6903885717597288 Precision 0.7846792253984982 Recall 0.616327797609809\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-17 10:35:52.878134\n",
      "Trained at 2018-05-17 10:45:37.182359\n",
      "TP 14707 FP 2972 FN 4622 F 0.7948011240812797 Precision 0.8318909440579219 Recall 0.7608774380464587\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-17 10:47:54.335286\n",
      "Trained at 2018-05-17 10:57:39.244534\n",
      "TP 15859 FP 3368 FN 3470 F 0.8226475775495383 Precision 0.8248296666146565 Recall 0.8204770034662942\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-17 10:59:57.210276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Unsupervised 2018-05-17 11:07:01.428521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-17 11:08:26.351734\n",
      "TP 16053 FP 3094 FN 3276 F 0.8344422497141075 Precision 0.8384081057084661 Recall 0.8305137358373429\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-17 11:10:44.744341\n",
      "Trained at 2018-05-17 11:16:28.943627\n",
      "TP 16572 FP 3133 FN 2757 F 0.8491059076702362 Precision 0.8410048211113931 Recall 0.8573645817165917\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-17 11:18:47.648377\n",
      "Trained at 2018-05-17 11:24:31.944732\n",
      "TP 16414 FP 2923 FN 2915 F 0.8490146381834169 Precision 0.8488390132905829 Recall 0.8491903357649129\n",
      "Epoch 6 start at 2018-05-17 11:26:50.815304\n",
      "Trained at 2018-05-17 11:32:35.293504\n",
      "TP 16717 FP 3023 FN 2612 F 0.855768000204766 Precision 0.8468591691995947 Recall 0.8648662631279425\n",
      "Best so far\n",
      "Epoch 7 start at 2018-05-17 11:34:53.986894\n",
      "Trained at 2018-05-17 11:40:38.191727\n",
      "TP 16787 FP 3013 FN 2542 F 0.8580336834572824 Precision 0.8478282828282828 Recall 0.8684877644989394\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-17 11:42:56.699946\n",
      "Trained at 2018-05-17 11:48:40.640640\n",
      "TP 16701 FP 2891 FN 2628 F 0.8581999434752448 Precision 0.8524397713352388 Recall 0.8640384913860003\n",
      "Best so far\n",
      "Epoch 9 start at 2018-05-17 11:50:58.300080\n",
      "Trained at 2018-05-17 11:56:42.958889\n",
      "TP 16983 FP 3135 FN 2346 F 0.8610540725530459 Precision 0.8441694005368326 Recall 0.8786279683377308\n",
      "Best so far\n",
      "Epoch 10 start at 2018-05-17 11:59:01.576441\n",
      "Trained at 2018-05-17 12:04:45.845390\n",
      "TP 16903 FP 3063 FN 2426 F 0.8603130169232727 Precision 0.8465892016427927 Recall 0.8744891096280201\n",
      "Epoch 11 start at 2018-05-17 12:07:04.265529\n",
      "Trained at 2018-05-17 12:12:48.783297\n",
      "TP 16992 FP 3006 FN 2337 F 0.8641391410481348 Precision 0.8496849684968497 Recall 0.8790935899425734\n",
      "Best so far\n",
      "Epoch 12 start at 2018-05-17 12:15:06.055809\n",
      "Trained at 2018-05-17 12:20:50.563899\n",
      "TP 17017 FP 3119 FN 2312 F 0.8623843912327379 Precision 0.84510329757648 Recall 0.8803869832893579\n",
      "Epoch 13 start at 2018-05-17 12:23:09.436253\n",
      "Trained at 2018-05-17 12:28:54.554233\n",
      "TP 16963 FP 3098 FN 2366 F 0.8612845899974613 Precision 0.8455710084243059 Recall 0.8775932536603032\n",
      "Epoch 14 start at 2018-05-17 12:31:13.455472\n",
      "Trained at 2018-05-17 12:36:58.264189\n",
      "TP 16966 FP 3019 FN 2363 F 0.8631022027776365 Precision 0.8489367025268951 Recall 0.8777484608619174\n",
      "Epoch 15 start at 2018-05-17 12:39:17.269267\n",
      "Trained at 2018-05-17 12:45:01.663219\n",
      "TP 16916 FP 2945 FN 2413 F 0.8632814493493238 Precision 0.8517194501787423 Recall 0.8751616741683481\n",
      "Epoch 16 start at 2018-05-17 12:47:21.341798\n",
      "Trained at 2018-05-17 12:53:06.492494\n",
      "TP 17063 FP 3196 FN 2266 F 0.8620288976457512 Precision 0.8422429537489511 Recall 0.8827668270474417\n",
      "Epoch 17 start at 2018-05-17 12:55:25.281907\n",
      "Trained at 2018-05-17 13:01:10.073936\n",
      "TP 16902 FP 3022 FN 2427 F 0.8611825847705907 Precision 0.8483236297932142 Recall 0.8744373738941487\n",
      "Epoch 18 start at 2018-05-17 13:03:28.699636\n",
      "Trained at 2018-05-17 13:09:12.544225\n",
      "TP 17009 FP 3108 FN 2320 F 0.8623941591035846 Precision 0.8455038027538897 Recall 0.8799730974183869\n",
      "Epoch 19 start at 2018-05-17 13:11:31.086194\n",
      "Trained at 2018-05-17 13:17:15.508745\n",
      "TP 17210 FP 3422 FN 2119 F 0.8613398063111534 Precision 0.8341411399767352 Recall 0.8903719799265353\n",
      "Epoch 20 start at 2018-05-17 13:19:31.556986\n",
      "Trained at 2018-05-17 13:25:16.283148\n",
      "TP 16999 FP 3077 FN 2330 F 0.8627839106712346 Precision 0.8467324168160988 Recall 0.879455740079673\n",
      "Epoch 21 start at 2018-05-17 13:27:35.114789\n",
      "Trained at 2018-05-17 13:33:20.116197\n",
      "TP 16880 FP 3049 FN 2449 F 0.8599521116715064 Precision 0.8470068744041347 Recall 0.8732991877489782\n",
      "Epoch 22 start at 2018-05-17 13:35:37.859864\n",
      "Trained at 2018-05-17 13:41:22.500026\n",
      "TP 16998 FP 3178 FN 2331 F 0.8605492975572713 Precision 0.8424861221252974 Recall 0.8794040043458017\n",
      "Epoch 23 start at 2018-05-17 13:43:42.170189\n",
      "Trained at 2018-05-17 13:49:26.551851\n",
      "TP 16863 FP 2955 FN 2466 F 0.861521955705418 Precision 0.850893127459885 Recall 0.8724196802731646\n",
      "Epoch 24 start at 2018-05-17 13:51:43.252771\n",
      "Trained at 2018-05-17 13:57:27.938519\n",
      "TP 16857 FP 2907 FN 2472 F 0.8624050341493362 Precision 0.8529143897996357 Recall 0.8721092658699363\n",
      "Epoch 25 start at 2018-05-17 13:59:46.586898\n",
      "Trained at 2018-05-17 14:05:31.624665\n",
      "TP 16924 FP 3056 FN 2405 F 0.8610750718664937 Precision 0.847047047047047 Recall 0.8755755600393191\n",
      "Epoch 26 start at 2018-05-17 14:07:47.901118\n",
      "Trained at 2018-05-17 14:13:32.503704\n",
      "TP 16976 FP 3200 FN 2353 F 0.8594355144918365 Precision 0.8413957176843775 Recall 0.8782658182006312\n",
      "Epoch 27 start at 2018-05-17 14:15:49.278278\n",
      "Trained at 2018-05-17 14:21:33.632830\n",
      "TP 16908 FP 3178 FN 2421 F 0.8579474819231258 Precision 0.8417803445185702 Recall 0.874747788297377\n",
      "Epoch 28 start at 2018-05-17 14:23:49.003353\n",
      "Trained at 2018-05-17 14:29:33.338694\n",
      "TP 17060 FP 3178 FN 2269 F 0.8623347739277681 Precision 0.8429686727937543 Recall 0.8826116198458275\n",
      "Epoch 29 start at 2018-05-17 14:31:51.074276\n",
      "Trained at 2018-05-17 14:37:36.352722\n",
      "TP 17110 FP 3229 FN 2219 F 0.8626600786528184 Precision 0.8412409656325286 Recall 0.8851984065393967\n"
     ]
    }
   ],
   "source": [
    "# interleave pretrain then train\n",
    "from chemlistem import neominimodel_pred\n",
    "nmm = neominimodel_pred.PredNeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patabs_a61k31a61_noct_s.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-17 15:35:30.364357\n",
      "Shufflesplit at 2018-05-17 15:35:30.364739\n",
      "Read annots at 2018-05-17 15:35:30.432808\n",
      "Read train seqs at 2018-05-17 15:35:30.715515\n",
      "Read test seqs at 2018-05-17 15:36:14.569009\n",
      "Corpus read at 2018-05-17 15:36:27.318422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-17 15:36:31.675992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "20000 batches of unsupervised\n",
      "0 9.021326 [9.021326, 4.5096493, 4.511677] 2018-05-17 15:36:59.289714\n",
      "100 6.9603705 [6.49965, 3.242784, 3.2568662] 2018-05-17 15:37:31.975749\n",
      "200 5.9906335 [5.5622673, 2.779823, 2.7824445] 2018-05-17 15:38:08.155286\n",
      "300 5.3108215 [5.1459, 2.565291, 2.5806088] 2018-05-17 15:38:40.866793\n",
      "400 4.868067 [5.022194, 2.499531, 2.5226626] 2018-05-17 15:39:14.165310\n",
      "500 4.5708785 [4.6987486, 2.350043, 2.3487053] 2018-05-17 15:39:57.059172\n",
      "600 4.323669 [4.467352, 2.234909, 2.2324429] 2018-05-17 15:40:35.934806\n",
      "700 4.2011223 [4.9083033, 2.4502525, 2.4580507] 2018-05-17 15:41:02.427276\n",
      "800 4.057597 [5.3188744, 2.639546, 2.6793284] 2018-05-17 15:41:37.006397\n",
      "900 4.001573 [4.2801495, 2.1420865, 2.138063] 2018-05-17 15:42:14.366497\n",
      "1000 3.7754474 [4.261237, 2.1454182, 2.115819] 2018-05-17 15:42:40.711665\n",
      "1100 3.7543814 [3.8587933, 1.9433613, 1.915432] 2018-05-17 15:43:14.565832\n",
      "1200 3.6962605 [3.2569814, 1.6419351, 1.6150464] 2018-05-17 15:43:42.694496\n",
      "1300 3.6545112 [3.0103276, 1.5076692, 1.5026584] 2018-05-17 15:44:14.740280\n",
      "1400 3.5538638 [3.837297, 1.9317726, 1.9055243] 2018-05-17 15:44:57.221872\n",
      "1500 3.5669796 [3.3955646, 1.6922894, 1.7032751] 2018-05-17 15:45:47.765898\n",
      "1600 3.4444244 [3.3134096, 1.6492522, 1.6641574] 2018-05-17 15:46:21.794567\n",
      "1700 3.3943326 [3.7094982, 1.8657811, 1.8437171] 2018-05-17 15:46:50.892791\n",
      "1800 3.3454952 [3.6761413, 1.8459265, 1.8302147] 2018-05-17 15:47:28.154344\n",
      "1900 3.2152648 [3.2216935, 1.6109054, 1.610788] 2018-05-17 15:48:01.379511\n",
      "2000 3.2622943 [7.5826817, 3.6003563, 3.9823253] 2018-05-17 15:48:34.576295\n",
      "2100 3.1939178 [3.1468458, 1.5745844, 1.5722613] 2018-05-17 15:49:06.136438\n",
      "2200 3.1727839 [3.4430175, 1.7287753, 1.7142423] 2018-05-17 15:49:36.245962\n",
      "2300 3.149276 [2.6754818, 1.3324305, 1.3430512] 2018-05-17 15:50:07.793280\n",
      "2400 3.1384144 [2.9950085, 1.4921964, 1.5028121] 2018-05-17 15:50:42.351119\n",
      "2500 3.0265894 [3.0461514, 1.521292, 1.5248594] 2018-05-17 15:51:15.523831\n",
      "2600 3.0459406 [1.4992517, 0.8409872, 0.6582645] 2018-05-17 15:51:44.533276\n",
      "2700 2.9401486 [2.5736895, 1.278669, 1.2950203] 2018-05-17 15:52:11.825845\n",
      "2800 2.9969501 [3.3055916, 1.6526221, 1.6529694] 2018-05-17 15:52:45.740779\n",
      "2900 2.9175637 [3.054078, 1.5218468, 1.5322313] 2018-05-17 15:53:12.018559\n",
      "3000 3.02436 [2.836111, 1.4086943, 1.4274167] 2018-05-17 15:54:04.143613\n",
      "3100 2.8319223 [2.7119422, 1.3364307, 1.3755116] 2018-05-17 15:54:35.773758\n",
      "3200 2.9198554 [3.0225973, 1.5079143, 1.514683] 2018-05-17 15:55:09.415344\n",
      "3300 2.84214 [3.5406914, 1.7651516, 1.7755396] 2018-05-17 15:55:40.530119\n",
      "3400 2.9204485 [2.55164, 1.3531494, 1.1984906] 2018-05-17 15:56:14.780023\n",
      "3500 2.8010788 [3.078069, 1.5421062, 1.5359628] 2018-05-17 15:56:45.903273\n",
      "3600 2.926931 [3.207583, 1.6042727, 1.6033101] 2018-05-17 15:57:23.161901\n",
      "3700 2.871112 [3.1145937, 1.5488706, 1.5657232] 2018-05-17 15:58:09.605877\n",
      "Skipped batch, length 17015\n",
      "3800 2.883167 [2.781001, 1.3825481, 1.3984529] 2018-05-17 15:58:53.844555\n",
      "3900 2.8199165 [2.947638, 1.4752133, 1.4724246] 2018-05-17 15:59:32.255038\n",
      "4000 2.806938 [2.9431157, 1.4595418, 1.4835739] 2018-05-17 16:00:00.799949\n",
      "4100 2.905362 [3.0016813, 1.5031846, 1.4984967] 2018-05-17 16:00:45.007671\n",
      "4200 2.7400522 [2.877716, 1.419573, 1.458143] 2018-05-17 16:01:14.925091\n",
      "4300 2.8224509 [1.6430378, 0.9111756, 0.73186225] 2018-05-17 16:01:49.936490\n",
      "Skipped batch, length 27480\n",
      "4400 2.7617106 [3.3738337, 1.6687634, 1.7050701] 2018-05-17 16:02:21.001781\n",
      "4500 2.7305558 [3.1188583, 1.5609899, 1.5578685] 2018-05-17 16:02:56.884521\n",
      "4600 2.8270092 [2.1998405, 1.093696, 1.1061447] 2018-05-17 16:03:27.399529\n",
      "4700 2.6557112 [2.0371122, 1.0173252, 1.0197871] 2018-05-17 16:03:58.402149\n",
      "4800 2.7756486 [2.7711015, 1.3865597, 1.3845417] 2018-05-17 16:04:37.661666\n",
      "4900 2.6896505 [2.6387777, 1.3121326, 1.3266453] 2018-05-17 16:05:08.590127\n",
      "5000 2.7484884 [3.1078815, 1.5472969, 1.5605845] 2018-05-17 16:05:43.598718\n",
      "5100 2.729476 [2.6730995, 1.3374949, 1.3356048] 2018-05-17 16:06:15.961964\n",
      "5200 2.6694577 [1.111001, 0.6619434, 0.44905767] 2018-05-17 16:06:41.191081\n",
      "5300 2.7385697 [3.0094328, 1.4976379, 1.5117948] 2018-05-17 16:07:16.028154\n",
      "5400 2.7631326 [3.1661398, 1.5866055, 1.5795343] 2018-05-17 16:07:59.362265\n",
      "5500 2.638215 [2.7698479, 1.3885357, 1.381312] 2018-05-17 16:08:30.700175\n",
      "5600 2.626869 [2.7094798, 1.35234, 1.3571398] 2018-05-17 16:09:00.452430\n",
      "5700 2.6595645 [2.5424526, 1.2761381, 1.2663145] 2018-05-17 16:09:31.147178\n",
      "5800 2.6552982 [3.0642285, 1.5314384, 1.5327902] 2018-05-17 16:10:12.739151\n",
      "5900 2.6453767 [2.5434382, 1.2652318, 1.2782063] 2018-05-17 16:10:43.877545\n",
      "6000 2.5761046 [2.706851, 1.3508896, 1.3559613] 2018-05-17 16:11:15.797186\n",
      "6100 2.5972984 [2.8259254, 1.4052238, 1.4207015] 2018-05-17 16:11:47.472527\n",
      "6200 2.593992 [3.260179, 1.6274143, 1.6327646] 2018-05-17 16:12:16.449858\n",
      "6300 2.6443229 [2.7970574, 1.401607, 1.3954504] 2018-05-17 16:12:50.390503\n",
      "6400 2.6579132 [1.1283383, 0.6811817, 0.44715664] 2018-05-17 16:13:31.659554\n",
      "6500 2.4940863 [2.7177627, 1.35449, 1.3632727] 2018-05-17 16:14:04.167851\n",
      "6600 2.5398965 [2.0536957, 1.0181015, 1.0355943] 2018-05-17 16:14:34.669318\n",
      "6700 2.5509472 [3.648552, 1.7948171, 1.8537347] 2018-05-17 16:15:06.577898\n",
      "6800 2.6347811 [2.4188495, 1.2126734, 1.2061759] 2018-05-17 16:15:43.629819\n",
      "6900 2.6372004 [3.0788798, 1.5285358, 1.550344] 2018-05-17 16:16:14.680897\n",
      "7000 2.506433 [2.4799614, 1.241698, 1.2382634] 2018-05-17 16:16:44.247508\n",
      "7100 2.5953205 [2.017051, 1.1051264, 0.9119246] 2018-05-17 16:17:12.509961\n",
      "7200 2.543733 [2.7296124, 1.3626442, 1.3669683] 2018-05-17 16:17:39.765074\n",
      "7300 2.5309312 [2.069095, 1.0253147, 1.0437802] 2018-05-17 16:18:15.512051\n",
      "7400 2.5094862 [2.1258402, 1.053185, 1.0726551] 2018-05-17 16:18:44.697212\n",
      "7500 2.56204 [2.6937268, 1.3406699, 1.3530569] 2018-05-17 16:19:11.517921\n",
      "7600 2.6097674 [2.7500336, 1.384779, 1.3652546] 2018-05-17 16:19:38.509046\n",
      "7700 2.4931545 [2.3591003, 1.165997, 1.1931033] 2018-05-17 16:20:11.396770\n",
      "7800 2.517136 [2.891782, 1.4458607, 1.4459213] 2018-05-17 16:20:43.801442\n",
      "7900 2.511502 [2.255221, 1.1167915, 1.1384294] 2018-05-17 16:21:20.346621\n",
      "8000 2.5110083 [2.1985855, 1.0964947, 1.1020908] 2018-05-17 16:21:56.727556\n",
      "8100 2.5593576 [2.1850812, 1.0837822, 1.101299] 2018-05-17 16:22:26.989891\n",
      "8200 2.5622988 [2.8872795, 1.4411784, 1.446101] 2018-05-17 16:23:17.336978\n",
      "8300 2.505352 [2.0505428, 1.0111343, 1.0394087] 2018-05-17 16:23:55.470770\n",
      "8400 2.446373 [1.7287909, 0.99775565, 0.73103523] 2018-05-17 16:24:26.123917\n",
      "8500 2.5143232 [2.7735562, 1.3875794, 1.3859769] 2018-05-17 16:24:59.848982\n",
      "8600 2.6385975 [2.272042, 1.1241416, 1.1479005] 2018-05-17 16:25:33.295005\n",
      "8700 2.4882424 [2.4558916, 1.2285771, 1.2273145] 2018-05-17 16:26:07.235018\n",
      "8800 2.5346382 [3.217307, 1.6057733, 1.6115339] 2018-05-17 16:26:52.353603\n",
      "8900 2.527245 [2.9492693, 1.4715374, 1.477732] 2018-05-17 16:27:29.171031\n",
      "9000 2.6012058 [2.164013, 1.0743198, 1.0896931] 2018-05-17 16:28:04.165926\n",
      "Skipped batch, length 62116\n",
      "9100 2.513757 [2.562646, 1.2793995, 1.2832465] 2018-05-17 16:28:42.552386\n",
      "9200 2.4999404 [2.8802862, 1.4272634, 1.4530227] 2018-05-17 16:29:17.367307\n",
      "9300 2.4965377 [2.7532334, 1.3699827, 1.3832508] 2018-05-17 16:29:49.772945\n",
      "9400 2.4820569 [2.5241897, 1.2455478, 1.2786419] 2018-05-17 16:30:23.934878\n",
      "9500 2.53069 [2.6236491, 1.302662, 1.3209872] 2018-05-17 16:31:06.956063\n",
      "9600 2.3937 [2.6591911, 1.3241715, 1.3350196] 2018-05-17 16:31:34.308361\n",
      "9700 2.4999855 [2.1659236, 1.0765462, 1.0893774] 2018-05-17 16:32:05.646150\n",
      "9800 2.4448469 [2.5899506, 1.2922189, 1.2977316] 2018-05-17 16:32:34.790346\n",
      "9900 2.3913112 [2.6310651, 1.3168302, 1.314235] 2018-05-17 16:33:06.681668\n",
      "10000 2.4831822 [2.4412885, 1.2161658, 1.2251227] 2018-05-17 16:33:39.277765\n",
      "10100 2.4090116 [1.4706331, 0.82892525, 0.6417079] 2018-05-17 16:34:12.326277\n",
      "10200 2.5940926 [2.9643688, 1.4782343, 1.4861345] 2018-05-17 16:34:51.253203\n",
      "10300 2.5111787 [2.263568, 1.1316386, 1.1319293] 2018-05-17 16:35:33.644690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10400 2.517065 [1.9118273, 1.0092373, 0.90259004] 2018-05-17 16:36:11.337572\n",
      "10500 2.4161572 [2.7048767, 1.3404658, 1.3644109] 2018-05-17 16:36:42.644968\n",
      "10600 2.5033522 [2.7540154, 1.3724196, 1.3815958] 2018-05-17 16:37:16.508211\n",
      "10700 2.4151127 [2.537217, 1.2658603, 1.2713566] 2018-05-17 16:37:47.804109\n",
      "10800 2.4644492 [2.1832392, 1.0785367, 1.1047025] 2018-05-17 16:38:17.223288\n",
      "10900 2.3914258 [2.4387956, 1.2017943, 1.2370014] 2018-05-17 16:38:42.943208\n",
      "11000 2.4413042 [2.0947943, 1.0618691, 1.032925] 2018-05-17 16:39:12.962794\n",
      "11100 2.4806614 [1.9255439, 0.95639455, 0.96914935] 2018-05-17 16:39:51.111353\n",
      "11200 2.4521117 [2.174569, 1.0823247, 1.0922441] 2018-05-17 16:40:31.322696\n",
      "11300 2.4429417 [2.5811205, 1.3802989, 1.2008216] 2018-05-17 16:41:09.273461\n",
      "Skipped batch, length 20547\n",
      "11400 2.4158144 [2.7348542, 1.3654803, 1.3693739] 2018-05-17 16:41:43.663239\n",
      "11500 2.4240012 [3.2051458, 1.5992768, 1.605869] 2018-05-17 16:42:11.904404\n",
      "11600 2.4616544 [2.697146, 1.3489103, 1.3482356] 2018-05-17 16:42:39.942617\n",
      "11700 2.5145643 [2.6031604, 1.3015084, 1.3016518] 2018-05-17 16:43:24.173353\n",
      "11800 2.4779484 [2.352927, 1.1605382, 1.1923888] 2018-05-17 16:44:01.182300\n",
      "11900 2.4301748 [2.5166614, 1.2500358, 1.2666256] 2018-05-17 16:44:31.977792\n",
      "12000 2.5046167 [1.6059194, 0.9190883, 0.686831] 2018-05-17 16:45:06.697079\n",
      "12100 2.4115574 [2.2879603, 1.1423107, 1.1456496] 2018-05-17 16:45:43.416347\n",
      "12200 2.3904932 [1.9432399, 0.9634978, 0.9797421] 2018-05-17 16:46:11.544661\n",
      "12300 2.4680166 [3.0649083, 1.5306654, 1.5342429] 2018-05-17 16:46:47.449335\n",
      "12400 2.3972955 [2.4783928, 1.2346747, 1.2437181] 2018-05-17 16:47:27.980961\n",
      "12500 2.4349296 [2.5632102, 1.2785043, 1.284706] 2018-05-17 16:48:03.444205\n",
      "12600 2.4386027 [2.0391355, 1.1088392, 0.9302963] 2018-05-17 16:48:39.845537\n",
      "12700 2.4312277 [2.5024595, 1.2582111, 1.2442484] 2018-05-17 16:49:19.730298\n",
      "12800 2.3781707 [2.2720435, 1.2473917, 1.0246518] 2018-05-17 16:49:53.010852\n",
      "12900 2.3979614 [2.4860392, 1.2373456, 1.2486935] 2018-05-17 16:50:23.955464\n",
      "Skipped batch, length 39533\n",
      "13000 2.3957443 [2.4224057, 1.2062898, 1.2161161] 2018-05-17 16:50:55.839221\n",
      "13100 2.4147933 [3.1305096, 1.5923862, 1.5381234] 2018-05-17 16:51:44.416383\n",
      "13200 2.41678 [2.786147, 1.3823017, 1.4038454] 2018-05-17 16:52:16.945503\n",
      "13300 2.4605498 [2.4545326, 1.2248435, 1.2296891] 2018-05-17 16:53:03.577624\n",
      "13400 2.4348326 [2.0743833, 1.0322156, 1.0421678] 2018-05-17 16:53:38.015290\n",
      "13500 2.3819232 [2.3851504, 1.2534316, 1.131719] 2018-05-17 16:54:05.745309\n",
      "13600 2.369117 [2.8254023, 1.4092622, 1.4161401] 2018-05-17 16:54:38.193504\n",
      "13700 2.3774018 [2.0663137, 1.0200762, 1.0462375] 2018-05-17 16:55:03.835387\n",
      "13800 2.4542441 [2.2352037, 1.1203096, 1.1148942] 2018-05-17 16:55:43.378126\n",
      "13900 2.4779208 [2.6532257, 1.323073, 1.3301526] 2018-05-17 16:56:18.181917\n",
      "14000 2.344889 [2.663812, 1.3109688, 1.3528432] 2018-05-17 16:56:47.191955\n",
      "14100 2.39984 [1.7205429, 0.8999231, 0.8206198] 2018-05-17 16:57:13.910498\n",
      "14200 2.4251273 [2.6550221, 1.3062949, 1.3487272] 2018-05-17 16:57:46.046777\n",
      "14300 2.4042335 [0.9262819, 0.55592835, 0.37035358] 2018-05-17 16:58:22.330365\n",
      "14400 2.372688 [2.2273726, 1.1043961, 1.1229764] 2018-05-17 16:58:55.330174\n",
      "14500 2.3536806 [2.484353, 1.2337102, 1.250643] 2018-05-17 16:59:19.468978\n",
      "14600 2.2954526 [2.2327023, 1.1127012, 1.1200011] 2018-05-17 16:59:56.462014\n",
      "14700 2.379563 [2.155772, 1.0772766, 1.0784954] 2018-05-17 17:00:36.645856\n",
      "14800 2.4023364 [2.5714314, 1.2755206, 1.2959108] 2018-05-17 17:01:24.111839\n",
      "14900 2.3646595 [1.8701129, 0.9322208, 0.9378921] 2018-05-17 17:01:54.955069\n",
      "15000 2.3898566 [2.7447176, 1.3743207, 1.3703969] 2018-05-17 17:02:33.360737\n",
      "15100 2.453286 [2.6082923, 1.2941418, 1.3141506] 2018-05-17 17:03:07.693283\n",
      "15200 2.346592 [1.20037, 0.7157787, 0.48459128] 2018-05-17 17:03:30.269714\n",
      "15300 2.3365946 [2.5557864, 1.2694347, 1.2863517] 2018-05-17 17:03:55.287504\n",
      "15400 2.3907108 [2.5356617, 1.2864964, 1.2491653] 2018-05-17 17:04:39.900421\n",
      "15500 2.3912003 [2.6677287, 1.3433614, 1.3243673] 2018-05-17 17:05:12.437659\n",
      "15600 2.3783395 [2.671492, 1.3248959, 1.3465964] 2018-05-17 17:05:41.994932\n",
      "15700 2.376145 [2.5372717, 1.2529993, 1.2842724] 2018-05-17 17:06:06.143946\n",
      "15800 2.3774395 [3.074545, 1.5310707, 1.5434741] 2018-05-17 17:06:37.891908\n",
      "15900 2.316758 [2.1507916, 1.06229, 1.0885017] 2018-05-17 17:07:07.137898\n",
      "16000 2.3511062 [2.7778335, 1.3678733, 1.40996] 2018-05-17 17:07:42.000869\n",
      "16100 2.3752444 [2.1573188, 1.0788069, 1.078512] 2018-05-17 17:08:11.094835\n",
      "16200 2.333801 [1.8281074, 0.9337262, 0.8943812] 2018-05-17 17:08:41.040536\n",
      "16300 2.3865902 [2.634626, 1.2939154, 1.3407106] 2018-05-17 17:09:10.061130\n",
      "16400 2.3045413 [2.1807702, 1.0898492, 1.0909209] 2018-05-17 17:09:39.229792\n",
      "Skipped batch, length 23801\n",
      "16500 2.3817456 [2.2725806, 1.1388475, 1.133733] 2018-05-17 17:10:09.803376\n",
      "16600 2.380448 [2.2231083, 1.1040652, 1.1190431] 2018-05-17 17:10:42.812301\n",
      "16700 2.3745737 [2.0978694, 1.0433061, 1.0545633] 2018-05-17 17:11:20.712335\n",
      "16800 2.3572516 [3.0063388, 1.5031202, 1.5032187] 2018-05-17 17:11:54.595274\n",
      "16900 2.4043086 [1.977073, 0.9848763, 0.9921966] 2018-05-17 17:12:29.633787\n",
      "17000 2.3093193 [2.2339506, 1.10572, 1.1282306] 2018-05-17 17:13:00.419818\n",
      "17100 2.3116512 [2.3397253, 1.1593335, 1.1803918] 2018-05-17 17:13:36.819974\n",
      "17200 2.383748 [2.8544044, 1.426431, 1.4279735] 2018-05-17 17:14:20.557738\n",
      "17300 2.3399544 [2.4716926, 1.2355487, 1.2361438] 2018-05-17 17:14:57.041074\n",
      "17400 2.3195252 [2.0329542, 1.0215455, 1.0114087] 2018-05-17 17:15:28.443334\n",
      "17500 2.3189404 [2.267002, 1.1301634, 1.1368387] 2018-05-17 17:16:00.329813\n",
      "17600 2.3380167 [2.1700892, 1.0762775, 1.0938118] 2018-05-17 17:16:35.460195\n",
      "17700 2.370992 [2.7869253, 1.3937052, 1.39322] 2018-05-17 17:17:13.683776\n",
      "17800 2.358116 [2.3130145, 1.1552057, 1.1578088] 2018-05-17 17:17:41.068273\n",
      "17900 2.350729 [1.0603563, 0.61835104, 0.4420052] 2018-05-17 17:18:10.081702\n",
      "18000 2.380694 [2.4025924, 1.2033597, 1.1992327] 2018-05-17 17:18:48.327670\n",
      "18100 2.3670988 [2.5019264, 1.2472646, 1.2546617] 2018-05-17 17:19:20.757707\n",
      "Skipped batch, length 18598\n",
      "18200 2.3266053 [2.52423, 1.2469144, 1.2773155] 2018-05-17 17:19:49.777251\n",
      "18300 2.370183 [3.0015724, 1.5052216, 1.4963508] 2018-05-17 17:20:37.163451\n",
      "18400 2.351498 [2.2006335, 1.0921216, 1.1085119] 2018-05-17 17:21:06.672984\n",
      "18500 2.37156 [2.7097456, 1.3509729, 1.3587728] 2018-05-17 17:21:40.951387\n",
      "18600 2.3313715 [2.6995893, 1.3548194, 1.3447698] 2018-05-17 17:22:14.867407\n",
      "18700 2.3563476 [2.2956862, 1.153229, 1.1424572] 2018-05-17 17:22:45.911326\n",
      "18800 2.3131292 [2.5085363, 1.2547896, 1.2537467] 2018-05-17 17:23:18.776983\n",
      "Skipped batch, length 1232093\n",
      "18900 2.3805766 [1.3085763, 0.785946, 0.5226303] 2018-05-17 17:23:51.111831\n",
      "19000 2.2870185 [2.153193, 1.0695803, 1.0836128] 2018-05-17 17:24:15.769135\n",
      "19100 2.3591573 [2.3710132, 1.1841964, 1.1868167] 2018-05-17 17:24:54.948205\n",
      "19200 2.323114 [2.1522772, 1.0759776, 1.0762997] 2018-05-17 17:25:38.219121\n",
      "19300 2.2608223 [2.3325846, 1.1714354, 1.1611493] 2018-05-17 17:26:06.054625\n",
      "19400 2.3553905 [2.0864592, 1.0308799, 1.0555792] 2018-05-17 17:26:37.483620\n",
      "19500 2.278309 [2.1198595, 1.0601952, 1.0596642] 2018-05-17 17:27:04.352775\n",
      "19600 2.3907778 [2.4341657, 1.2186918, 1.2154739] 2018-05-17 17:27:44.214419\n",
      "19700 2.4109123 [1.7130051, 0.8223294, 0.8906756] 2018-05-17 17:28:27.102057\n",
      "19800 2.362516 [2.009571, 0.9950965, 1.0144746] 2018-05-17 17:29:04.625122\n",
      "19900 2.3484435 [2.175252, 1.0805866, 1.0946655] 2018-05-17 17:29:31.400505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-17 17:30:04.529080\n",
      "Trained at 2018-05-17 17:35:50.230200\n",
      "TP 14694 FP 2824 FN 4635 F 0.7975683230656498 Precision 0.8387943829204247 Recall 0.7602048735061306\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-17 17:38:06.061152\n",
      "Trained at 2018-05-17 17:43:48.907203\n",
      "TP 16018 FP 2837 FN 3311 F 0.8389901529436413 Precision 0.8495359321134978 Recall 0.8287029851518444\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-17 17:46:04.995490\n",
      "Trained at 2018-05-17 17:51:48.045388\n",
      "TP 16524 FP 2951 FN 2805 F 0.8516647768271313 Precision 0.8484724005134788 Recall 0.8548812664907651\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-17 17:54:04.619267\n",
      "Trained at 2018-05-17 17:59:48.048039\n",
      "TP 16583 FP 2898 FN 2746 F 0.8545735635145582 Precision 0.8512396694214877 Recall 0.8579336747891769\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-17 18:02:04.663855\n",
      "Trained at 2018-05-17 18:07:48.647005\n",
      "TP 16705 FP 2881 FN 2624 F 0.8585378388796094 Precision 0.8529051363218626 Recall 0.8642454343214858\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-17 18:10:04.932393\n",
      "Trained at 2018-05-17 18:15:48.821894\n",
      "TP 16773 FP 2867 FN 2556 F 0.8608381020811414 Precision 0.8540224032586557 Recall 0.86776346422474\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-17 18:18:04.295549\n",
      "Trained at 2018-05-17 18:23:48.132691\n",
      "TP 16876 FP 2993 FN 2453 F 0.8610643400173478 Precision 0.8493633298102572 Recall 0.8730922448134927\n",
      "Best so far\n",
      "Epoch 7 start at 2018-05-17 18:26:03.961598\n",
      "Trained at 2018-05-17 18:31:48.108888\n",
      "TP 16858 FP 3130 FN 2471 F 0.8575425388508787 Precision 0.8434060436261757 Recall 0.8721610016038077\n",
      "Epoch 8 start at 2018-05-17 18:34:05.472726\n",
      "Trained at 2018-05-17 18:39:48.977553\n",
      "TP 16947 FP 3116 FN 2382 F 0.8604285134037368 Precision 0.844689228928874 Recall 0.876765481918361\n",
      "Epoch 9 start at 2018-05-17 18:42:05.482824\n",
      "Trained at 2018-05-17 18:47:49.105733\n",
      "TP 16797 FP 2899 FN 2532 F 0.860832799487508 Precision 0.8528127538586515 Recall 0.8690051218376532\n",
      "Epoch 10 start at 2018-05-17 18:50:04.902308\n",
      "Trained at 2018-05-17 18:55:48.834045\n",
      "TP 16793 FP 3023 FN 2536 F 0.8579895261208328 Precision 0.8474465078724264 Recall 0.8687981789021677\n",
      "Epoch 11 start at 2018-05-17 18:58:06.440824\n",
      "Trained at 2018-05-17 19:03:50.354270\n",
      "TP 17041 FP 3349 FN 2288 F 0.8580779979354969 Precision 0.8357528200098088 Recall 0.8816286409022712\n",
      "Epoch 12 start at 2018-05-17 19:06:07.579931\n",
      "Trained at 2018-05-17 19:11:50.269997\n",
      "TP 17029 FP 3144 FN 2300 F 0.8621841932054073 Precision 0.8441481187726169 Recall 0.8810078120958146\n",
      "Best so far\n",
      "Epoch 13 start at 2018-05-17 19:14:06.204812\n",
      "Trained at 2018-05-17 19:19:49.717014\n",
      "TP 16778 FP 3100 FN 2551 F 0.855867574667789 Precision 0.8440486970520173 Recall 0.868022142894097\n",
      "Epoch 14 start at 2018-05-17 19:22:06.089201\n",
      "Trained at 2018-05-17 19:27:49.385959\n",
      "TP 16897 FP 3012 FN 2432 F 0.8612569447984098 Precision 0.8487116379526847 Recall 0.8741786952247917\n",
      "Epoch 15 start at 2018-05-17 19:30:06.233434\n",
      "Trained at 2018-05-17 19:35:49.701042\n",
      "TP 17061 FP 3301 FN 2268 F 0.8596911138545262 Precision 0.837884294273647 Recall 0.8826633555796989\n",
      "Epoch 16 start at 2018-05-17 19:38:04.460598\n",
      "Trained at 2018-05-17 19:43:48.309340\n",
      "TP 16870 FP 3014 FN 2459 F 0.8604289393823477 Precision 0.8484208408770871 Recall 0.8727818304102644\n",
      "Epoch 17 start at 2018-05-17 19:46:02.876986\n",
      "Trained at 2018-05-17 19:51:46.663564\n",
      "TP 16914 FP 3027 FN 2415 F 0.8614209320091673 Precision 0.8482021964796148 Recall 0.8750582027006053\n",
      "Epoch 18 start at 2018-05-17 19:54:00.976260\n",
      "Trained at 2018-05-17 19:59:44.209991\n",
      "TP 17016 FP 3210 FN 2313 F 0.8603716344330679 Precision 0.841293384752299 Recall 0.8803352475554865\n",
      "Epoch 19 start at 2018-05-17 20:02:01.204784\n",
      "Trained at 2018-05-17 20:07:45.154617\n",
      "TP 16908 FP 3120 FN 2421 F 0.8592118301699825 Precision 0.8442180946674656 Recall 0.874747788297377\n",
      "Epoch 20 start at 2018-05-17 20:10:00.974565\n",
      "Trained at 2018-05-17 20:15:44.756757\n",
      "TP 16971 FP 3038 FN 2358 F 0.8628298337485383 Precision 0.8481683242540856 Recall 0.8780071395312743\n",
      "Best so far\n",
      "Epoch 21 start at 2018-05-17 20:18:00.394057\n",
      "Trained at 2018-05-17 20:23:43.821941\n",
      "TP 16896 FP 2958 FN 2433 F 0.8624148227547661 Precision 0.8510123904502871 Recall 0.8741269594909203\n",
      "Epoch 22 start at 2018-05-17 20:25:59.387736\n",
      "Trained at 2018-05-17 20:31:42.681403\n",
      "TP 16852 FP 2855 FN 2477 F 0.8634081360795164 Precision 0.8551276196275435 Recall 0.8718505872005794\n",
      "Best so far\n",
      "Epoch 23 start at 2018-05-17 20:33:59.775392\n",
      "Trained at 2018-05-17 20:39:42.925140\n",
      "TP 16767 FP 2900 FN 2562 F 0.8599343522412555 Precision 0.8525448721208115 Recall 0.8674530498215117\n",
      "Epoch 24 start at 2018-05-17 20:41:58.748279\n",
      "Trained at 2018-05-17 20:47:41.263269\n",
      "TP 16924 FP 3038 FN 2405 F 0.8614695477335776 Precision 0.8478108405971345 Recall 0.8755755600393191\n",
      "Epoch 25 start at 2018-05-17 20:49:55.251523\n",
      "Trained at 2018-05-17 20:55:37.088490\n",
      "TP 16991 FP 3064 FN 2338 F 0.8628377005890717 Precision 0.8472201446023435 Recall 0.879041854208702\n",
      "Epoch 26 start at 2018-05-17 20:57:52.444418\n",
      "Trained at 2018-05-17 21:03:34.649173\n",
      "TP 16941 FP 2973 FN 2388 F 0.8633896491093953 Precision 0.8507080445917445 Recall 0.8764550675151327\n",
      "Epoch 27 start at 2018-05-17 21:05:48.368309\n",
      "Trained at 2018-05-17 21:11:30.004665\n",
      "TP 16980 FP 3066 FN 2349 F 0.8624761904761905 Precision 0.847051780903921 Recall 0.8784727611361167\n",
      "Epoch 28 start at 2018-05-17 21:13:45.383955\n",
      "Trained at 2018-05-17 21:19:27.269150\n",
      "TP 17015 FP 3028 FN 2314 F 0.8643198211927258 Precision 0.8489248116549418 Recall 0.8802835118216152\n",
      "Best so far\n",
      "Epoch 29 start at 2018-05-17 21:21:42.767593\n",
      "Trained at 2018-05-17 21:27:24.418678\n",
      "TP 16881 FP 3069 FN 2448 F 0.8595432673947911 Precision 0.8461654135338346 Recall 0.8733509234828496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-17 21:29:39.403799\n",
      "Shufflesplit at 2018-05-17 21:29:39.404059\n",
      "Read annots at 2018-05-17 21:29:39.478003\n",
      "Read train seqs at 2018-05-17 21:29:39.696801\n",
      "Read test seqs at 2018-05-17 21:30:18.942539\n",
      "Corpus read at 2018-05-17 21:30:30.528927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-17 21:30:34.106598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 batches of unsupervised\n",
      "0 9.021916 [9.021916, 4.511559, 4.5103574] 2018-05-17 21:30:55.974217\n",
      "100 7.1007347 [6.275953, 3.1538901, 3.1220627] 2018-05-17 21:31:27.640338\n",
      "200 6.0111065 [5.739669, 2.8949368, 2.8447318] 2018-05-17 21:32:07.268044\n",
      "300 5.278547 [4.9741497, 2.497655, 2.4764946] 2018-05-17 21:32:38.283337\n",
      "400 5.0842595 [4.9759684, 2.4962578, 2.4797106] 2018-05-17 21:33:11.011774\n",
      "500 4.81693 [4.8913336, 2.4473493, 2.4439843] 2018-05-17 21:33:49.113490\n",
      "600 4.552961 [4.461816, 2.2282586, 2.2335575] 2018-05-17 21:34:14.361568\n",
      "700 4.1965775 [2.0131404, 1.1066967, 0.9064437] 2018-05-17 21:34:46.299938\n",
      "800 4.2350044 [4.025382, 2.0034156, 2.0219667] 2018-05-17 21:35:28.426012\n",
      "900 3.9868166 [4.1319695, 2.0678902, 2.0640793] 2018-05-17 21:36:07.379274\n",
      "1000 3.7376633 [3.8500173, 1.9259982, 1.9240191] 2018-05-17 21:36:37.265713\n",
      "1100 3.762603 [4.0370774, 2.0159955, 2.021082] 2018-05-17 21:37:14.160887\n",
      "1200 3.6674871 [4.191739, 2.0960956, 2.0956435] 2018-05-17 21:37:46.202845\n",
      "1300 3.7064588 [3.2149425, 1.5956488, 1.6192936] 2018-05-17 21:38:19.762625\n",
      "1400 3.5391235 [3.6389203, 1.809998, 1.8289223] 2018-05-17 21:38:46.222478\n",
      "1500 3.424151 [3.2063663, 1.5896912, 1.6166751] 2018-05-17 21:39:12.594603\n",
      "1600 3.3654284 [3.561889, 1.7820435, 1.7798455] 2018-05-17 21:39:48.333067\n",
      "1700 3.3545651 [3.1538796, 1.5601239, 1.5937556] 2018-05-17 21:40:22.931294\n",
      "1800 3.2800689 [3.281692, 1.6329688, 1.6487231] 2018-05-17 21:41:05.793841\n",
      "1900 3.2096229 [3.5398996, 1.7740889, 1.7658107] 2018-05-17 21:41:33.267750\n",
      "2000 3.23126 [3.5204358, 1.7558382, 1.7645977] 2018-05-17 21:42:09.578022\n",
      "2100 3.2435331 [3.189778, 1.59484, 1.594938] 2018-05-17 21:42:59.584011\n",
      "2200 3.14924 [3.6755712, 1.8160517, 1.8595195] 2018-05-17 21:43:37.683593\n",
      "2300 3.1323092 [2.3768516, 1.1585672, 1.2182844] 2018-05-17 21:44:14.573674\n",
      "2400 2.9842596 [2.8998594, 1.4404607, 1.4593989] 2018-05-17 21:44:49.067515\n",
      "2500 3.1150942 [3.2939057, 1.6336029, 1.6603029] 2018-05-17 21:45:18.449945\n",
      "2600 3.0324075 [3.2022767, 1.5906434, 1.6116333] 2018-05-17 21:45:59.382383\n",
      "2700 2.9087076 [2.7478561, 1.3724475, 1.3754088] 2018-05-17 21:46:30.901027\n",
      "2800 3.0445325 [3.034491, 1.5333408, 1.5011501] 2018-05-17 21:47:03.250840\n",
      "2900 2.9905365 [1.2946953, 0.7519356, 0.54275966] 2018-05-17 21:47:39.748519\n",
      "3000 2.9037669 [3.6037297, 1.7959327, 1.8077972] 2018-05-17 21:48:07.593744\n",
      "3100 3.068296 [3.228894, 1.6236174, 1.6052766] 2018-05-17 21:48:44.043767\n",
      "3200 2.9325075 [2.767043, 1.3844504, 1.3825928] 2018-05-17 21:49:24.910820\n",
      "3300 2.8646123 [2.6036243, 1.2896364, 1.313988] 2018-05-17 21:50:02.337876\n",
      "3400 2.808747 [2.7036362, 1.3383899, 1.3652463] 2018-05-17 21:50:42.151396\n",
      "3500 2.8653066 [1.5165904, 0.8319105, 0.6846799] 2018-05-17 21:51:19.252338\n",
      "3600 2.9102776 [1.3222663, 0.7792285, 0.5430379] 2018-05-17 21:51:47.911116\n",
      "3700 2.783324 [2.896268, 1.4443896, 1.4518784] 2018-05-17 21:52:24.818863\n",
      "3800 2.7993011 [2.668974, 1.3287507, 1.3402231] 2018-05-17 21:52:55.258238\n",
      "Skipped batch, length 59106\n",
      "3900 2.8398595 [3.1955745, 1.6000013, 1.5955732] 2018-05-17 21:53:42.098762\n",
      "4000 2.797623 [2.5422616, 1.2714487, 1.2708127] 2018-05-17 21:54:27.213692\n",
      "4100 2.7560813 [3.0323298, 1.5193269, 1.5130029] 2018-05-17 21:54:58.695934\n",
      "4200 2.7680357 [3.123836, 1.5579778, 1.5658581] 2018-05-17 21:55:28.699340\n",
      "4300 2.7414794 [2.7730348, 1.3864629, 1.3865719] 2018-05-17 21:56:03.021503\n",
      "4400 2.798693 [2.9899187, 1.4961739, 1.4937449] 2018-05-17 21:56:43.015635\n",
      "4500 2.7753255 [2.9784276, 1.4839759, 1.4944518] 2018-05-17 21:57:11.821709\n",
      "4600 2.7465408 [2.6620648, 1.3249862, 1.3370786] 2018-05-17 21:57:44.162095\n",
      "4700 2.6921191 [2.714498, 1.3539401, 1.3605578] 2018-05-17 21:58:14.914375\n",
      "4800 2.7628744 [2.8845973, 1.4338719, 1.4507253] 2018-05-17 21:58:43.541049\n",
      "4900 2.8002088 [2.803725, 1.4051102, 1.3986148] 2018-05-17 21:59:23.772292\n",
      "5000 2.7117841 [3.0041149, 1.4973658, 1.506749] 2018-05-17 21:59:53.273875\n",
      "5100 2.6960783 [2.6546385, 1.3218, 1.3328385] 2018-05-17 22:00:27.329482\n",
      "5200 2.695901 [2.8583152, 1.41734, 1.4409752] 2018-05-17 22:01:07.591490\n",
      "5300 2.6243744 [3.0740461, 1.5359789, 1.5380671] 2018-05-17 22:01:36.872631\n",
      "5400 2.6311464 [2.3210838, 1.2390382, 1.0820456] 2018-05-17 22:02:03.980376\n",
      "5500 2.7678843 [3.0598679, 1.528266, 1.5316019] 2018-05-17 22:02:39.944950\n",
      "5600 2.7720358 [1.6943822, 0.8643467, 0.8300355] 2018-05-17 22:03:23.622506\n",
      "5700 2.658234 [2.9803667, 1.493576, 1.4867908] 2018-05-17 22:04:00.733915\n",
      "5800 2.570461 [2.870314, 1.434623, 1.4356909] 2018-05-17 22:04:31.401902\n",
      "5900 2.6407914 [2.9729102, 1.485611, 1.4872992] 2018-05-17 22:05:02.757887\n",
      "6000 2.6462836 [1.6870463, 0.9584805, 0.7285659] 2018-05-17 22:05:40.902218\n",
      "6100 2.6355126 [2.9508042, 1.475395, 1.4754093] 2018-05-17 22:06:26.392265\n",
      "6200 2.588381 [2.5237985, 1.2429329, 1.2808657] 2018-05-17 22:07:03.441394\n",
      "6300 2.565103 [2.9046454, 1.4625964, 1.4420489] 2018-05-17 22:07:35.671483\n",
      "6400 2.6444962 [2.415033, 1.1810694, 1.2339637] 2018-05-17 22:08:08.398083\n",
      "6500 2.5768127 [3.7098808, 1.8496492, 1.8602318] 2018-05-17 22:08:45.284411\n",
      "6600 2.6440256 [2.664556, 1.336667, 1.327889] 2018-05-17 22:09:21.020762\n",
      "6700 2.5584164 [1.6404892, 0.8903434, 0.7501458] 2018-05-17 22:09:55.943219\n",
      "6800 2.5695066 [1.5711834, 0.8815844, 0.68959904] 2018-05-17 22:10:28.760618\n",
      "6900 2.6378655 [2.667276, 1.3327105, 1.3345654] 2018-05-17 22:11:04.226216\n",
      "7000 2.5668714 [2.863421, 1.4230778, 1.4403431] 2018-05-17 22:11:37.708343\n",
      "7100 2.488602 [1.6827791, 0.985307, 0.6974721] 2018-05-17 22:12:11.736501\n",
      "7200 2.6356034 [2.73976, 1.3649107, 1.3748493] 2018-05-17 22:12:42.875625\n",
      "7300 2.5048223 [2.7286444, 1.3602465, 1.368398] 2018-05-17 22:13:08.552214\n",
      "7400 2.505063 [2.404443, 1.1991172, 1.2053258] 2018-05-17 22:13:38.768311\n",
      "7500 2.6186945 [2.748021, 1.3740523, 1.3739686] 2018-05-17 22:14:14.187747\n",
      "7600 2.5017622 [2.6221337, 1.3066672, 1.3154666] 2018-05-17 22:14:42.079069\n",
      "7700 2.536928 [2.6261454, 1.3099277, 1.3162178] 2018-05-17 22:15:09.384553\n",
      "7800 2.5165982 [1.9150481, 0.9672222, 0.9478259] 2018-05-17 22:15:43.539513\n",
      "7900 2.5278518 [3.089321, 1.543998, 1.5453229] 2018-05-17 22:16:22.858925\n",
      "8000 2.5969267 [2.416194, 1.210413, 1.205781] 2018-05-17 22:17:03.458091\n",
      "8100 2.4874482 [2.761276, 1.3842549, 1.3770211] 2018-05-17 22:17:34.153912\n",
      "8200 2.5815136 [2.7762675, 1.392954, 1.3833137] 2018-05-17 22:18:10.909419\n",
      "8300 2.5622888 [2.8319144, 1.4200176, 1.4118967] 2018-05-17 22:18:49.200113\n",
      "8400 2.5366726 [2.4349134, 1.2086308, 1.2262826] 2018-05-17 22:19:27.832789\n",
      "8500 2.5269806 [2.5211413, 1.256814, 1.2643273] 2018-05-17 22:20:00.763097\n",
      "8600 2.543281 [2.8125646, 1.4042003, 1.4083643] 2018-05-17 22:20:28.638924\n",
      "8700 2.4312103 [2.7875576, 1.3944097, 1.3931478] 2018-05-17 22:20:54.977033\n",
      "8800 2.429578 [1.5323794, 0.8797567, 0.65262276] 2018-05-17 22:21:24.423329\n",
      "8900 2.567833 [2.613599, 1.3090556, 1.3045435] 2018-05-17 22:22:03.047628\n",
      "9000 2.5360582 [2.1266036, 1.0586412, 1.0679625] 2018-05-17 22:22:39.409639\n",
      "9100 2.5029151 [2.5215354, 1.2561302, 1.2654053] 2018-05-17 22:23:11.227672\n",
      "9200 2.5680716 [1.2486398, 0.73705167, 0.51158816] 2018-05-17 22:23:49.334795\n",
      "9300 2.4140565 [2.7690525, 1.3852417, 1.3838106] 2018-05-17 22:24:20.212129\n",
      "9400 2.582045 [2.2965465, 1.139196, 1.1573503] 2018-05-17 22:24:50.260109\n",
      "9500 2.4518702 [1.9912628, 0.98459065, 1.0066721] 2018-05-17 22:25:16.046029\n",
      "9600 2.4808385 [2.54132, 1.272994, 1.268326] 2018-05-17 22:25:48.448716\n",
      "9700 2.4573264 [2.0786452, 1.0356879, 1.0429573] 2018-05-17 22:26:15.718871\n",
      "9800 2.4201832 [2.8962083, 1.445936, 1.4502723] 2018-05-17 22:26:45.693185\n",
      "9900 2.492048 [2.5421073, 1.2685304, 1.273577] 2018-05-17 22:27:22.700239\n",
      "10000 2.4763165 [2.2743764, 1.1342415, 1.140135] 2018-05-17 22:27:56.664467\n",
      "10100 2.4420447 [2.7735171, 1.3774478, 1.3960692] 2018-05-17 22:28:22.725240\n",
      "10200 2.5775573 [2.891325, 1.4433382, 1.447987] 2018-05-17 22:29:09.114897\n",
      "10300 2.4295037 [1.2907753, 0.7685454, 0.5222299] 2018-05-17 22:29:42.591561\n",
      "10400 2.4743748 [3.0557415, 1.5093236, 1.546418] 2018-05-17 22:30:12.054724\n",
      "10500 2.4529722 [2.2519064, 1.1173596, 1.1345466] 2018-05-17 22:30:46.748372\n",
      "10600 2.4479277 [2.4543138, 1.227069, 1.2272446] 2018-05-17 22:31:21.637173\n",
      "10700 2.4501252 [2.684133, 1.3384401, 1.3456931] 2018-05-17 22:31:49.801430\n",
      "10800 2.4218493 [2.2747083, 1.1252744, 1.1494339] 2018-05-17 22:32:22.336776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10900 2.418712 [1.1406288, 0.63953006, 0.5010988] 2018-05-17 22:32:49.833880\n",
      "11000 2.4127474 [2.6080117, 1.303411, 1.3046007] 2018-05-17 22:33:19.019009\n",
      "11100 2.4360545 [2.494981, 1.2448788, 1.2501023] 2018-05-17 22:33:48.053732\n",
      "11200 2.5449448 [2.6543279, 1.3285108, 1.325817] 2018-05-17 22:34:15.650905\n",
      "11300 2.4342735 [2.3220232, 1.1607685, 1.1612546] 2018-05-17 22:34:49.337665\n",
      "11400 2.3828392 [2.8470266, 1.421504, 1.4255226] 2018-05-17 22:35:19.861260\n",
      "Skipped batch, length 16544\n",
      "11500 2.4955306 [2.7714486, 1.3999851, 1.3714634] 2018-05-17 22:35:50.679810\n",
      "11600 2.4352884 [1.5645443, 0.8798572, 0.68468714] 2018-05-17 22:36:21.625691\n",
      "11700 2.4425173 [2.2586064, 1.1201669, 1.1384395] 2018-05-17 22:36:54.255341\n",
      "11800 2.439436 [2.2806053, 1.139545, 1.1410604] 2018-05-17 22:37:22.795173\n",
      "11900 2.417976 [2.3769107, 1.2037388, 1.1731718] 2018-05-17 22:37:49.060006\n",
      "12000 2.4200025 [2.395583, 1.1892979, 1.206285] 2018-05-17 22:38:31.166504\n",
      "12100 2.4360628 [1.2752929, 0.7956482, 0.47964466] 2018-05-17 22:39:04.690870\n",
      "12200 2.4410136 [2.2582846, 1.119961, 1.1383235] 2018-05-17 22:39:33.993148\n",
      "12300 2.4046662 [2.5679562, 1.2896814, 1.2782748] 2018-05-17 22:39:59.637799\n",
      "12400 2.5107577 [2.3263998, 1.1586306, 1.1677692] 2018-05-17 22:40:30.683480\n",
      "12500 2.4033594 [1.676872, 0.84219337, 0.8346787] 2018-05-17 22:41:03.686882\n",
      "12600 2.468398 [2.1955583, 1.0950897, 1.1004686] 2018-05-17 22:41:40.412861\n",
      "12700 2.394919 [1.1778764, 0.6939964, 0.48387992] 2018-05-17 22:42:09.642806\n",
      "12800 2.388913 [1.5716166, 0.7836715, 0.78794515] 2018-05-17 22:42:38.185361\n",
      "12900 2.406579 [2.561685, 1.3353947, 1.2262902] 2018-05-17 22:43:06.925756\n",
      "13000 2.4017608 [2.7115097, 1.3560114, 1.3554983] 2018-05-17 22:43:42.092840\n",
      "13100 2.3614538 [2.504726, 1.2489429, 1.2557831] 2018-05-17 22:44:08.368964\n",
      "Skipped batch, length 20380\n",
      "13200 2.3773923 [2.3095298, 1.1546854, 1.1548443] 2018-05-17 22:44:39.689030\n",
      "13300 2.3917854 [2.5735679, 1.2834911, 1.2900766] 2018-05-17 22:45:07.391433\n",
      "13400 2.3929117 [2.2465615, 1.1272262, 1.1193352] 2018-05-17 22:45:35.840509\n",
      "13500 2.301279 [2.1376166, 1.0596201, 1.0779966] 2018-05-17 22:46:03.878102\n",
      "13600 2.337905 [2.5882416, 1.2858461, 1.3023955] 2018-05-17 22:46:30.434984\n",
      "13700 2.3990319 [1.358275, 0.7840146, 0.5742605] 2018-05-17 22:47:04.300160\n",
      "13800 2.3505914 [2.4325292, 1.2163725, 1.2161567] 2018-05-17 22:47:36.669568\n",
      "13900 2.4475813 [3.0702658, 1.5164386, 1.553827] 2018-05-17 22:48:14.490576\n",
      "14000 2.435478 [2.2481446, 1.1219716, 1.126173] 2018-05-17 22:48:50.859031\n",
      "14100 2.3690748 [2.4109297, 1.1993136, 1.211616] 2018-05-17 22:49:19.678996\n",
      "14200 2.4824836 [2.1682584, 1.0750177, 1.0932407] 2018-05-17 22:49:52.693488\n",
      "14300 2.3595762 [1.9845324, 1.0008411, 0.9836912] 2018-05-17 22:50:23.112776\n",
      "14400 2.356777 [1.2180104, 0.6890223, 0.5289882] 2018-05-17 22:50:49.959512\n",
      "14500 2.3773212 [2.4836454, 1.2485942, 1.2350513] 2018-05-17 22:51:18.937029\n",
      "14600 2.4682708 [2.4152806, 1.2006793, 1.2146013] 2018-05-17 22:51:58.317674\n",
      "14700 2.3770323 [2.4764915, 1.2344931, 1.2419982] 2018-05-17 22:52:26.068398\n",
      "14800 2.4071019 [1.5900321, 0.8930449, 0.6969872] 2018-05-17 22:53:02.781034\n",
      "14900 2.4680033 [2.9623802, 1.5019426, 1.4604375] 2018-05-17 22:53:41.729076\n",
      "15000 2.427588 [2.6924114, 1.3554587, 1.3369527] 2018-05-17 22:54:15.467055\n",
      "15100 2.4267664 [1.1362048, 0.6873933, 0.4488115] 2018-05-17 22:54:54.888381\n",
      "15200 2.3549783 [2.1899016, 1.0858886, 1.104013] 2018-05-17 22:55:27.608652\n",
      "15300 2.290194 [2.129808, 1.0586336, 1.0711744] 2018-05-17 22:55:59.145164\n",
      "15400 2.3810265 [2.8198962, 1.407718, 1.4121782] 2018-05-17 22:56:40.037283\n",
      "15500 2.2943687 [2.6048005, 1.3021746, 1.3026259] 2018-05-17 22:57:15.826782\n",
      "15600 2.3879206 [2.5680037, 1.288152, 1.2798516] 2018-05-17 22:58:01.581077\n",
      "15700 2.3566468 [2.4489717, 1.2073361, 1.2416357] 2018-05-17 22:58:32.363923\n",
      "15800 2.329299 [2.3563046, 1.1600356, 1.196269] 2018-05-17 22:59:04.744041\n",
      "15900 2.3829184 [2.3704038, 1.1811917, 1.1892121] 2018-05-17 22:59:39.929559\n",
      "16000 2.414585 [2.220669, 1.1078563, 1.1128128] 2018-05-17 23:00:26.753326\n",
      "16100 2.3280218 [2.2154748, 1.1036775, 1.1117973] 2018-05-17 23:00:58.680073\n",
      "16200 2.3373928 [2.4801726, 1.2414738, 1.2386987] 2018-05-17 23:01:26.904688\n",
      "Skipped batch, length 20745\n",
      "16300 2.4183276 [2.2350788, 1.1129097, 1.122169] 2018-05-17 23:02:01.913313\n",
      "16400 2.339661 [2.8115368, 1.4048195, 1.4067173] 2018-05-17 23:02:31.990400\n",
      "16500 2.268276 [2.1611807, 1.0731876, 1.0879931] 2018-05-17 23:03:01.498789\n",
      "16600 2.3512022 [2.9282136, 1.4613068, 1.4669068] 2018-05-17 23:03:32.353145\n",
      "16700 2.3540828 [2.1707835, 1.0560815, 1.114702] 2018-05-17 23:03:59.351273\n",
      "16800 2.3634725 [2.1734538, 1.0729686, 1.1004852] 2018-05-17 23:04:44.695265\n",
      "Skipped batch, length 17232\n",
      "16900 2.3327346 [2.6555293, 1.305913, 1.3496163] 2018-05-17 23:05:12.368477\n",
      "17000 2.3154683 [2.149207, 1.0780244, 1.0711826] 2018-05-17 23:05:44.691023\n",
      "Skipped batch, length 21204\n",
      "17100 2.3423424 [1.2261992, 0.79275215, 0.433447] 2018-05-17 23:06:10.774657\n",
      "17200 2.3758342 [2.455958, 1.2385566, 1.2174013] 2018-05-17 23:06:50.522113\n",
      "17300 2.4627218 [2.4119115, 1.1985254, 1.213386] 2018-05-17 23:07:30.299841\n",
      "17400 2.3655534 [2.741541, 1.3559582, 1.3855827] 2018-05-17 23:08:06.364376\n",
      "17500 2.3389819 [2.2549176, 1.1219363, 1.1329812] 2018-05-17 23:08:36.834060\n",
      "17600 2.3394232 [1.9475214, 0.9699444, 0.977577] 2018-05-17 23:09:10.877256\n",
      "17700 2.3963163 [2.5414937, 1.260411, 1.2810826] 2018-05-17 23:09:41.620875\n",
      "17800 2.3327765 [2.3479116, 1.1790228, 1.1688888] 2018-05-17 23:10:18.973785\n",
      "17900 2.4217877 [2.5118775, 1.2571826, 1.2546949] 2018-05-17 23:10:54.786616\n",
      "18000 2.3322053 [2.6347852, 1.3163464, 1.3184389] 2018-05-17 23:11:19.474594\n",
      "18100 2.3466113 [3.1972747, 1.6027489, 1.5945258] 2018-05-17 23:11:56.033489\n",
      "18200 2.378247 [2.4373865, 1.2129267, 1.2244598] 2018-05-17 23:12:31.338956\n",
      "18300 2.400607 [2.8182619, 1.405447, 1.4128149] 2018-05-17 23:13:04.251522\n",
      "Skipped batch, length 16771\n",
      "18400 2.4092305 [1.3076023, 0.77389836, 0.5337039] 2018-05-17 23:13:42.339315\n",
      "18500 2.3193948 [2.4832468, 1.2416774, 1.2415693] 2018-05-17 23:14:07.651147\n",
      "18600 2.3546083 [2.8414655, 1.4045078, 1.4369578] 2018-05-17 23:14:38.236755\n",
      "18700 2.3194354 [2.0745413, 1.0289881, 1.0455532] 2018-05-17 23:15:06.897389\n",
      "18800 2.3572838 [2.878551, 1.434767, 1.4437839] 2018-05-17 23:15:40.948148\n",
      "18900 2.2928147 [1.9617581, 0.9768777, 0.98488045] 2018-05-17 23:16:11.714518\n",
      "19000 2.2228007 [0.92751986, 0.58192956, 0.3455903] 2018-05-17 23:16:44.733394\n",
      "19100 2.3959901 [2.5195572, 1.2653034, 1.2542539] 2018-05-17 23:17:14.594576\n",
      "19200 2.3301518 [2.446544, 1.2235928, 1.2229512] 2018-05-17 23:17:37.944931\n",
      "19300 2.329297 [1.608773, 0.8893124, 0.71946055] 2018-05-17 23:18:19.936020\n",
      "19400 2.2819088 [1.8072729, 0.89778984, 0.9094831] 2018-05-17 23:18:58.027841\n",
      "19500 2.2567468 [2.0728192, 1.0259398, 1.0468793] 2018-05-17 23:19:26.242243\n",
      "19600 2.372132 [2.4157662, 1.1981733, 1.217593] 2018-05-17 23:20:02.822192\n",
      "19700 2.4160292 [2.6320243, 1.3172874, 1.3147368] 2018-05-17 23:20:35.908482\n",
      "19800 2.2472847 [2.1595285, 1.0781865, 1.081342] 2018-05-17 23:21:04.448619\n",
      "19900 2.4246905 [2.3983264, 1.1999643, 1.1983621] 2018-05-17 23:21:38.098964\n",
      "20000 2.3495717 [2.1562262, 1.0705857, 1.0856404] 2018-05-17 23:22:12.378738\n",
      "20100 2.3334196 [1.9369957, 0.98518, 0.95181566] 2018-05-17 23:22:40.479140\n",
      "20200 2.2919762 [2.3393676, 1.1663827, 1.172985] 2018-05-17 23:23:09.900027\n",
      "20300 2.3538113 [2.0545764, 1.0241628, 1.0304136] 2018-05-17 23:23:51.888031\n",
      "20400 2.3166904 [2.2417908, 1.1096978, 1.1320928] 2018-05-17 23:24:22.944800\n",
      "20500 2.225811 [2.6042604, 1.2930689, 1.3111917] 2018-05-17 23:24:51.164856\n",
      "20600 2.3112822 [1.0419976, 0.63313377, 0.40886378] 2018-05-17 23:25:22.364224\n",
      "20700 2.3181405 [2.7536013, 1.3773566, 1.3762447] 2018-05-17 23:26:00.366489\n",
      "20800 2.2477293 [2.087413, 1.0889621, 0.998451] 2018-05-17 23:26:28.782552\n",
      "20900 2.3466275 [2.566143, 1.2787807, 1.2873623] 2018-05-17 23:27:01.897722\n",
      "21000 2.2798493 [1.9642043, 0.97309387, 0.99111044] 2018-05-17 23:27:33.247032\n",
      "21100 2.35357 [2.716546, 1.3529162, 1.3636297] 2018-05-17 23:28:07.710049\n",
      "21200 2.3695154 [2.5422573, 1.2614971, 1.2807603] 2018-05-17 23:28:41.693911\n",
      "21300 2.3056836 [2.3880928, 1.1588924, 1.2292004] 2018-05-17 23:29:19.418967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21400 2.317119 [2.5172582, 1.2555392, 1.2617191] 2018-05-17 23:29:53.720477\n",
      "21500 2.2963202 [2.4687245, 1.2241541, 1.2445704] 2018-05-17 23:30:23.714360\n",
      "Skipped batch, length 31556\n",
      "21600 2.2760718 [2.1312757, 1.0562559, 1.0750198] 2018-05-17 23:30:54.253804\n",
      "21700 2.2814455 [2.2428033, 1.1151625, 1.1276408] 2018-05-17 23:31:22.801076\n",
      "21800 2.3744552 [2.6492097, 1.3223317, 1.3268781] 2018-05-17 23:31:46.740759\n",
      "21900 2.3306167 [1.8872726, 0.9400992, 0.94717336] 2018-05-17 23:32:20.061150\n",
      "22000 2.278691 [2.9762545, 1.4767697, 1.4994848] 2018-05-17 23:32:47.099803\n",
      "22100 2.3388674 [2.4336724, 1.2011089, 1.2325635] 2018-05-17 23:33:21.462836\n",
      "22200 2.3760705 [2.4385118, 1.2182794, 1.2202325] 2018-05-17 23:34:05.956038\n",
      "22300 2.272867 [2.2490735, 1.1104641, 1.1386094] 2018-05-17 23:34:26.189623\n",
      "22400 2.335001 [2.5980716, 1.2916684, 1.3064032] 2018-05-17 23:35:04.166370\n",
      "22500 2.2507887 [2.027412, 1.0269984, 1.0004134] 2018-05-17 23:35:30.451283\n",
      "22600 2.288904 [1.4617527, 0.83717996, 0.6245727] 2018-05-17 23:35:59.594851\n",
      "22700 2.2870328 [2.2902412, 1.1380136, 1.1522276] 2018-05-17 23:36:37.597080\n",
      "22800 2.2551494 [2.045805, 1.0156845, 1.0301204] 2018-05-17 23:37:04.367683\n",
      "22900 2.3665764 [2.2553182, 1.1237814, 1.1315366] 2018-05-17 23:37:35.727785\n",
      "23000 2.2727435 [1.9398632, 0.96564424, 0.97421896] 2018-05-17 23:38:06.183567\n",
      "23100 2.2588825 [1.9264507, 0.96795774, 0.95849305] 2018-05-17 23:38:45.582317\n",
      "23200 2.3154306 [2.5364556, 1.265137, 1.2713187] 2018-05-17 23:39:14.765271\n",
      "23300 2.292349 [2.525576, 1.2612084, 1.2643676] 2018-05-17 23:39:49.918917\n",
      "23400 2.293909 [1.8000795, 0.8880948, 0.9119847] 2018-05-17 23:40:28.105685\n",
      "Skipped batch, length 19445\n",
      "23500 2.3231418 [2.4306948, 1.2209144, 1.2097805] 2018-05-17 23:41:04.946848\n",
      "23600 2.3109481 [2.7255976, 1.362822, 1.3627756] 2018-05-17 23:41:39.363507\n",
      "23700 2.297792 [2.5634546, 1.2827858, 1.2806687] 2018-05-17 23:42:03.921708\n",
      "23800 2.2790704 [2.2801843, 1.1302534, 1.1499308] 2018-05-17 23:42:48.181842\n",
      "23900 2.2716622 [2.5441744, 1.273143, 1.2710314] 2018-05-17 23:43:15.123353\n",
      "Skipped batch, length 17851\n",
      "24000 2.2728343 [1.2078038, 0.75578016, 0.4520237] 2018-05-17 23:43:45.109121\n",
      "24100 2.2617445 [2.3913116, 1.2008908, 1.1904207] 2018-05-17 23:44:14.672638\n",
      "24200 2.3500519 [2.4954104, 1.2468362, 1.2485743] 2018-05-17 23:44:45.167378\n",
      "24300 2.2622519 [2.5392592, 1.2774252, 1.261834] 2018-05-17 23:45:10.805499\n",
      "24400 2.334964 [2.4988384, 1.246247, 1.2525914] 2018-05-17 23:45:41.188921\n",
      "24500 2.3247194 [2.4395351, 1.2039448, 1.2355902] 2018-05-17 23:46:15.143536\n",
      "24600 2.3290937 [2.1934993, 1.0931094, 1.10039] 2018-05-17 23:46:35.456785\n",
      "Skipped batch, length 30016\n",
      "24700 2.2631161 [2.3245714, 1.157651, 1.1669204] 2018-05-17 23:47:06.931219\n",
      "24800 2.2917252 [2.1134887, 1.0480449, 1.0654438] 2018-05-17 23:47:40.651415\n",
      "24900 2.2594492 [2.2062201, 1.1084251, 1.097795] 2018-05-17 23:48:10.812653\n",
      "25000 2.2313554 [2.8353124, 1.4192907, 1.4160218] 2018-05-17 23:48:39.489277\n",
      "25100 2.3478723 [2.6040838, 1.2993348, 1.304749] 2018-05-17 23:49:15.012887\n",
      "25200 2.264981 [2.764101, 1.3842096, 1.3798914] 2018-05-17 23:49:49.218647\n",
      "25300 2.3233683 [1.8313382, 0.9766117, 0.85472643] 2018-05-17 23:50:22.695327\n",
      "25400 2.3273385 [1.9341905, 0.9770301, 0.9571605] 2018-05-17 23:50:54.826075\n",
      "25500 2.3292487 [1.8945023, 0.92858195, 0.9659203] 2018-05-17 23:51:25.413672\n",
      "25600 2.2767801 [1.9911474, 0.9881021, 1.0030453] 2018-05-17 23:51:54.988362\n",
      "25700 2.3248353 [2.4914541, 1.2503371, 1.2411169] 2018-05-17 23:52:20.990335\n",
      "25800 2.270631 [1.8758352, 0.9281321, 0.9477031] 2018-05-17 23:52:52.531898\n",
      "25900 2.2504132 [2.84864, 1.4223423, 1.4262977] 2018-05-17 23:53:33.976788\n",
      "26000 2.3113296 [2.3795362, 1.1761348, 1.2034013] 2018-05-17 23:54:13.194160\n",
      "26100 2.2503958 [2.0093608, 1.0149946, 0.9943661] 2018-05-17 23:54:47.299558\n",
      "26200 2.27067 [2.0941901, 1.0382568, 1.0559332] 2018-05-17 23:55:11.185640\n",
      "26300 2.295803 [2.6857839, 1.3606482, 1.3251357] 2018-05-17 23:55:48.259514\n",
      "26400 2.3113222 [2.989798, 1.4912543, 1.4985437] 2018-05-17 23:56:20.196778\n",
      "Skipped batch, length 35790\n",
      "26500 2.309821 [2.518985, 1.2575297, 1.2614553] 2018-05-17 23:56:50.082016\n",
      "Skipped batch, length 16988\n",
      "26600 2.2827594 [1.6549563, 0.8541148, 0.8008415] 2018-05-17 23:57:11.962182\n",
      "26700 2.2244344 [1.8813926, 0.93754464, 0.94384795] 2018-05-17 23:57:34.994624\n",
      "26800 2.2831066 [2.5841012, 1.2887235, 1.2953777] 2018-05-17 23:58:03.770807\n",
      "26900 2.2228005 [2.344554, 1.1772163, 1.1673378] 2018-05-17 23:58:33.286825\n",
      "27000 2.2087028 [2.325006, 1.1687194, 1.1562866] 2018-05-17 23:59:04.349335\n",
      "27100 2.2794147 [1.2918795, 0.7530155, 0.538864] 2018-05-17 23:59:32.702264\n",
      "27200 2.313453 [1.6152129, 0.7994354, 0.81577754] 2018-05-18 00:00:03.961778\n",
      "27300 2.3261747 [2.6706045, 1.3452303, 1.3253741] 2018-05-18 00:00:38.472237\n",
      "27400 2.245231 [1.9412638, 0.97557503, 0.96568877] 2018-05-18 00:01:07.045365\n",
      "27500 2.2440188 [2.3109813, 1.1618989, 1.1490825] 2018-05-18 00:01:35.964009\n",
      "27600 2.3178103 [2.435145, 1.2173669, 1.2177781] 2018-05-18 00:02:20.103572\n",
      "27700 2.2737064 [2.0126858, 1.0048742, 1.0078114] 2018-05-18 00:02:55.630227\n",
      "27800 2.2879288 [2.6920092, 1.3380477, 1.3539615] 2018-05-18 00:03:27.731946\n",
      "27900 2.319571 [2.4639711, 1.2287791, 1.2351921] 2018-05-18 00:04:00.276680\n",
      "28000 2.240809 [2.5184455, 1.258428, 1.2600175] 2018-05-18 00:04:32.911160\n",
      "28100 2.3270235 [2.3559484, 1.1740521, 1.1818964] 2018-05-18 00:05:07.863459\n",
      "Skipped batch, length 37923\n",
      "28200 2.3284595 [2.7039614, 1.3540592, 1.3499022] 2018-05-18 00:05:39.561781\n",
      "28300 2.2226038 [2.466477, 1.2360595, 1.2304175] 2018-05-18 00:06:11.143395\n",
      "28400 2.2379987 [2.1438475, 1.0670763, 1.076771] 2018-05-18 00:06:42.087243\n",
      "28500 2.2909205 [2.572322, 1.2834789, 1.2888432] 2018-05-18 00:07:09.627944\n",
      "28600 2.2513828 [2.212058, 1.1077707, 1.1042873] 2018-05-18 00:07:37.856424\n",
      "28700 2.2572792 [2.1540613, 1.0745695, 1.0794919] 2018-05-18 00:08:02.076184\n",
      "28800 2.2976658 [2.575006, 1.2618308, 1.3131753] 2018-05-18 00:08:32.873547\n",
      "28900 2.288731 [2.6462362, 1.3169148, 1.3293214] 2018-05-18 00:09:00.554640\n",
      "29000 2.2473633 [1.4489768, 0.81633115, 0.6326456] 2018-05-18 00:09:40.947634\n",
      "29100 2.2352145 [2.1466546, 1.0688306, 1.0778241] 2018-05-18 00:10:09.338672\n",
      "Skipped batch, length 19999\n",
      "29200 2.3063145 [2.7463603, 1.3728703, 1.37349] 2018-05-18 00:10:42.863381\n",
      "29300 2.2505774 [1.288059, 0.75499666, 0.5330623] 2018-05-18 00:11:21.907668\n",
      "29400 2.3218114 [2.5487309, 1.2756019, 1.273129] 2018-05-18 00:11:55.569202\n",
      "29500 2.2216942 [2.1371627, 1.0692317, 1.0679308] 2018-05-18 00:12:31.106949\n",
      "29600 2.3298686 [2.4163928, 1.2045743, 1.2118185] 2018-05-18 00:13:01.925069\n",
      "29700 2.3227742 [1.7235248, 0.8502989, 0.8732259] 2018-05-18 00:13:38.426340\n",
      "Skipped batch, length 24495\n",
      "29800 2.192633 [2.395811, 1.2027817, 1.1930294] 2018-05-18 00:14:06.343784\n",
      "29900 2.243322 [2.3152752, 1.1633105, 1.1519647] 2018-05-18 00:14:31.583567\n",
      "30000 2.2245443 [2.107895, 1.0510211, 1.0568738] 2018-05-18 00:14:55.147239\n",
      "30100 2.2943285 [2.2453609, 1.1141185, 1.1312423] 2018-05-18 00:15:36.326513\n",
      "30200 2.2587843 [1.9981279, 0.9982203, 0.99990755] 2018-05-18 00:16:06.010145\n",
      "30300 2.2836652 [2.1934495, 1.1742479, 1.0192018] 2018-05-18 00:16:35.357804\n",
      "30400 2.2960298 [1.771353, 0.94099563, 0.8303573] 2018-05-18 00:17:06.675301\n",
      "30500 2.292194 [2.4424143, 1.2264639, 1.2159503] 2018-05-18 00:17:38.574017\n",
      "30600 2.2683778 [2.2270358, 1.1089039, 1.1181319] 2018-05-18 00:18:17.526911\n",
      "30700 2.2188208 [2.1455646, 1.0573868, 1.0881777] 2018-05-18 00:18:43.264636\n",
      "30800 2.249733 [2.8250446, 1.407975, 1.4170698] 2018-05-18 00:19:15.843519\n",
      "30900 2.2175674 [2.3898394, 1.1946776, 1.1951618] 2018-05-18 00:19:48.764840\n",
      "31000 2.1918724 [2.2757087, 1.1347322, 1.1409763] 2018-05-18 00:20:18.476590\n",
      "31100 2.2976174 [2.68329, 1.3428746, 1.3404152] 2018-05-18 00:20:57.292372\n",
      "31200 2.2372086 [2.370302, 1.1827183, 1.1875837] 2018-05-18 00:21:24.117472\n",
      "31300 2.2873569 [2.5939908, 1.2951573, 1.2988336] 2018-05-18 00:22:03.284699\n",
      "31400 2.26518 [2.180435, 1.0866055, 1.0938294] 2018-05-18 00:22:33.571387\n",
      "31500 2.2620296 [2.529921, 1.2614932, 1.268428] 2018-05-18 00:22:59.794973\n",
      "31600 2.27937 [1.8334095, 0.9167659, 0.9166437] 2018-05-18 00:23:35.393060\n",
      "31700 2.2828982 [2.128044, 1.0639346, 1.0641093] 2018-05-18 00:24:07.584321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31800 2.236023 [2.806349, 1.401418, 1.4049311] 2018-05-18 00:24:37.344193\n",
      "31900 2.303618 [2.4530742, 1.2271597, 1.2259145] 2018-05-18 00:25:13.758132\n",
      "32000 2.2337065 [2.5493639, 1.2730412, 1.2763226] 2018-05-18 00:25:45.892794\n",
      "32100 2.3117561 [2.1894956, 1.0911711, 1.0983243] 2018-05-18 00:26:17.562669\n",
      "32200 2.3097055 [2.133678, 1.0713248, 1.0623533] 2018-05-18 00:26:46.224397\n",
      "32300 2.2397285 [2.6237378, 1.3215547, 1.3021832] 2018-05-18 00:27:13.358396\n",
      "32400 2.283548 [2.653016, 1.3365948, 1.3164213] 2018-05-18 00:27:48.188267\n",
      "32500 2.2468317 [2.0372777, 0.99995154, 1.0373261] 2018-05-18 00:28:18.491491\n",
      "32600 2.2462242 [1.7577188, 0.8872699, 0.8704488] 2018-05-18 00:28:43.602678\n",
      "32700 2.1991243 [1.5705013, 0.89991146, 0.67058986] 2018-05-18 00:29:09.282328\n",
      "32800 2.1917574 [1.9440616, 0.97490835, 0.9691533] 2018-05-18 00:29:37.881625\n",
      "32900 2.2790017 [2.536976, 1.258148, 1.2788281] 2018-05-18 00:30:13.576781\n",
      "33000 2.2471008 [2.0540156, 1.0245157, 1.0295] 2018-05-18 00:30:53.750750\n",
      "33100 2.1580236 [2.1646824, 1.0785999, 1.0860823] 2018-05-18 00:31:25.711464\n",
      "33200 2.3902855 [2.2155914, 1.1016368, 1.1139545] 2018-05-18 00:32:13.761663\n",
      "33300 2.2859044 [1.98194, 0.97223467, 1.0097053] 2018-05-18 00:32:53.683757\n",
      "Skipped batch, length 33628\n",
      "33400 2.1869483 [2.0100353, 0.99207866, 1.0179566] 2018-05-18 00:33:24.797121\n",
      "33500 2.233264 [2.3627195, 1.1837752, 1.1789445] 2018-05-18 00:33:57.316312\n",
      "33600 2.2373195 [2.382526, 1.1854693, 1.1970568] 2018-05-18 00:34:37.848496\n",
      "33700 2.2276325 [1.9326042, 0.9609251, 0.9716791] 2018-05-18 00:35:05.456431\n",
      "Skipped batch, length 166818\n",
      "33800 2.251153 [2.4281106, 1.2153149, 1.2127957] 2018-05-18 00:35:31.347388\n",
      "33900 2.248855 [2.3731322, 1.18954, 1.1835921] 2018-05-18 00:36:03.532632\n",
      "34000 2.2466712 [2.4530802, 1.2207999, 1.2322804] 2018-05-18 00:36:32.610522\n",
      "34100 2.260589 [2.0408533, 1.0209014, 1.0199518] 2018-05-18 00:37:03.149376\n",
      "34200 2.236099 [1.101013, 0.6528106, 0.44820243] 2018-05-18 00:37:35.558638\n",
      "34300 2.2304223 [2.3023994, 1.144269, 1.1581304] 2018-05-18 00:38:08.882466\n",
      "34400 2.2863207 [2.5813212, 1.2883795, 1.2929417] 2018-05-18 00:38:47.156828\n",
      "34500 2.266937 [1.9309299, 0.96813583, 0.96279407] 2018-05-18 00:39:18.768659\n",
      "34600 2.2250667 [2.487863, 1.251674, 1.2361889] 2018-05-18 00:39:51.811752\n",
      "34700 2.2712295 [2.694282, 1.3116038, 1.3826783] 2018-05-18 00:40:24.952446\n",
      "34800 2.3344488 [2.7290215, 1.3627269, 1.3662945] 2018-05-18 00:41:01.682402\n",
      "34900 2.3172777 [2.0565524, 1.0226123, 1.0339402] 2018-05-18 00:41:35.367258\n",
      "35000 2.294472 [2.4109466, 1.2017422, 1.2092044] 2018-05-18 00:42:07.772387\n",
      "35100 2.2329574 [2.0443432, 1.0195429, 1.0248003] 2018-05-18 00:42:39.096100\n",
      "35200 2.32273 [2.242138, 1.1116283, 1.1305095] 2018-05-18 00:43:12.847920\n",
      "35300 2.2301362 [2.3242974, 1.165114, 1.1591835] 2018-05-18 00:43:44.405486\n",
      "35400 2.243344 [2.3456957, 1.1732922, 1.1724036] 2018-05-18 00:44:09.459146\n",
      "35500 2.27822 [1.5195389, 0.8492121, 0.6703267] 2018-05-18 00:44:38.279909\n",
      "Skipped batch, length 82351\n",
      "35600 2.2581851 [2.9261672, 1.4585977, 1.4675696] 2018-05-18 00:45:06.495341\n",
      "35700 2.317746 [2.2135935, 1.1148887, 1.0987049] 2018-05-18 00:45:41.460593\n",
      "35800 2.2590945 [2.3213835, 1.1576638, 1.1637197] 2018-05-18 00:46:08.929375\n",
      "35900 2.251452 [2.3517575, 1.1736195, 1.1781379] 2018-05-18 00:46:40.631730\n",
      "36000 2.263669 [2.2823572, 1.1408163, 1.1415408] 2018-05-18 00:47:11.988266\n",
      "36100 2.1675189 [2.4092417, 1.2008778, 1.2083639] 2018-05-18 00:47:40.317673\n",
      "36200 2.28234 [2.5541883, 1.2758114, 1.2783768] 2018-05-18 00:48:13.180395\n",
      "36300 2.1686516 [2.2615013, 1.1351542, 1.126347] 2018-05-18 00:48:47.515151\n",
      "Skipped batch, length 26621\n",
      "36400 2.258319 [2.612197, 1.3181385, 1.2940583] 2018-05-18 00:49:15.912498\n",
      "36500 2.226384 [1.9382918, 0.97488636, 0.9634055] 2018-05-18 00:49:50.532278\n",
      "36600 2.232592 [2.091557, 1.0422887, 1.0492684] 2018-05-18 00:50:21.954604\n",
      "36700 2.2525413 [2.4817524, 1.239713, 1.2420394] 2018-05-18 00:50:56.137923\n",
      "36800 2.2900262 [2.5336108, 1.2636113, 1.2699996] 2018-05-18 00:51:28.867339\n",
      "36900 2.2074015 [1.8274028, 0.9047328, 0.92267007] 2018-05-18 00:51:56.875362\n",
      "37000 2.206686 [2.5403328, 1.2637908, 1.276542] 2018-05-18 00:52:26.859219\n",
      "37100 2.2472627 [2.7290688, 1.3731282, 1.3559405] 2018-05-18 00:53:00.695838\n",
      "37200 2.223449 [2.3283021, 1.1667843, 1.1615179] 2018-05-18 00:53:31.464021\n",
      "37300 2.320516 [2.744441, 1.3775411, 1.3669001] 2018-05-18 00:54:01.496428\n",
      "37400 2.2723799 [2.1924431, 1.0983834, 1.0940597] 2018-05-18 00:54:32.969676\n",
      "37500 2.1944027 [2.5157018, 1.2569311, 1.2587707] 2018-05-18 00:55:01.309723\n",
      "37600 2.3194382 [2.1875896, 1.0779713, 1.1096182] 2018-05-18 00:55:35.839728\n",
      "37700 2.2654293 [2.0296597, 1.0053291, 1.0243307] 2018-05-18 00:56:08.373576\n",
      "37800 2.2450676 [2.7800956, 1.3892365, 1.3908591] 2018-05-18 00:56:57.009947\n",
      "37900 2.2265058 [2.5621495, 1.2795856, 1.2825639] 2018-05-18 00:57:27.079717\n",
      "38000 2.2083292 [2.1907973, 1.0979613, 1.0928359] 2018-05-18 00:57:50.259227\n",
      "38100 2.25166 [2.1576052, 1.0673674, 1.0902379] 2018-05-18 00:58:27.397468\n",
      "38200 2.182254 [2.471088, 1.2345924, 1.2364955] 2018-05-18 00:58:57.853754\n",
      "38300 2.2164042 [2.2633076, 1.1715425, 1.0917652] 2018-05-18 00:59:27.264191\n",
      "38400 2.2309442 [2.416389, 1.2291019, 1.187287] 2018-05-18 01:00:04.826573\n",
      "38500 2.2449396 [2.1373634, 1.0688784, 1.0684849] 2018-05-18 01:00:31.553598\n",
      "38600 2.2471006 [1.825587, 0.9075048, 0.91808224] 2018-05-18 01:01:05.380814\n",
      "38700 2.3048003 [2.7481713, 1.3687426, 1.3794287] 2018-05-18 01:01:39.648052\n",
      "38800 2.2784548 [2.4117355, 1.2898228, 1.1219128] 2018-05-18 01:02:11.130890\n",
      "38900 2.283968 [2.0579631, 1.0246768, 1.0332863] 2018-05-18 01:02:43.290859\n",
      "39000 2.206808 [2.4811134, 1.2414868, 1.2396266] 2018-05-18 01:03:20.608867\n",
      "39100 2.2500486 [1.9225533, 0.9542927, 0.9682605] 2018-05-18 01:03:59.428051\n",
      "39200 2.2690198 [2.3320134, 1.1674323, 1.1645811] 2018-05-18 01:04:31.226770\n",
      "39300 2.2222874 [2.592385, 1.319098, 1.273287] 2018-05-18 01:05:07.014129\n",
      "39400 2.2351034 [2.340278, 1.1704265, 1.1698514] 2018-05-18 01:05:39.024287\n",
      "39500 2.1911063 [2.91868, 1.4597068, 1.4589732] 2018-05-18 01:06:11.147332\n",
      "39600 2.2545333 [1.8865017, 0.94330466, 0.943197] 2018-05-18 01:06:47.980458\n",
      "39700 2.2342565 [2.5837352, 1.2922502, 1.2914851] 2018-05-18 01:07:15.433466\n",
      "39800 2.2540445 [0.9636781, 0.5698991, 0.39377904] 2018-05-18 01:07:53.567103\n",
      "39900 2.2849522 [2.1416295, 1.0718255, 1.069804] 2018-05-18 01:08:35.633661\n",
      "40000 2.2395816 [1.995024, 0.99817055, 0.9968534] 2018-05-18 01:09:09.426697\n",
      "40100 2.1805146 [1.9522998, 0.9768636, 0.9754362] 2018-05-18 01:09:36.562266\n",
      "40200 2.2733703 [2.7680385, 1.3834713, 1.3845673] 2018-05-18 01:10:07.258939\n",
      "40300 2.2093601 [2.2636433, 1.1273394, 1.1363039] 2018-05-18 01:10:36.299996\n",
      "40400 2.2370172 [2.4506736, 1.2157434, 1.2349303] 2018-05-18 01:11:26.386499\n",
      "40500 2.198828 [2.4784074, 1.2389345, 1.2394729] 2018-05-18 01:11:58.984690\n",
      "40600 2.29111 [2.2555208, 1.1367011, 1.1188196] 2018-05-18 01:12:33.866409\n",
      "40700 2.2629867 [2.357193, 1.1736484, 1.1835448] 2018-05-18 01:13:06.188279\n",
      "40800 2.230129 [2.1873968, 1.0940763, 1.0933205] 2018-05-18 01:13:43.639155\n",
      "40900 2.2510653 [2.655119, 1.332629, 1.3224899] 2018-05-18 01:14:17.517171\n",
      "41000 2.2369938 [2.4729435, 1.2372382, 1.2357054] 2018-05-18 01:14:46.170869\n",
      "41100 2.2268527 [1.6857469, 0.85296583, 0.832781] 2018-05-18 01:15:16.829126\n",
      "41200 2.2789254 [3.3344502, 1.6999505, 1.6344999] 2018-05-18 01:15:48.843845\n",
      "41300 2.275776 [2.583827, 1.2893587, 1.2944682] 2018-05-18 01:16:30.685645\n",
      "41400 2.1972609 [1.7463624, 0.8692982, 0.87706417] 2018-05-18 01:16:57.430818\n",
      "41500 2.1685119 [1.9238186, 0.9508573, 0.9729613] 2018-05-18 01:17:32.087818\n",
      "41600 2.1819694 [2.5363889, 1.268215, 1.268174] 2018-05-18 01:18:22.040920\n",
      "41700 2.2759442 [2.543825, 1.2711112, 1.2727137] 2018-05-18 01:18:54.975165\n",
      "41800 2.1950927 [2.009397, 1.0041004, 1.0052967] 2018-05-18 01:19:21.564014\n",
      "41900 2.237567 [2.2971332, 1.1465898, 1.1505435] 2018-05-18 01:20:01.197161\n",
      "42000 2.28426 [2.973575, 1.4809508, 1.4926244] 2018-05-18 01:20:33.190779\n",
      "42100 2.2483084 [2.3618867, 1.1791947, 1.182692] 2018-05-18 01:21:03.282924\n",
      "42200 2.2856343 [1.8669281, 0.9116665, 0.9552616] 2018-05-18 01:21:32.979346\n",
      "42300 2.26395 [3.2173767, 1.5984516, 1.6189251] 2018-05-18 01:22:05.605224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42400 2.2664797 [2.1795995, 1.0885112, 1.0910883] 2018-05-18 01:22:39.147634\n",
      "42500 2.2783582 [2.5148408, 1.2594495, 1.2553914] 2018-05-18 01:23:12.063195\n",
      "42600 2.2273333 [2.1567652, 1.0743139, 1.0824513] 2018-05-18 01:23:52.702493\n",
      "42700 2.300086 [2.5005093, 1.2520039, 1.2485054] 2018-05-18 01:24:35.731792\n",
      "42800 2.215707 [2.3287165, 1.1579876, 1.1707289] 2018-05-18 01:25:07.566776\n",
      "42900 2.196726 [2.5102673, 1.2245252, 1.285742] 2018-05-18 01:25:32.927429\n",
      "43000 2.212716 [2.2244308, 1.1025518, 1.121879] 2018-05-18 01:26:06.732195\n",
      "43100 2.2396617 [2.3004425, 1.1474694, 1.152973] 2018-05-18 01:26:42.992168\n",
      "43200 2.2447672 [2.147526, 1.0634689, 1.0840571] 2018-05-18 01:27:14.369503\n",
      "43300 2.2703354 [2.8780468, 1.4390264, 1.4390204] 2018-05-18 01:27:45.433107\n",
      "43400 2.255196 [1.9787679, 0.9851738, 0.99359405] 2018-05-18 01:28:18.918436\n",
      "43500 2.2262273 [1.2499305, 0.72988796, 0.52004254] 2018-05-18 01:28:52.243894\n",
      "43600 2.195516 [2.4111013, 1.2013303, 1.2097709] 2018-05-18 01:29:23.581304\n",
      "43700 2.214989 [1.960775, 0.9782914, 0.9824836] 2018-05-18 01:29:59.374773\n",
      "43800 2.1951954 [2.2678835, 1.1302192, 1.1376643] 2018-05-18 01:30:39.787164\n",
      "43900 2.2174811 [2.3603776, 1.1874762, 1.1729014] 2018-05-18 01:31:24.396882\n",
      "44000 2.229375 [2.1753707, 1.0701848, 1.1051857] 2018-05-18 01:31:56.521771\n",
      "44100 2.2240007 [2.031786, 1.0160356, 1.0157503] 2018-05-18 01:32:36.580836\n",
      "44200 2.1840408 [2.4308164, 1.2162004, 1.2146161] 2018-05-18 01:33:01.875061\n",
      "44300 2.2157402 [1.8061506, 0.89292395, 0.9132266] 2018-05-18 01:33:27.823622\n",
      "44400 2.1922226 [2.23355, 1.1120237, 1.1215265] 2018-05-18 01:34:02.690103\n",
      "44500 2.1920185 [2.3807764, 1.176187, 1.2045895] 2018-05-18 01:34:29.063632\n",
      "44600 2.2926595 [2.3618538, 1.1796987, 1.1821551] 2018-05-18 01:35:21.769503\n",
      "44700 2.1847281 [2.1893144, 1.0911288, 1.0981855] 2018-05-18 01:36:09.528955\n",
      "44800 2.1918201 [1.8890096, 0.9298196, 0.95919] 2018-05-18 01:36:49.578468\n",
      "44900 2.214015 [1.4081926, 0.8104607, 0.59773195] 2018-05-18 01:37:23.675601\n",
      "45000 2.24214 [2.509904, 1.2533877, 1.2565162] 2018-05-18 01:37:59.080062\n",
      "45100 2.3076813 [2.626596, 1.302326, 1.32427] 2018-05-18 01:38:31.580198\n",
      "45200 2.2520993 [2.1402988, 1.066125, 1.0741737] 2018-05-18 01:39:09.502033\n",
      "45300 2.21645 [2.3394704, 1.1755412, 1.1639291] 2018-05-18 01:39:38.388407\n",
      "45400 2.1997716 [2.4863062, 1.238474, 1.2478323] 2018-05-18 01:40:06.910657\n",
      "45500 2.223586 [2.3905032, 1.2018815, 1.1886216] 2018-05-18 01:40:35.271896\n",
      "45600 2.2503293 [2.6447394, 1.3223467, 1.3223927] 2018-05-18 01:41:13.917410\n",
      "45700 2.2226152 [2.2950473, 1.1542294, 1.1408179] 2018-05-18 01:41:43.399675\n",
      "45800 2.1955628 [1.9608271, 0.9793609, 0.98146623] 2018-05-18 01:42:11.463208\n",
      "45900 2.1926262 [1.9537, 0.9768981, 0.9768019] 2018-05-18 01:42:39.922954\n",
      "46000 2.2504525 [2.506105, 1.2434297, 1.2626752] 2018-05-18 01:43:14.965926\n",
      "46100 2.1886759 [2.3573961, 1.1775512, 1.1798449] 2018-05-18 01:43:45.741542\n",
      "46200 2.2056386 [2.0093112, 1.0169109, 0.9924002] 2018-05-18 01:44:15.555028\n",
      "46300 2.1926746 [1.0017462, 0.6100038, 0.3917424] 2018-05-18 01:44:42.493530\n",
      "46400 2.281806 [2.5018048, 1.2413425, 1.2604623] 2018-05-18 01:45:24.251192\n",
      "46500 2.2131934 [2.233652, 1.0972373, 1.1364148] 2018-05-18 01:45:50.598835\n",
      "46600 2.2048488 [1.0739475, 0.67031693, 0.40363058] 2018-05-18 01:46:23.769279\n",
      "46700 2.223346 [2.3721151, 1.1880155, 1.1840997] 2018-05-18 01:46:55.609489\n",
      "46800 2.306338 [2.268054, 1.127922, 1.140132] 2018-05-18 01:47:30.901971\n",
      "46900 2.253725 [2.192884, 1.0958561, 1.0970278] 2018-05-18 01:48:06.211677\n",
      "47000 2.208559 [2.5037115, 1.2505035, 1.2532079] 2018-05-18 01:48:49.488615\n",
      "47100 2.2323916 [2.996945, 1.5001801, 1.4967647] 2018-05-18 01:49:18.915184\n",
      "47200 2.1737897 [1.8303752, 0.8996322, 0.930743] 2018-05-18 01:49:47.299943\n",
      "47300 2.147141 [2.0553532, 1.0219014, 1.0334519] 2018-05-18 01:50:25.368513\n",
      "47400 2.2329612 [1.9638209, 0.9820212, 0.98179966] 2018-05-18 01:50:57.222314\n",
      "47500 2.1530764 [2.7336135, 1.3695061, 1.3641074] 2018-05-18 01:51:24.281836\n",
      "47600 2.2461843 [2.2891011, 1.1452452, 1.1438558] 2018-05-18 01:51:54.542264\n",
      "47700 2.301305 [2.0288138, 1.0094718, 1.0193422] 2018-05-18 01:52:34.803422\n",
      "47800 2.1854882 [2.4695594, 1.2343559, 1.2352035] 2018-05-18 01:53:02.396869\n",
      "47900 2.2269075 [1.9823465, 0.98468864, 0.99765784] 2018-05-18 01:53:34.982942\n",
      "48000 2.1827857 [2.112915, 1.0498687, 1.0630462] 2018-05-18 01:54:05.438562\n",
      "48100 2.215384 [1.5901837, 0.91924465, 0.670939] 2018-05-18 01:54:36.086627\n",
      "Skipped batch, length 25010\n",
      "48200 2.1581202 [1.2323477, 0.71716756, 0.51518023] 2018-05-18 01:55:03.231371\n",
      "48300 2.2480423 [2.1440294, 1.0647118, 1.0793176] 2018-05-18 01:55:47.752955\n",
      "48400 2.2354121 [1.8575466, 0.92567456, 0.93187207] 2018-05-18 01:56:23.855386\n",
      "48500 2.229989 [2.247014, 1.1263217, 1.1206925] 2018-05-18 01:56:56.039274\n",
      "48600 2.1738563 [2.620439, 1.3071983, 1.3132409] 2018-05-18 01:57:29.401260\n",
      "48700 2.1568284 [2.1162066, 1.0588362, 1.0573704] 2018-05-18 01:57:56.753862\n",
      "48800 2.179343 [2.1746306, 1.0700436, 1.1045872] 2018-05-18 01:58:23.197877\n",
      "48900 2.2214165 [1.1887994, 0.7402053, 0.44859403] 2018-05-18 01:58:53.306858\n",
      "Skipped batch, length 107183\n",
      "49000 2.170066 [2.528543, 1.2574939, 1.2710493] 2018-05-18 01:59:30.423317\n",
      "49100 2.2362573 [2.4044266, 1.2036257, 1.2008008] 2018-05-18 02:00:04.184801\n",
      "49200 2.2971952 [2.413055, 1.2086291, 1.2044258] 2018-05-18 02:00:35.324266\n",
      "49300 2.2369998 [3.1748366, 1.5948849, 1.5799518] 2018-05-18 02:01:25.750067\n",
      "49400 2.1458013 [1.2748418, 0.75259995, 0.52224183] 2018-05-18 02:01:53.286577\n",
      "49500 2.1664915 [1.9602499, 0.9843414, 0.97590846] 2018-05-18 02:02:26.775768\n",
      "49600 2.1642747 [2.277581, 1.1389539, 1.138627] 2018-05-18 02:02:56.963229\n",
      "49700 2.2364607 [2.377892, 1.1898476, 1.1880443] 2018-05-18 02:03:28.036747\n",
      "49800 2.1929386 [2.689824, 1.3463786, 1.3434455] 2018-05-18 02:03:53.813163\n",
      "49900 2.234006 [1.1785069, 0.69158244, 0.48692438] 2018-05-18 02:04:31.990039\n",
      "50000 2.1826696 [2.0889697, 1.0306561, 1.0583136] 2018-05-18 02:05:05.206397\n",
      "50100 2.2789376 [2.1273115, 1.0601057, 1.0672058] 2018-05-18 02:05:36.113696\n",
      "Skipped batch, length 23697\n",
      "50200 2.1964846 [2.4077191, 1.2035496, 1.2041695] 2018-05-18 02:06:08.662521\n",
      "50300 2.2384784 [2.2882264, 1.1456504, 1.142576] 2018-05-18 02:06:42.600469\n",
      "50400 2.1788757 [1.7203047, 0.8545189, 0.8657858] 2018-05-18 02:07:18.237660\n",
      "50500 2.2462094 [2.466299, 1.2271259, 1.2391733] 2018-05-18 02:08:00.271575\n",
      "50600 2.210719 [2.0983171, 1.0513093, 1.0470078] 2018-05-18 02:08:33.262950\n",
      "50700 2.2024915 [1.9989445, 0.9955327, 1.0034118] 2018-05-18 02:08:56.340929\n",
      "50800 2.1902308 [2.1075385, 1.0519458, 1.0555927] 2018-05-18 02:09:27.890164\n",
      "50900 2.2294292 [2.064548, 1.024327, 1.040221] 2018-05-18 02:10:01.533430\n",
      "51000 2.175041 [1.9786088, 1.0137954, 0.9648134] 2018-05-18 02:10:28.619589\n",
      "51100 2.1998425 [2.282105, 1.1444772, 1.1376278] 2018-05-18 02:10:58.706919\n",
      "51200 2.238027 [2.0176306, 0.9988087, 1.018822] 2018-05-18 02:11:40.351784\n",
      "51300 2.165656 [3.0184212, 1.4854467, 1.5329746] 2018-05-18 02:12:12.104932\n",
      "51400 2.1851408 [2.6106277, 1.3102593, 1.3003683] 2018-05-18 02:12:45.319865\n",
      "51500 2.259141 [2.3368905, 1.1608136, 1.1760769] 2018-05-18 02:13:12.232717\n",
      "51600 2.230258 [1.9996537, 0.977864, 1.0217897] 2018-05-18 02:13:57.339919\n",
      "51700 2.1905217 [2.4785142, 1.2439916, 1.2345226] 2018-05-18 02:14:32.542099\n",
      "51800 2.2330315 [2.6064835, 1.3025825, 1.303901] 2018-05-18 02:15:07.679035\n",
      "51900 2.1932855 [1.8293564, 0.9053351, 0.92402124] 2018-05-18 02:15:44.969904\n",
      "52000 2.2841818 [2.2495518, 1.1270436, 1.1225082] 2018-05-18 02:16:20.877739\n",
      "52100 2.2318013 [2.520606, 1.2550473, 1.2655588] 2018-05-18 02:17:05.639050\n",
      "52200 2.1949115 [2.012156, 0.9965522, 1.0156038] 2018-05-18 02:17:39.675815\n",
      "52300 2.1829493 [1.5223975, 0.8517844, 0.67061317] 2018-05-18 02:18:18.424101\n",
      "52400 2.2388935 [2.751193, 1.3567467, 1.3944465] 2018-05-18 02:18:47.670374\n",
      "52500 2.1856625 [2.6849687, 1.3473232, 1.3376455] 2018-05-18 02:19:14.285655\n",
      "52600 2.230256 [2.477071, 1.244256, 1.232815] 2018-05-18 02:19:43.589164\n",
      "52700 2.279721 [2.4339561, 1.216465, 1.2174911] 2018-05-18 02:20:20.174132\n",
      "52800 2.1670713 [2.033937, 1.0147365, 1.0192006] 2018-05-18 02:20:59.187180\n",
      "52900 2.2605646 [1.8213362, 0.9048432, 0.91649294] 2018-05-18 02:21:35.402769\n",
      "53000 2.1772087 [2.1594882, 1.0781782, 1.0813099] 2018-05-18 02:22:06.723207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53100 2.253319 [2.1936877, 1.0930903, 1.1005974] 2018-05-18 02:22:58.356014\n",
      "53200 2.2150185 [2.1277351, 1.0678563, 1.0598788] 2018-05-18 02:23:28.034217\n",
      "53300 2.2057688 [1.9518353, 0.9735726, 0.97826266] 2018-05-18 02:23:58.354312\n",
      "53400 2.2354794 [2.492362, 1.2454345, 1.2469274] 2018-05-18 02:24:34.112676\n",
      "53500 2.235122 [2.7096558, 1.3602731, 1.3493828] 2018-05-18 02:25:03.921674\n",
      "53600 2.2175338 [2.4728956, 1.2350533, 1.2378423] 2018-05-18 02:25:36.687882\n",
      "53700 2.1074772 [2.4686058, 1.2311696, 1.2374362] 2018-05-18 02:26:04.874086\n",
      "53800 2.2873757 [1.9338028, 0.97105014, 0.96275264] 2018-05-18 02:26:42.794348\n",
      "53900 2.2258854 [2.2218528, 1.1123343, 1.1095184] 2018-05-18 02:27:15.266731\n",
      "54000 2.2367501 [2.0097804, 1.015731, 0.9940495] 2018-05-18 02:28:00.244372\n",
      "54100 2.212328 [1.2505546, 0.763857, 0.48669752] 2018-05-18 02:28:29.317352\n",
      "54200 2.2774644 [2.7271485, 1.3649645, 1.362184] 2018-05-18 02:29:07.511082\n",
      "54300 2.2279398 [2.430985, 1.2131505, 1.2178344] 2018-05-18 02:29:38.536043\n",
      "54400 2.197747 [2.479855, 1.2413452, 1.23851] 2018-05-18 02:30:06.741407\n",
      "54500 2.1864066 [2.5750413, 1.3618089, 1.2132325] 2018-05-18 02:30:36.288344\n",
      "54600 2.194242 [2.396638, 1.1931171, 1.2035208] 2018-05-18 02:31:05.006989\n",
      "54700 2.2104383 [2.1251078, 1.0681905, 1.0569173] 2018-05-18 02:31:40.330296\n",
      "54800 2.2075882 [2.7929988, 1.37119, 1.421809] 2018-05-18 02:32:10.616576\n",
      "54900 2.1659849 [2.2646518, 1.1361687, 1.1284829] 2018-05-18 02:32:43.183216\n",
      "55000 2.2078927 [2.4540796, 1.2069144, 1.2471652] 2018-05-18 02:33:18.329927\n",
      "55100 2.1951017 [2.4477863, 1.2270346, 1.2207518] 2018-05-18 02:33:49.081706\n",
      "55200 2.2727873 [2.55695, 1.2575797, 1.2993704] 2018-05-18 02:34:24.601297\n",
      "55300 2.2088234 [1.8966954, 0.9346712, 0.9620241] 2018-05-18 02:34:59.631298\n",
      "55400 2.2464204 [1.9588561, 0.96356875, 0.9952874] 2018-05-18 02:35:35.508110\n",
      "55500 2.331036 [2.0771801, 1.02554, 1.0516402] 2018-05-18 02:36:16.606333\n",
      "55600 2.2148633 [2.0871537, 1.051261, 1.0358927] 2018-05-18 02:36:59.367769\n",
      "55700 2.2130318 [2.4295297, 1.2178519, 1.2116778] 2018-05-18 02:37:34.517778\n",
      "55800 2.2135208 [1.9193317, 0.9508896, 0.9684421] 2018-05-18 02:37:59.420894\n",
      "55900 2.1286607 [1.4429381, 0.7512059, 0.6917321] 2018-05-18 02:38:28.012494\n",
      "56000 2.218969 [1.9521855, 0.9676503, 0.9845352] 2018-05-18 02:38:54.177811\n",
      "56100 2.2290235 [1.5015627, 0.8541326, 0.6474301] 2018-05-18 02:39:35.858812\n",
      "56200 2.2222495 [2.0947847, 1.0386635, 1.0561213] 2018-05-18 02:40:15.378457\n",
      "56300 2.2645426 [1.8512312, 0.92278624, 0.928445] 2018-05-18 02:40:52.842429\n",
      "56400 2.155322 [2.6539903, 1.3505888, 1.3034014] 2018-05-18 02:41:20.010100\n",
      "56500 2.1909184 [2.146594, 1.0747024, 1.0718918] 2018-05-18 02:41:57.469299\n",
      "56600 2.2265785 [2.473977, 1.2369215, 1.2370557] 2018-05-18 02:42:23.603443\n",
      "56700 2.2173977 [2.238687, 1.1224618, 1.1162252] 2018-05-18 02:43:01.031065\n",
      "56800 2.2320216 [2.3692346, 1.182765, 1.1864696] 2018-05-18 02:43:37.404848\n",
      "56900 2.272647 [2.7593813, 1.3823032, 1.377078] 2018-05-18 02:44:07.704858\n",
      "57000 2.206528 [2.1962967, 1.0922837, 1.1040131] 2018-05-18 02:44:39.146827\n",
      "57100 2.2404892 [2.2468934, 1.1226013, 1.1242921] 2018-05-18 02:45:16.929204\n",
      "57200 2.2229922 [2.2280214, 1.1104918, 1.1175296] 2018-05-18 02:45:50.195564\n",
      "57300 2.2155585 [2.264586, 1.1312943, 1.1332917] 2018-05-18 02:46:18.960075\n",
      "57400 2.2300296 [2.0177383, 1.0069902, 1.010748] 2018-05-18 02:47:03.419847\n",
      "57500 2.2428138 [2.103194, 1.0492624, 1.0539316] 2018-05-18 02:47:38.609767\n",
      "57600 2.1806173 [2.924561, 1.465767, 1.4587941] 2018-05-18 02:48:08.172357\n",
      "57700 2.2517998 [2.2405875, 1.1237977, 1.1167898] 2018-05-18 02:48:39.336691\n",
      "57800 2.2460601 [1.9171246, 0.957149, 0.9599756] 2018-05-18 02:49:08.274857\n",
      "57900 2.2004938 [2.1554928, 1.0754507, 1.0800422] 2018-05-18 02:49:33.932234\n",
      "58000 2.19437 [1.5847375, 0.890996, 0.69374156] 2018-05-18 02:50:05.040434\n",
      "58100 2.1758938 [2.2835507, 1.1498815, 1.1336694] 2018-05-18 02:50:36.373536\n",
      "58200 2.198465 [2.1314206, 1.0677099, 1.0637106] 2018-05-18 02:51:10.949226\n",
      "58300 2.2154474 [2.483357, 1.2438407, 1.2395163] 2018-05-18 02:51:41.571305\n",
      "58400 2.1899216 [2.0630019, 1.0259036, 1.0370983] 2018-05-18 02:52:17.352694\n",
      "58500 2.2099235 [2.4771557, 1.2478743, 1.2292813] 2018-05-18 02:52:42.904940\n",
      "58600 2.230199 [2.0295246, 1.0089138, 1.0206108] 2018-05-18 02:53:24.604730\n",
      "58700 2.194665 [2.06509, 1.0337791, 1.0313108] 2018-05-18 02:53:58.743691\n",
      "58800 2.313188 [2.7076097, 1.3498287, 1.3577809] 2018-05-18 02:54:38.526328\n",
      "58900 2.181444 [2.210573, 1.1011124, 1.1094606] 2018-05-18 02:55:10.384110\n",
      "59000 2.1886544 [1.9648948, 0.97363937, 0.99125546] 2018-05-18 02:55:34.562917\n",
      "59100 2.1560862 [2.3493822, 1.1749148, 1.1744673] 2018-05-18 02:56:17.119208\n",
      "59200 2.269083 [1.3353133, 0.76434785, 0.5709654] 2018-05-18 02:56:54.789700\n",
      "59300 2.1770804 [1.6724297, 0.8402616, 0.8321681] 2018-05-18 02:57:31.146480\n",
      "59400 2.2194133 [2.3148842, 1.1478624, 1.1670218] 2018-05-18 02:58:07.468787\n",
      "59500 2.1273024 [2.4830017, 1.2441752, 1.2388266] 2018-05-18 02:58:41.568555\n",
      "59600 2.2286854 [2.5026903, 1.2557201, 1.2469702] 2018-05-18 02:59:15.426347\n",
      "59700 2.2260804 [2.8190007, 1.4278473, 1.3911536] 2018-05-18 02:59:44.294823\n",
      "59800 2.202205 [2.2003336, 1.1004043, 1.0999293] 2018-05-18 03:00:17.475941\n",
      "59900 2.2356458 [2.7593555, 1.3967849, 1.3625708] 2018-05-18 03:00:49.810620\n",
      "60000 2.194838 [2.1553, 1.0752293, 1.0800706] 2018-05-18 03:01:23.281241\n",
      "60100 2.1892276 [2.3956761, 1.194725, 1.2009511] 2018-05-18 03:01:49.140307\n",
      "60200 2.160894 [2.32054, 1.1613076, 1.1592324] 2018-05-18 03:02:23.968744\n",
      "60300 2.1260862 [2.1596067, 1.0719589, 1.0876478] 2018-05-18 03:02:51.500038\n",
      "60400 2.2768486 [2.3542562, 1.1812848, 1.1729715] 2018-05-18 03:03:27.434304\n",
      "60500 2.1561832 [2.4161391, 1.215204, 1.2009352] 2018-05-18 03:03:53.965030\n",
      "60600 2.217268 [2.2332385, 1.1143556, 1.1188829] 2018-05-18 03:04:28.122438\n",
      "60700 2.2521822 [2.627282, 1.3121837, 1.3150982] 2018-05-18 03:04:59.309061\n",
      "60800 2.2352567 [2.448697, 1.241616, 1.2070811] 2018-05-18 03:05:31.528287\n",
      "60900 2.1600103 [0.880523, 0.5437576, 0.3367654] 2018-05-18 03:06:05.050466\n",
      "61000 2.1431446 [2.7629986, 1.3735576, 1.3894411] 2018-05-18 03:06:29.339544\n",
      "61100 2.1359882 [3.032477, 1.5520717, 1.4804051] 2018-05-18 03:06:58.507214\n",
      "61200 2.2003062 [2.0034976, 0.98935425, 1.0141432] 2018-05-18 03:07:32.429490\n",
      "61300 2.1353493 [1.3896327, 0.798365, 0.5912677] 2018-05-18 03:08:01.522972\n",
      "Skipped batch, length 21658\n",
      "61400 2.2457156 [2.3553936, 1.1662407, 1.189153] 2018-05-18 03:08:37.579562\n",
      "61500 2.220539 [2.4985824, 1.2536957, 1.2448868] 2018-05-18 03:09:15.426833\n",
      "61600 2.1944854 [2.3258712, 1.1591125, 1.1667588] 2018-05-18 03:09:47.220058\n",
      "61700 2.088221 [2.003662, 1.0040369, 0.9996252] 2018-05-18 03:10:18.268497\n",
      "61800 2.239044 [2.4250822, 1.2087595, 1.2163227] 2018-05-18 03:10:47.168459\n",
      "61900 2.2384057 [2.1925435, 1.0860944, 1.1064491] 2018-05-18 03:11:20.803684\n",
      "62000 2.2467244 [2.5930047, 1.2828008, 1.3102038] 2018-05-18 03:11:56.245383\n",
      "62100 2.146969 [2.387075, 1.1974235, 1.1896515] 2018-05-18 03:12:25.783236\n",
      "62200 2.2156954 [2.4194546, 1.2066206, 1.2128341] 2018-05-18 03:12:58.867737\n",
      "62300 2.1642306 [2.2013664, 1.0852909, 1.1160755] 2018-05-18 03:13:27.509704\n",
      "62400 2.2003324 [2.368149, 1.1875994, 1.1805496] 2018-05-18 03:13:54.087811\n",
      "62500 2.2420127 [2.345349, 1.1693966, 1.1759524] 2018-05-18 03:14:28.138725\n",
      "62600 2.2158895 [2.706887, 1.3527665, 1.3541205] 2018-05-18 03:15:10.262572\n",
      "62700 2.160593 [0.93989015, 0.5711261, 0.36876407] 2018-05-18 03:15:47.835868\n",
      "62800 2.1964724 [2.3617933, 1.1787562, 1.183037] 2018-05-18 03:16:17.892533\n",
      "62900 2.219649 [2.515169, 1.2600282, 1.2551407] 2018-05-18 03:16:44.407635\n",
      "63000 2.162355 [1.5545597, 0.8785157, 0.67604405] 2018-05-18 03:17:12.900321\n",
      "63100 2.1963212 [2.910246, 1.4574245, 1.4528213] 2018-05-18 03:17:41.919570\n",
      "63200 2.2199256 [1.6107249, 0.9153861, 0.6953388] 2018-05-18 03:18:27.350687\n",
      "63300 2.173218 [2.443797, 1.220193, 1.223604] 2018-05-18 03:18:54.966121\n",
      "63400 2.1930356 [2.014792, 1.006059, 1.0087329] 2018-05-18 03:19:24.639296\n",
      "63500 2.2188764 [1.8686193, 0.92825603, 0.9403633] 2018-05-18 03:20:01.714136\n",
      "63600 2.174811 [2.0851727, 1.027378, 1.0577946] 2018-05-18 03:20:30.725247\n",
      "63700 2.2312918 [2.3997355, 1.1929849, 1.2067505] 2018-05-18 03:21:03.202820\n",
      "63800 2.2538445 [2.1980686, 1.0925076, 1.1055611] 2018-05-18 03:21:38.810626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63900 2.1027732 [1.89846, 0.9503747, 0.9480853] 2018-05-18 03:22:07.388680\n",
      "64000 2.203537 [2.2029505, 1.0966074, 1.106343] 2018-05-18 03:22:42.115943\n",
      "64100 2.185153 [2.4071, 1.2019227, 1.2051773] 2018-05-18 03:23:20.986105\n",
      "64200 2.2439175 [2.0838654, 1.0404716, 1.0433939] 2018-05-18 03:23:59.468772\n",
      "64300 2.145346 [2.2766838, 1.1257792, 1.1509045] 2018-05-18 03:24:28.538252\n",
      "Skipped batch, length 23047\n",
      "64400 2.162682 [1.9630284, 0.97681904, 0.98620945] 2018-05-18 03:25:04.999038\n",
      "64500 2.2213128 [1.7338767, 0.8647343, 0.8691425] 2018-05-18 03:25:35.546504\n",
      "64600 2.197621 [2.580995, 1.2871606, 1.2938344] 2018-05-18 03:26:09.152893\n",
      "64700 2.1302743 [1.8584487, 0.9212495, 0.9371992] 2018-05-18 03:26:42.940713\n",
      "64800 2.2365155 [1.8440353, 0.91302097, 0.9310143] 2018-05-18 03:27:16.013258\n",
      "64900 2.170166 [2.9730663, 1.4966303, 1.4764361] 2018-05-18 03:27:44.041060\n",
      "65000 2.2265007 [2.199296, 1.100798, 1.098498] 2018-05-18 03:28:24.326150\n",
      "65100 2.248201 [2.7047849, 1.3489587, 1.3558261] 2018-05-18 03:29:04.972117\n",
      "65200 2.2021775 [2.2577186, 1.1265941, 1.1311246] 2018-05-18 03:29:36.541192\n",
      "65300 2.2149878 [2.7007968, 1.3403974, 1.3603995] 2018-05-18 03:30:05.983109\n",
      "65400 2.2024457 [2.4427252, 1.217701, 1.2250242] 2018-05-18 03:30:36.095578\n",
      "65500 2.2025726 [2.393908, 1.1901652, 1.203743] 2018-05-18 03:31:07.822768\n",
      "65600 2.2189548 [1.0173607, 0.6131443, 0.40421635] 2018-05-18 03:31:47.131239\n",
      "65700 2.2084823 [2.4420443, 1.2198786, 1.2221657] 2018-05-18 03:32:17.548667\n",
      "65800 2.1503549 [2.6483407, 1.3216792, 1.3266616] 2018-05-18 03:32:53.005407\n",
      "65900 2.182648 [2.4307892, 1.2199606, 1.2108287] 2018-05-18 03:33:16.860953\n",
      "66000 2.1825457 [2.391705, 1.1965828, 1.1951222] 2018-05-18 03:33:55.685569\n",
      "66100 2.1403246 [2.383595, 1.182679, 1.200916] 2018-05-18 03:34:31.985351\n",
      "66200 2.1823761 [2.233234, 1.1177344, 1.1154996] 2018-05-18 03:34:57.775132\n",
      "66300 2.2635148 [2.514872, 1.2655468, 1.2493253] 2018-05-18 03:35:37.550113\n",
      "66400 2.103918 [2.2164626, 1.1044909, 1.1119719] 2018-05-18 03:36:02.246874\n",
      "66500 2.267831 [2.6272926, 1.3141708, 1.3131217] 2018-05-18 03:36:40.983147\n",
      "66600 2.1457038 [2.109168, 1.0541376, 1.0550306] 2018-05-18 03:37:09.289992\n",
      "66700 2.2546618 [2.195843, 1.0990598, 1.0967832] 2018-05-18 03:37:42.459586\n",
      "66800 2.200504 [2.3283694, 1.1649992, 1.1633701] 2018-05-18 03:38:12.413243\n",
      "66900 2.2434685 [2.5081766, 1.2543507, 1.2538259] 2018-05-18 03:38:36.885746\n",
      "67000 2.1981344 [2.296242, 1.1486584, 1.1475836] 2018-05-18 03:39:21.538287\n",
      "67100 2.1714675 [2.3167152, 1.1504552, 1.16626] 2018-05-18 03:39:51.597833\n",
      "67200 2.140308 [2.544789, 1.2715526, 1.2732365] 2018-05-18 03:40:24.624069\n",
      "67300 2.2392838 [2.4582586, 1.2288017, 1.2294569] 2018-05-18 03:41:12.069848\n",
      "67400 2.193282 [2.8513415, 1.4238617, 1.4274797] 2018-05-18 03:41:44.953555\n",
      "67500 2.1767886 [2.3497972, 1.1730661, 1.1767312] 2018-05-18 03:42:16.775456\n",
      "67600 2.1517277 [2.0116682, 1.0109694, 1.0006988] 2018-05-18 03:42:42.256304\n",
      "67700 2.242288 [2.53396, 1.2666745, 1.2672856] 2018-05-18 03:43:23.718558\n",
      "67800 2.2966413 [2.7872272, 1.4024111, 1.384816] 2018-05-18 03:43:56.346354\n",
      "67900 2.1883771 [2.1049948, 1.0520067, 1.052988] 2018-05-18 03:44:31.478060\n",
      "68000 2.2145414 [2.4111137, 1.2021606, 1.2089531] 2018-05-18 03:45:06.125944\n",
      "68100 2.2221916 [2.169399, 1.0784183, 1.0909808] 2018-05-18 03:45:37.507626\n",
      "68200 2.1944594 [2.0967193, 1.0449233, 1.051796] 2018-05-18 03:46:13.785474\n",
      "68300 2.1982648 [2.055713, 1.0269939, 1.0287191] 2018-05-18 03:46:40.250830\n",
      "68400 2.2387586 [2.0349534, 1.0423181, 0.9926352] 2018-05-18 03:47:10.743536\n",
      "68500 2.2316854 [2.445484, 1.2216165, 1.2238674] 2018-05-18 03:47:46.828390\n",
      "68600 2.1937108 [2.3374245, 1.1711755, 1.166249] 2018-05-18 03:48:26.574701\n",
      "68700 2.2050848 [2.5150447, 1.2928597, 1.2221851] 2018-05-18 03:49:00.628790\n",
      "68800 2.1581416 [2.50276, 1.2505358, 1.2522242] 2018-05-18 03:49:33.586320\n",
      "68900 2.1749666 [2.2659154, 1.1186365, 1.1472788] 2018-05-18 03:50:05.499616\n",
      "69000 2.2113783 [2.3318553, 1.1594687, 1.1723868] 2018-05-18 03:50:39.514399\n",
      "69100 2.1503627 [1.0292771, 0.65177685, 0.37750018] 2018-05-18 03:51:13.322325\n",
      "69200 2.2442925 [2.8190546, 1.4013618, 1.4176927] 2018-05-18 03:51:55.359161\n",
      "69300 2.195643 [2.1085873, 1.0548353, 1.053752] 2018-05-18 03:52:31.590101\n",
      "69400 2.1431503 [2.5060139, 1.2527409, 1.253273] 2018-05-18 03:52:59.728130\n",
      "69500 2.1863325 [2.5080853, 1.242461, 1.2656243] 2018-05-18 03:53:28.697313\n",
      "69600 2.2002184 [1.9956452, 1.0053155, 0.9903296] 2018-05-18 03:54:15.442566\n",
      "69700 2.111644 [2.6197162, 1.3186188, 1.3010973] 2018-05-18 03:54:47.620917\n",
      "69800 2.2352674 [2.8998632, 1.4714246, 1.4284388] 2018-05-18 03:55:19.339343\n",
      "69900 2.2198792 [2.215646, 1.1115975, 1.1040485] 2018-05-18 03:55:50.762272\n",
      "70000 2.1836615 [2.4117057, 1.2154775, 1.1962283] 2018-05-18 03:56:20.533426\n",
      "70100 2.2187643 [2.1639779, 1.0867474, 1.0772305] 2018-05-18 03:56:50.123799\n",
      "70200 2.1776702 [2.5330737, 1.2692652, 1.2638085] 2018-05-18 03:57:22.095443\n",
      "70300 2.1792471 [2.0765088, 1.0190232, 1.0574856] 2018-05-18 03:57:55.749405\n",
      "70400 2.148927 [2.4887133, 1.2480819, 1.2406312] 2018-05-18 03:58:42.458961\n",
      "70500 2.2131152 [2.3576074, 1.1724713, 1.1851362] 2018-05-18 03:59:33.563842\n",
      "70600 2.2803426 [2.5453339, 1.2858833, 1.2594504] 2018-05-18 04:00:13.559100\n",
      "70700 2.166026 [2.3074636, 1.1588671, 1.1485965] 2018-05-18 04:00:40.129901\n",
      "70800 2.1570997 [2.212849, 1.1031122, 1.1097367] 2018-05-18 04:01:15.530527\n",
      "70900 2.130118 [2.5068927, 1.2493306, 1.2575619] 2018-05-18 04:01:42.262316\n",
      "71000 2.1581504 [2.535627, 1.2704862, 1.2651405] 2018-05-18 04:02:06.988798\n",
      "71100 2.1762078 [1.5718805, 0.9131443, 0.65873617] 2018-05-18 04:02:49.417372\n",
      "71200 2.169455 [2.2782326, 1.1430569, 1.1351757] 2018-05-18 04:03:18.284033\n",
      "71300 2.1905775 [1.9583519, 0.9999198, 0.9584321] 2018-05-18 04:03:45.795131\n",
      "71400 2.189573 [2.1196356, 1.0612235, 1.0584122] 2018-05-18 04:04:30.406154\n",
      "71500 2.2390738 [2.311699, 1.1342924, 1.1774065] 2018-05-18 04:05:08.559322\n",
      "71600 2.3081841 [2.378784, 1.18642, 1.192364] 2018-05-18 04:05:36.594981\n",
      "71700 2.16102 [1.936183, 0.97739315, 0.9587898] 2018-05-18 04:06:07.092162\n",
      "Skipped batch, length 28912\n",
      "71800 2.1880405 [2.624417, 1.3264487, 1.2979684] 2018-05-18 04:06:38.424192\n",
      "71900 2.2589602 [2.3741155, 1.178033, 1.1960824] 2018-05-18 04:07:14.858219\n",
      "72000 2.2548995 [2.2316022, 1.1352627, 1.0963396] 2018-05-18 04:08:04.067913\n",
      "72100 2.136962 [2.7069645, 1.3459754, 1.3609891] 2018-05-18 04:08:36.178668\n",
      "72200 2.1570425 [2.107063, 1.0511783, 1.0558847] 2018-05-18 04:09:09.223767\n",
      "72300 2.2204773 [2.3457725, 1.1702926, 1.1754799] 2018-05-18 04:09:57.340535\n",
      "72400 2.1618187 [2.1278143, 1.0616207, 1.0661937] 2018-05-18 04:10:30.371718\n",
      "72500 2.1848354 [2.3919857, 1.1993327, 1.192653] 2018-05-18 04:11:01.479562\n",
      "72600 2.1654122 [2.4802713, 1.2366681, 1.2436032] 2018-05-18 04:11:28.589752\n",
      "Skipped batch, length 42251\n",
      "Skipped batch, length 40597\n",
      "72800 2.1867464 [1.9887292, 0.99728096, 0.9914483] 2018-05-18 04:12:53.959617\n",
      "72900 2.2320983 [2.1362383, 1.0777712, 1.0584671] 2018-05-18 04:13:25.925063\n",
      "73000 2.149051 [2.231029, 1.120109, 1.11092] 2018-05-18 04:13:57.611951\n",
      "73100 2.1248603 [1.7408116, 0.8727628, 0.8680488] 2018-05-18 04:14:25.648042\n",
      "73200 2.165327 [2.1244557, 1.0618575, 1.0625982] 2018-05-18 04:14:58.914252\n",
      "73300 2.1578844 [2.2297995, 1.1018207, 1.1279788] 2018-05-18 04:15:33.454503\n",
      "73400 2.2535098 [2.5877836, 1.2793181, 1.3084655] 2018-05-18 04:16:13.734963\n",
      "73500 2.1714056 [2.3688574, 1.186216, 1.1826413] 2018-05-18 04:16:50.944503\n",
      "73600 2.1829374 [2.309258, 1.1541436, 1.1551144] 2018-05-18 04:17:20.903874\n",
      "73700 2.2252407 [2.6495228, 1.331027, 1.3184958] 2018-05-18 04:17:56.074653\n",
      "73800 2.2737527 [2.2480993, 1.117667, 1.1304324] 2018-05-18 04:18:42.120595\n",
      "73900 2.2064044 [1.9424901, 0.9696026, 0.97288746] 2018-05-18 04:19:13.153706\n",
      "74000 2.1298885 [2.722374, 1.3595326, 1.3628414] 2018-05-18 04:19:52.041007\n",
      "74100 2.1350796 [0.9129138, 0.5591027, 0.3538111] 2018-05-18 04:20:28.246285\n",
      "74200 2.188286 [2.5121546, 1.2548088, 1.2573458] 2018-05-18 04:21:14.678026\n",
      "74300 2.1839125 [2.6601152, 1.3305087, 1.3296064] 2018-05-18 04:21:48.480928\n",
      "74400 2.1575508 [2.4123273, 1.204027, 1.2083004] 2018-05-18 04:22:28.079448\n",
      "74500 2.1759882 [2.037272, 1.0131758, 1.0240961] 2018-05-18 04:23:07.754991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74600 2.272837 [2.3531814, 1.1786706, 1.1745107] 2018-05-18 04:23:39.505503\n",
      "74700 2.1824858 [2.0220232, 1.0043051, 1.0177182] 2018-05-18 04:24:13.811287\n",
      "74800 2.1533947 [2.3260322, 1.1584996, 1.1675324] 2018-05-18 04:24:51.698008\n",
      "74900 2.1069176 [3.0059843, 1.5267189, 1.4792656] 2018-05-18 04:25:16.238473\n",
      "75000 2.147768 [2.2194815, 1.1202854, 1.0991961] 2018-05-18 04:25:48.687008\n",
      "75100 2.1931639 [1.9256608, 0.966744, 0.9589168] 2018-05-18 04:26:21.395188\n",
      "75200 2.1700225 [1.8999419, 0.9295699, 0.970372] 2018-05-18 04:26:53.749818\n",
      "Skipped batch, length 54157\n",
      "75300 2.1567802 [2.03529, 1.0101994, 1.0250906] 2018-05-18 04:27:20.287156\n",
      "75400 2.1804748 [1.6727872, 0.94150925, 0.7312779] 2018-05-18 04:28:02.723754\n",
      "75500 2.0958831 [2.3936014, 1.2019119, 1.1916895] 2018-05-18 04:28:29.891526\n",
      "75600 2.2623558 [2.4513497, 1.2275088, 1.223841] 2018-05-18 04:28:58.951426\n",
      "75700 2.1906066 [2.0274205, 1.0400684, 0.987352] 2018-05-18 04:29:28.929818\n",
      "75800 2.1820915 [1.0542661, 0.65906435, 0.39520177] 2018-05-18 04:30:00.390703\n",
      "75900 2.186758 [2.3017116, 1.1437973, 1.1579142] 2018-05-18 04:30:30.085991\n",
      "76000 2.2368753 [2.0847864, 1.0413015, 1.0434848] 2018-05-18 04:31:00.337959\n",
      "76100 2.2313998 [2.055302, 1.0361763, 1.0191256] 2018-05-18 04:31:50.200389\n",
      "76200 2.1547954 [2.4958546, 1.2453074, 1.2505472] 2018-05-18 04:32:22.621995\n",
      "76300 2.1645238 [2.6873453, 1.352783, 1.3345623] 2018-05-18 04:33:01.387805\n",
      "76400 2.14683 [2.0676355, 1.0319031, 1.0357323] 2018-05-18 04:33:30.593606\n",
      "76500 2.1533074 [2.3293097, 1.162297, 1.1670127] 2018-05-18 04:33:57.565747\n",
      "76600 2.107498 [2.3381772, 1.1943985, 1.1437788] 2018-05-18 04:34:26.246071\n",
      "76700 2.201118 [2.1862178, 1.0933065, 1.0929112] 2018-05-18 04:35:02.843994\n",
      "Skipped batch, length 22432\n",
      "76800 2.1752558 [2.2477176, 1.1282098, 1.1195078] 2018-05-18 04:35:34.061665\n",
      "76900 2.161759 [2.5182462, 1.2563193, 1.2619269] 2018-05-18 04:36:06.149506\n",
      "77000 2.245489 [1.7705214, 0.881706, 0.8888154] 2018-05-18 04:36:36.316912\n",
      "77100 2.1385856 [1.8620541, 0.9179085, 0.94414556] 2018-05-18 04:37:01.122192\n",
      "77200 2.173447 [2.3328643, 1.1572211, 1.1756432] 2018-05-18 04:37:34.613677\n",
      "77300 2.1207664 [2.4790528, 1.2344315, 1.2446213] 2018-05-18 04:38:07.808335\n",
      "77400 2.21976 [2.3948722, 1.2028488, 1.1920235] 2018-05-18 04:38:44.659817\n",
      "77500 2.202796 [1.7961835, 0.8874841, 0.9086994] 2018-05-18 04:39:14.728286\n",
      "77600 2.175413 [2.3994584, 1.1953312, 1.2041272] 2018-05-18 04:40:00.695113\n",
      "Skipped batch, length 44702\n",
      "77700 2.1577504 [2.085752, 1.0435214, 1.0422305] 2018-05-18 04:40:51.468507\n",
      "77800 2.1845536 [2.4012733, 1.2005651, 1.2007082] 2018-05-18 04:41:25.467861\n",
      "77900 2.1683662 [2.3021936, 1.1516365, 1.150557] 2018-05-18 04:41:59.507236\n",
      "78000 2.125855 [1.9552847, 0.9900831, 0.9652016] 2018-05-18 04:42:36.433826\n",
      "78100 2.1868029 [2.4635448, 1.2365668, 1.2269782] 2018-05-18 04:43:13.495456\n",
      "78200 2.176845 [1.9100646, 1.0105052, 0.8995594] 2018-05-18 04:43:46.397031\n",
      "78300 2.2230566 [2.097446, 1.0399376, 1.0575082] 2018-05-18 04:44:24.599500\n",
      "78400 2.192687 [2.2114453, 1.0961303, 1.115315] 2018-05-18 04:44:57.754035\n",
      "78500 2.1345353 [2.487574, 1.2465177, 1.2410564] 2018-05-18 04:45:29.092519\n",
      "78600 2.1787674 [2.4780018, 1.2377553, 1.2402465] 2018-05-18 04:46:02.098779\n",
      "78700 2.2139368 [2.3401175, 1.1686062, 1.1715113] 2018-05-18 04:46:34.124666\n",
      "78800 2.1715665 [1.9040405, 0.9535055, 0.95053494] 2018-05-18 04:47:05.435670\n",
      "78900 2.1703463 [1.8174291, 0.90159625, 0.9158328] 2018-05-18 04:47:46.388044\n",
      "79000 2.1632435 [2.3414679, 1.1757305, 1.1657374] 2018-05-18 04:48:19.804792\n",
      "Skipped batch, length 1233816\n",
      "79100 2.2016098 [1.9509224, 0.96136105, 0.9895613] 2018-05-18 04:48:50.468325\n",
      "79200 2.185048 [2.5223086, 1.2561715, 1.2661371] 2018-05-18 04:49:23.204652\n",
      "79300 2.197128 [2.642883, 1.3361022, 1.3067808] 2018-05-18 04:49:55.737262\n",
      "79400 2.2643297 [2.5543265, 1.2627863, 1.2915401] 2018-05-18 04:50:29.289245\n",
      "79500 2.2258728 [2.2620742, 1.1370218, 1.1250525] 2018-05-18 04:51:16.121534\n",
      "79600 2.1706727 [2.3573244, 1.1769571, 1.1803672] 2018-05-18 04:51:49.811070\n",
      "79700 2.1351635 [2.6092374, 1.3113978, 1.2978396] 2018-05-18 04:52:28.198362\n",
      "79800 2.1646864 [2.11872, 1.0496751, 1.0690448] 2018-05-18 04:53:14.957613\n",
      "79900 2.2071679 [2.4024534, 1.2011654, 1.201288] 2018-05-18 04:53:46.010292\n",
      "80000 2.1860633 [2.4342208, 1.2137392, 1.2204816] 2018-05-18 04:54:29.803832\n",
      "80100 2.1252608 [2.2761998, 1.141402, 1.1347978] 2018-05-18 04:55:00.871732\n",
      "80200 2.2470438 [1.9409723, 0.97026366, 0.9707087] 2018-05-18 04:55:32.775344\n",
      "80300 2.156741 [2.1611264, 1.0753171, 1.0858092] 2018-05-18 04:56:08.531954\n",
      "80400 2.2543652 [2.3400433, 1.1727517, 1.1672916] 2018-05-18 04:56:41.836810\n",
      "80500 2.1395216 [2.4024374, 1.2009673, 1.2014701] 2018-05-18 04:57:12.550577\n",
      "80600 2.2305026 [2.061376, 1.0236481, 1.0377278] 2018-05-18 04:57:55.541094\n",
      "80700 2.201326 [2.563747, 1.3042561, 1.259491] 2018-05-18 04:58:39.424340\n",
      "80800 2.1600754 [2.2824192, 1.144604, 1.1378152] 2018-05-18 04:59:14.789783\n",
      "80900 2.22143 [1.9958746, 0.9875651, 1.0083096] 2018-05-18 04:59:44.529171\n",
      "81000 2.2915065 [2.324543, 1.1549294, 1.1696136] 2018-05-18 05:00:20.616506\n",
      "81100 2.2234972 [2.4934216, 1.2487955, 1.2446262] 2018-05-18 05:01:00.032098\n",
      "81200 2.1299453 [1.6420994, 0.8246357, 0.81746364] 2018-05-18 05:01:34.402039\n",
      "81300 2.1599135 [2.251052, 1.1177001, 1.1333519] 2018-05-18 05:02:10.474058\n",
      "81400 2.2026348 [2.2790966, 1.1403484, 1.138748] 2018-05-18 05:02:53.373782\n",
      "81500 2.1928647 [2.0102825, 1.0012243, 1.0090582] 2018-05-18 05:03:30.013264\n",
      "81600 2.226258 [2.668885, 1.3426695, 1.3262155] 2018-05-18 05:04:06.687173\n",
      "81700 2.2333288 [2.411384, 1.2046207, 1.2067634] 2018-05-18 05:04:38.626629\n",
      "81800 2.1725817 [2.2374072, 1.1223731, 1.1150342] 2018-05-18 05:05:10.081272\n",
      "81900 2.1725183 [1.9210752, 0.95565045, 0.9654248] 2018-05-18 05:05:38.738311\n",
      "82000 2.1655736 [2.3227606, 1.1459515, 1.1768091] 2018-05-18 05:06:12.110078\n",
      "Skipped batch, length 25661\n",
      "82100 2.2103941 [2.1973422, 1.0869255, 1.1104167] 2018-05-18 05:06:46.720420\n",
      "82200 2.153148 [1.1437044, 0.70290333, 0.44080108] 2018-05-18 05:07:22.263163\n",
      "82300 2.156858 [2.201141, 1.0953709, 1.1057702] 2018-05-18 05:07:59.987901\n",
      "82400 2.1743128 [1.6086216, 0.7977799, 0.8108417] 2018-05-18 05:08:29.887914\n",
      "82500 2.2419827 [0.92555445, 0.56813824, 0.3574162] 2018-05-18 05:09:08.837772\n",
      "82600 2.0790648 [2.7921622, 1.3904271, 1.4017351] 2018-05-18 05:09:37.305977\n",
      "82700 2.1260214 [2.3483977, 1.1745995, 1.1737983] 2018-05-18 05:10:08.155053\n",
      "82800 2.2423615 [2.3469474, 1.1730802, 1.1738672] 2018-05-18 05:10:41.396573\n",
      "Skipped batch, length 18636\n",
      "82900 2.1719651 [2.3556323, 1.1758271, 1.1798052] 2018-05-18 05:11:17.776701\n",
      "83000 2.1779613 [2.3102736, 1.1662254, 1.1440482] 2018-05-18 05:11:46.385438\n",
      "Skipped batch, length 18382\n",
      "83100 2.1801896 [1.3210635, 0.7769066, 0.54415697] 2018-05-18 05:12:19.960208\n",
      "83200 2.214884 [2.3257194, 1.1624666, 1.1632528] 2018-05-18 05:12:48.009406\n",
      "83300 2.1774697 [2.0732155, 1.0413675, 1.031848] 2018-05-18 05:13:17.091785\n",
      "83400 2.1789122 [2.0537093, 1.0214479, 1.0322614] 2018-05-18 05:14:06.033163\n",
      "83500 2.204335 [2.2256873, 1.0944881, 1.1311991] 2018-05-18 05:14:34.640753\n",
      "83600 2.1872716 [2.1895533, 1.0935032, 1.09605] 2018-05-18 05:15:13.927073\n",
      "83700 2.21257 [1.8325568, 0.92929804, 0.9032588] 2018-05-18 05:15:44.154955\n",
      "83800 2.1808753 [2.637393, 1.3244039, 1.312989] 2018-05-18 05:16:12.259679\n",
      "83900 2.1917918 [2.4212923, 1.2112012, 1.2100911] 2018-05-18 05:16:43.321402\n",
      "84000 2.1618388 [2.1377943, 1.0561566, 1.0816376] 2018-05-18 05:17:24.795402\n",
      "84100 2.1872237 [2.2834008, 1.1358094, 1.1475914] 2018-05-18 05:18:07.000870\n",
      "84200 2.1274333 [1.7842958, 0.880092, 0.9042038] 2018-05-18 05:18:38.187318\n",
      "84300 2.1994038 [2.5511765, 1.2688065, 1.2823701] 2018-05-18 05:19:13.839637\n",
      "84400 2.1852071 [1.9956304, 1.0046818, 0.99094856] 2018-05-18 05:19:45.699961\n",
      "84500 2.2106762 [2.4417496, 1.2245626, 1.2171869] 2018-05-18 05:20:21.622366\n",
      "84600 2.2328186 [2.4632134, 1.224783, 1.2384305] 2018-05-18 05:20:58.176812\n",
      "84700 2.1610076 [2.0912023, 1.041323, 1.0498794] 2018-05-18 05:21:27.487700\n",
      "84800 2.2093499 [2.4544775, 1.2388597, 1.2156179] 2018-05-18 05:21:59.622300\n",
      "84900 2.1980596 [2.307675, 1.149544, 1.1581309] 2018-05-18 05:22:36.039933\n",
      "85000 2.1110537 [1.2616019, 0.7607285, 0.5008734] 2018-05-18 05:23:05.220754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85100 2.2524266 [2.1808662, 1.0849152, 1.0959511] 2018-05-18 05:23:38.879978\n",
      "85200 2.1573713 [2.3167787, 1.1581627, 1.1586158] 2018-05-18 05:24:22.213586\n",
      "85300 2.2009575 [2.732735, 1.3635914, 1.3691435] 2018-05-18 05:24:53.091201\n",
      "85400 2.145083 [2.3653185, 1.1817782, 1.1835403] 2018-05-18 05:25:28.976494\n",
      "85500 2.1613245 [2.431109, 1.211057, 1.220052] 2018-05-18 05:26:02.872683\n",
      "85600 2.1687276 [2.03106, 1.0098065, 1.0212535] 2018-05-18 05:26:29.129584\n",
      "85700 2.216705 [1.0079105, 0.6127199, 0.39519063] 2018-05-18 05:27:01.233556\n",
      "85800 2.1814492 [2.5167408, 1.2605305, 1.2562103] 2018-05-18 05:27:35.028260\n",
      "85900 2.21029 [1.7125101, 0.8435596, 0.8689505] 2018-05-18 05:28:05.262624\n",
      "86000 2.2344093 [2.1419768, 1.075794, 1.0661829] 2018-05-18 05:28:42.995952\n",
      "86100 2.1284285 [1.8183572, 0.9338513, 0.8845059] 2018-05-18 05:29:07.411801\n",
      "86200 2.1917534 [2.2236433, 1.1130351, 1.1106083] 2018-05-18 05:29:42.978977\n",
      "86300 2.1840792 [2.4103951, 1.2078233, 1.2025717] 2018-05-18 05:30:14.887944\n",
      "86400 2.1223457 [2.8879023, 1.43539, 1.4525123] 2018-05-18 05:30:49.799246\n",
      "86500 2.1842527 [1.510897, 0.84679365, 0.6641034] 2018-05-18 05:31:29.356673\n",
      "86600 2.1605723 [2.269227, 1.1341805, 1.1350465] 2018-05-18 05:31:59.998476\n",
      "86700 2.121154 [2.1277282, 1.0515282, 1.0762] 2018-05-18 05:32:21.798024\n",
      "86800 2.1376574 [2.425645, 1.2094381, 1.216207] 2018-05-18 05:32:45.062474\n",
      "86900 2.2141972 [2.3759263, 1.1912091, 1.1847172] 2018-05-18 05:33:15.394115\n",
      "87000 2.1692257 [2.0416667, 1.0103819, 1.0312848] 2018-05-18 05:33:49.463735\n",
      "87100 2.1918926 [2.120447, 1.05917, 1.0612769] 2018-05-18 05:34:17.515002\n",
      "87200 2.1731179 [2.4817634, 1.2257819, 1.2559813] 2018-05-18 05:34:46.741399\n",
      "87300 2.198999 [2.2448943, 1.1142616, 1.1306326] 2018-05-18 05:35:24.238489\n",
      "87400 2.1720884 [1.4647621, 0.8330482, 0.63171387] 2018-05-18 05:35:55.070501\n",
      "87500 2.1639977 [1.8059785, 0.89203876, 0.9139398] 2018-05-18 05:36:28.769494\n",
      "87600 2.2325678 [2.7961879, 1.4008241, 1.3953637] 2018-05-18 05:37:02.972425\n",
      "87700 2.2610993 [1.9059031, 0.9481466, 0.9577566] 2018-05-18 05:37:51.205830\n",
      "87800 2.1261435 [2.0419092, 1.0161599, 1.0257494] 2018-05-18 05:38:25.711990\n",
      "87900 2.1628284 [2.4036412, 1.2091464, 1.194495] 2018-05-18 05:38:58.041041\n",
      "88000 2.1942148 [2.7502873, 1.3688974, 1.3813899] 2018-05-18 05:39:44.012891\n",
      "88100 2.167671 [2.6562586, 1.3635699, 1.2926886] 2018-05-18 05:40:23.210626\n",
      "88200 2.16212 [1.6153574, 0.810019, 0.8053384] 2018-05-18 05:40:52.411242\n",
      "88300 2.2266784 [1.4511743, 0.8367618, 0.61441255] 2018-05-18 05:41:24.385500\n",
      "88400 2.1486478 [2.3740902, 1.1678855, 1.2062047] 2018-05-18 05:41:56.054668\n",
      "88500 2.1982534 [2.0181704, 1.0049958, 1.0131745] 2018-05-18 05:42:26.504923\n",
      "88600 2.2035682 [1.4770963, 0.8382585, 0.63883775] 2018-05-18 05:43:06.135891\n",
      "88700 2.1965568 [1.8771255, 0.93915504, 0.93797046] 2018-05-18 05:43:41.959222\n",
      "88800 2.1715014 [1.6089585, 0.7986442, 0.81031424] 2018-05-18 05:44:12.589093\n",
      "88900 2.1468554 [2.2202644, 1.111871, 1.1083933] 2018-05-18 05:44:48.597903\n",
      "89000 2.1209617 [2.0554955, 1.0343775, 1.021118] 2018-05-18 05:45:20.205834\n",
      "89100 2.271873 [2.6016655, 1.3041153, 1.2975502] 2018-05-18 05:45:49.853630\n",
      "89200 2.1584344 [1.1787779, 0.70141935, 0.47735858] 2018-05-18 05:46:26.798858\n",
      "89300 2.1352139 [2.4692278, 1.2479591, 1.2212688] 2018-05-18 05:46:56.610521\n",
      "89400 2.1229365 [2.1951804, 1.1009902, 1.0941904] 2018-05-18 05:47:24.783463\n",
      "89500 2.1553001 [1.8906255, 0.9737211, 0.91690433] 2018-05-18 05:47:50.466291\n",
      "89600 2.1604187 [1.8001798, 1.0354781, 0.7647017] 2018-05-18 05:48:25.827417\n",
      "89700 2.1451936 [2.832048, 1.4211477, 1.4109002] 2018-05-18 05:49:04.206177\n",
      "89800 2.1829722 [2.5392888, 1.268419, 1.2708697] 2018-05-18 05:49:37.577440\n",
      "89900 2.191175 [2.2049074, 1.1052588, 1.0996485] 2018-05-18 05:50:09.232021\n",
      "90000 2.153924 [1.4888799, 0.7726432, 0.7162367] 2018-05-18 05:50:43.610898\n",
      "90100 2.2146761 [1.8772435, 0.9321694, 0.94507414] 2018-05-18 05:51:19.153891\n",
      "90200 2.1569488 [1.9434974, 0.9640067, 0.9794907] 2018-05-18 05:51:47.386764\n",
      "90300 2.208063 [2.5008314, 1.3375237, 1.1633077] 2018-05-18 05:52:17.504793\n",
      "90400 2.128349 [2.0130417, 1.0169871, 0.9960547] 2018-05-18 05:52:45.269884\n",
      "90500 2.1870124 [2.254283, 1.1207889, 1.1334939] 2018-05-18 05:53:17.395113\n",
      "90600 2.210232 [2.0051873, 1.003763, 1.0014243] 2018-05-18 05:53:49.413415\n",
      "90700 2.1712534 [1.6975092, 0.96091974, 0.73658943] 2018-05-18 05:54:23.954173\n",
      "90800 2.189982 [2.2021902, 1.1111519, 1.0910382] 2018-05-18 05:54:54.150145\n",
      "90900 2.1391914 [2.3924422, 1.2018851, 1.190557] 2018-05-18 05:55:26.364154\n",
      "91000 2.1610808 [2.8272524, 1.4146582, 1.4125941] 2018-05-18 05:55:57.312458\n",
      "91100 2.111767 [2.0009284, 1.0065341, 0.99439424] 2018-05-18 05:56:23.186206\n",
      "91200 2.0821095 [2.4542184, 1.2226527, 1.2315658] 2018-05-18 05:56:48.326331\n",
      "91300 2.0938635 [2.4938383, 1.2505089, 1.2433293] 2018-05-18 05:57:18.613555\n",
      "Skipped batch, length 18966\n",
      "91400 2.162965 [2.579403, 1.2945739, 1.284829] 2018-05-18 05:57:42.519022\n",
      "91500 2.232492 [2.461874, 1.2303207, 1.2315532] 2018-05-18 05:58:22.734407\n",
      "91600 2.1907258 [2.5740528, 1.2869761, 1.2870767] 2018-05-18 05:59:01.650046\n",
      "91700 2.136844 [2.0091877, 0.99596834, 1.0132194] 2018-05-18 05:59:39.204473\n",
      "91800 2.143994 [1.964328, 0.98882633, 0.97550166] 2018-05-18 06:00:09.280864\n",
      "91900 2.201721 [1.8995504, 0.9495816, 0.9499688] 2018-05-18 06:00:40.429625\n",
      "92000 2.2001858 [2.1266785, 1.0621856, 1.0644927] 2018-05-18 06:01:25.679869\n",
      "92100 2.192255 [2.5659387, 1.2845733, 1.2813654] 2018-05-18 06:01:55.950249\n",
      "92200 2.124834 [2.2196884, 1.1099669, 1.1097217] 2018-05-18 06:02:23.029911\n",
      "92300 2.1725037 [2.1147366, 1.0523458, 1.0623907] 2018-05-18 06:02:59.981487\n",
      "92400 2.1489947 [2.271294, 1.1177278, 1.1535664] 2018-05-18 06:03:29.141688\n",
      "92500 2.1964777 [2.1570375, 1.0772934, 1.0797441] 2018-05-18 06:04:05.090988\n",
      "92600 2.1531267 [2.762587, 1.3864372, 1.3761499] 2018-05-18 06:04:45.619928\n",
      "92700 2.1775112 [2.351059, 1.1762086, 1.1748502] 2018-05-18 06:05:15.900990\n",
      "92800 2.2257712 [3.0334423, 1.5255566, 1.5078857] 2018-05-18 06:05:49.413531\n",
      "92900 2.1358035 [2.2510962, 1.1323907, 1.1187055] 2018-05-18 06:06:25.735995\n",
      "93000 2.1669226 [2.4029875, 1.2063948, 1.1965926] 2018-05-18 06:06:56.038465\n",
      "93100 2.186871 [2.3951988, 1.1917775, 1.2034215] 2018-05-18 06:07:29.409204\n",
      "93200 2.1548328 [1.9114362, 0.95214415, 0.95929205] 2018-05-18 06:08:03.027938\n",
      "93300 2.183868 [2.2966757, 1.1470573, 1.1496183] 2018-05-18 06:08:37.964917\n",
      "93400 2.1719177 [1.9691265, 0.9815577, 0.9875687] 2018-05-18 06:09:04.798317\n",
      "93500 2.227542 [2.2665548, 1.1257704, 1.1407844] 2018-05-18 06:09:43.883271\n",
      "93600 2.1725852 [2.2665894, 1.1406, 1.1259894] 2018-05-18 06:10:11.793715\n",
      "93700 2.166274 [2.8979177, 1.4545611, 1.4433565] 2018-05-18 06:10:57.837806\n",
      "93800 2.1682665 [2.3170278, 1.1559129, 1.1611149] 2018-05-18 06:11:49.627028\n",
      "93900 2.103612 [2.0802302, 1.0574744, 1.022756] 2018-05-18 06:12:21.392380\n",
      "94000 2.1534433 [2.4887295, 1.2428253, 1.2459041] 2018-05-18 06:12:51.442421\n",
      "94100 2.2407925 [2.1962867, 1.0948781, 1.1014087] 2018-05-18 06:13:28.308390\n",
      "94200 2.1295805 [2.2128465, 1.1086932, 1.1041533] 2018-05-18 06:14:01.900400\n",
      "94300 2.1863554 [2.397667, 1.1978718, 1.1997951] 2018-05-18 06:14:37.917466\n",
      "94400 2.2440968 [2.3391097, 1.1680901, 1.1710196] 2018-05-18 06:15:05.377480\n",
      "94500 2.0793273 [2.1380734, 1.0702863, 1.067787] 2018-05-18 06:15:28.472496\n",
      "94600 2.1669328 [2.171864, 1.0794241, 1.0924399] 2018-05-18 06:15:58.564629\n",
      "94700 2.1757562 [2.1182346, 1.0532699, 1.0649647] 2018-05-18 06:16:29.063696\n",
      "94800 2.274828 [1.7519625, 0.8970239, 0.8549386] 2018-05-18 06:17:22.026125\n",
      "94900 2.15786 [2.3296356, 1.1658192, 1.1638165] 2018-05-18 06:17:52.988439\n",
      "95000 2.1169338 [2.1915827, 1.0975718, 1.0940108] 2018-05-18 06:18:21.814060\n",
      "Skipped batch, length 17539\n",
      "Skipped batch, length 67342\n",
      "95100 2.1317034 [2.4745498, 1.2377297, 1.2368201] 2018-05-18 06:18:54.085189\n",
      "95200 2.1928735 [1.8719026, 0.9352233, 0.9366793] 2018-05-18 06:19:19.564440\n",
      "95300 2.1905198 [2.295344, 1.1454111, 1.149933] 2018-05-18 06:19:49.051506\n",
      "95400 2.269657 [1.8697078, 0.9289937, 0.9407141] 2018-05-18 06:20:23.986552\n",
      "95500 2.20114 [2.2840638, 1.1042534, 1.1798105] 2018-05-18 06:20:58.408905\n",
      "95600 2.136761 [1.9419317, 0.9727975, 0.9691343] 2018-05-18 06:21:35.251394\n",
      "95700 2.0875611 [2.0714376, 1.0361958, 1.0352418] 2018-05-18 06:22:12.526363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95800 2.14331 [1.7568198, 0.86826885, 0.888551] 2018-05-18 06:22:43.169314\n",
      "95900 2.2027798 [1.281463, 0.7610694, 0.5203936] 2018-05-18 06:23:13.980388\n",
      "96000 2.1836963 [2.4061792, 1.2011924, 1.2049868] 2018-05-18 06:23:45.641173\n",
      "96100 2.1648939 [2.280641, 1.1429195, 1.1377214] 2018-05-18 06:24:23.884033\n",
      "96200 2.133882 [2.243548, 1.1221143, 1.1214336] 2018-05-18 06:25:01.484506\n",
      "Skipped batch, length 27421\n",
      "96300 2.1565073 [2.4765973, 1.2404581, 1.2361392] 2018-05-18 06:25:34.514582\n",
      "96400 2.1899583 [2.3088808, 1.1591535, 1.1497272] 2018-05-18 06:26:12.604614\n",
      "96500 2.1857383 [2.566975, 1.2892494, 1.2777258] 2018-05-18 06:26:45.815115\n",
      "96600 2.1765847 [2.39128, 1.1926349, 1.198645] 2018-05-18 06:27:21.541111\n",
      "96700 2.1449986 [2.4684606, 1.2367092, 1.2317512] 2018-05-18 06:27:52.610308\n",
      "96800 2.1739721 [2.0182672, 1.0063798, 1.0118873] 2018-05-18 06:28:25.201172\n",
      "96900 2.1435938 [2.1198707, 1.0598229, 1.0600476] 2018-05-18 06:28:56.664737\n",
      "97000 2.1258276 [2.0474, 1.0242242, 1.023176] 2018-05-18 06:29:30.463818\n",
      "97100 2.201353 [2.5381393, 1.265548, 1.2725914] 2018-05-18 06:30:11.671222\n",
      "97200 2.2047455 [1.6316714, 0.9529721, 0.6786994] 2018-05-18 06:30:51.123749\n",
      "97300 2.1723 [1.3419235, 0.7950393, 0.5468842] 2018-05-18 06:31:32.851814\n",
      "97400 2.1449943 [2.50357, 1.2525313, 1.2510387] 2018-05-18 06:32:07.461436\n",
      "97500 2.1289153 [1.7847476, 0.89730376, 0.88744384] 2018-05-18 06:32:45.730686\n",
      "97600 2.1549566 [2.4466639, 1.2251799, 1.221484] 2018-05-18 06:33:16.823188\n",
      "Skipped batch, length 48737\n",
      "97700 2.1237612 [2.791895, 1.394773, 1.3971218] 2018-05-18 06:33:50.200866\n",
      "97800 2.2128224 [0.9725686, 0.61837053, 0.35419804] 2018-05-18 06:34:25.305271\n",
      "97900 2.1537025 [1.9195478, 0.9657954, 0.95375246] 2018-05-18 06:34:55.355744\n",
      "98000 2.128927 [1.8069224, 0.88760495, 0.91931754] 2018-05-18 06:35:33.339120\n",
      "98100 2.1704128 [2.5676322, 1.2774324, 1.2901996] 2018-05-18 06:36:14.391076\n",
      "98200 2.1846826 [2.3201935, 1.1556478, 1.1645458] 2018-05-18 06:36:49.617416\n",
      "98300 2.1395268 [2.3444479, 1.1692238, 1.1752241] 2018-05-18 06:37:20.032062\n",
      "98400 2.2090557 [1.8165634, 0.9167531, 0.8998102] 2018-05-18 06:37:52.087671\n",
      "98500 2.16675 [1.8057728, 0.9111181, 0.89465475] 2018-05-18 06:38:26.284251\n",
      "98600 2.2043047 [1.4623334, 0.8387773, 0.6235562] 2018-05-18 06:38:58.843237\n",
      "98700 2.1532197 [2.00385, 1.0009592, 1.0028908] 2018-05-18 06:39:28.468146\n",
      "98800 2.1488037 [1.8607491, 0.9219085, 0.9388406] 2018-05-18 06:39:53.625519\n",
      "98900 2.1736715 [2.307611, 1.1550546, 1.1525564] 2018-05-18 06:40:29.290807\n",
      "99000 2.1295846 [2.4217162, 1.2010679, 1.2206484] 2018-05-18 06:41:00.914462\n",
      "99100 2.101126 [2.0521219, 1.0244141, 1.0277078] 2018-05-18 06:41:28.435716\n",
      "99200 2.203288 [2.1429315, 1.0719069, 1.0710244] 2018-05-18 06:42:02.130690\n",
      "99300 2.1464133 [1.9844799, 0.9915891, 0.9928907] 2018-05-18 06:42:28.590889\n",
      "99400 2.155963 [1.7049458, 0.8532282, 0.85171753] 2018-05-18 06:43:01.454835\n",
      "99500 2.2099934 [2.4663606, 1.2395439, 1.2268165] 2018-05-18 06:43:41.004608\n",
      "99600 2.1881735 [2.327817, 1.1590179, 1.1687992] 2018-05-18 06:44:18.654517\n",
      "99700 2.1592321 [1.1892205, 0.6934468, 0.49577373] 2018-05-18 06:44:47.516930\n",
      "99800 2.1874814 [1.9361405, 0.96853334, 0.9676072] 2018-05-18 06:45:13.643695\n",
      "99900 2.228749 [1.8398509, 0.96005607, 0.8797948] 2018-05-18 06:45:49.428111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-18 06:46:18.457236\n",
      "Trained at 2018-05-18 06:51:59.446272\n",
      "TP 15328 FP 3516 FN 4001 F 0.8030807114976554 Precision 0.8134154107408194 Recall 0.7930053287805887\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-18 06:54:15.595612\n",
      "Trained at 2018-05-18 06:59:55.164101\n",
      "TP 15732 FP 2701 FN 3597 F 0.8332185795243896 Precision 0.8534693213258829 Recall 0.8139065652646282\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-18 07:02:10.699799\n",
      "Trained at 2018-05-18 07:07:48.771348\n",
      "TP 16472 FP 3065 FN 2857 F 0.8476303195595122 Precision 0.8431181860060398 Recall 0.8521910083294532\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-18 07:10:03.572463\n",
      "Trained at 2018-05-18 07:15:43.075261\n",
      "TP 16626 FP 2975 FN 2703 F 0.8541484716157205 Precision 0.8482220294882914 Recall 0.8601583113456465\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-18 07:17:57.430237\n",
      "Trained at 2018-05-18 07:23:36.643062\n",
      "TP 16735 FP 2987 FN 2594 F 0.8570843256254641 Precision 0.8485447723354629 Recall 0.8657975063376274\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-18 07:25:51.069654\n",
      "Trained at 2018-05-18 07:31:29.616583\n",
      "TP 16820 FP 2993 FN 2509 F 0.8594348781360176 Precision 0.848937566244385 Recall 0.8701950437166951\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-18 07:33:45.927834\n",
      "Trained at 2018-05-18 07:39:25.080245\n",
      "TP 16835 FP 2931 FN 2494 F 0.8612354521038496 Precision 0.8517150662754225 Recall 0.870971079724766\n",
      "Best so far\n",
      "Epoch 7 start at 2018-05-18 07:41:41.461278\n",
      "Trained at 2018-05-18 07:47:20.211754\n",
      "TP 16972 FP 3198 FN 2357 F 0.8593635281905871 Precision 0.8414476945959346 Recall 0.8780588752651456\n",
      "Epoch 8 start at 2018-05-18 07:49:36.168258\n",
      "Trained at 2018-05-18 07:55:14.701826\n",
      "TP 16962 FP 2982 FN 2367 F 0.8637995569475212 Precision 0.8504813477737665 Recall 0.8775415179264318\n",
      "Best so far\n",
      "Epoch 9 start at 2018-05-18 07:57:31.155707\n",
      "Trained at 2018-05-18 08:03:10.344618\n",
      "TP 17012 FP 3114 FN 2317 F 0.8623495121023952 Precision 0.8452747689555798 Recall 0.880128304620001\n",
      "Epoch 10 start at 2018-05-18 08:05:26.591910\n",
      "Trained at 2018-05-18 08:11:04.574587\n",
      "TP 17117 FP 3315 FN 2212 F 0.8609944417896934 Precision 0.8377545027407988 Recall 0.8855605566764965\n",
      "Epoch 11 start at 2018-05-18 08:13:21.506433\n",
      "Trained at 2018-05-18 08:19:01.460609\n",
      "TP 16865 FP 2816 FN 2464 F 0.8646500897205844 Precision 0.8569178395406738 Recall 0.8725231517409074\n",
      "Best so far\n",
      "Epoch 12 start at 2018-05-18 08:21:18.571713\n",
      "Trained at 2018-05-18 08:26:57.535014\n",
      "TP 16903 FP 3126 FN 2426 F 0.8589359215407287 Precision 0.8439263068550602 Recall 0.8744891096280201\n",
      "Epoch 13 start at 2018-05-18 08:29:14.706447\n",
      "Trained at 2018-05-18 08:34:54.196264\n",
      "TP 17045 FP 3064 FN 2284 F 0.8643947461838836 Precision 0.8476304142423791 Recall 0.8818355838377567\n",
      "Epoch 14 start at 2018-05-18 08:37:12.902356\n",
      "Trained at 2018-05-18 08:42:51.528158\n",
      "TP 16946 FP 3013 FN 2383 F 0.8626552636937487 Precision 0.8490405330928403 Recall 0.8767137461844896\n",
      "Epoch 15 start at 2018-05-18 08:45:08.970788\n",
      "Trained at 2018-05-18 08:50:48.401895\n",
      "TP 16976 FP 3100 FN 2353 F 0.8616165461235884 Precision 0.8455867702729627 Recall 0.8782658182006312\n",
      "Epoch 16 start at 2018-05-18 08:53:04.228506\n",
      "Trained at 2018-05-18 08:58:43.099385\n",
      "TP 17089 FP 3214 FN 2240 F 0.8623839321760194 Precision 0.8416982711914496 Recall 0.8841119561280977\n",
      "Epoch 17 start at 2018-05-18 09:01:00.707934\n",
      "Trained at 2018-05-18 09:06:40.434314\n",
      "TP 17119 FP 3138 FN 2210 F 0.8649017329358865 Precision 0.8450905859702819 Recall 0.8856640281442393\n",
      "Best so far\n",
      "Epoch 18 start at 2018-05-18 09:08:56.506679\n",
      "Trained at 2018-05-18 09:14:34.845457\n",
      "TP 17110 FP 3332 FN 2219 F 0.8604259384979005 Precision 0.837002250269054 Recall 0.8851984065393967\n",
      "Epoch 19 start at 2018-05-18 09:16:52.316204\n",
      "Trained at 2018-05-18 09:22:31.370120\n",
      "TP 17198 FP 3452 FN 2131 F 0.8603516846344331 Precision 0.8328329297820823 Recall 0.8897511511200786\n",
      "Epoch 20 start at 2018-05-18 09:24:52.452045\n",
      "Trained at 2018-05-18 09:30:32.133918\n",
      "TP 17168 FP 3291 FN 2161 F 0.8629737609329446 Precision 0.8391416980302068 Recall 0.8881990791039371\n",
      "Epoch 21 start at 2018-05-18 09:32:48.362490\n",
      "Trained at 2018-05-18 09:38:26.889536\n",
      "TP 17018 FP 2936 FN 2311 F 0.8664307715805819 Precision 0.8528615816377668 Recall 0.8804387190232293\n",
      "Best so far\n",
      "Epoch 22 start at 2018-05-18 09:40:43.843158\n",
      "Trained at 2018-05-18 09:46:22.945606\n",
      "TP 17161 FP 3273 FN 2168 F 0.8631642481704097 Precision 0.8398257805618088 Recall 0.8878369289668374\n",
      "Epoch 23 start at 2018-05-18 09:48:41.302013\n",
      "Trained at 2018-05-18 09:54:19.514458\n",
      "TP 17221 FP 3532 FN 2108 F 0.8592884586597476 Precision 0.8298077386401966 Recall 0.8909410729991205\n",
      "Epoch 24 start at 2018-05-18 09:56:37.918366\n",
      "Trained at 2018-05-18 10:02:17.303769\n",
      "TP 17168 FP 3253 FN 2161 F 0.8637987421383648 Precision 0.8407031976886539 Recall 0.8881990791039371\n",
      "Epoch 25 start at 2018-05-18 10:04:34.381119\n",
      "Trained at 2018-05-18 10:10:12.918029\n",
      "TP 17063 FP 3286 FN 2266 F 0.8600735924189727 Precision 0.8385178632856651 Recall 0.8827668270474417\n",
      "Epoch 26 start at 2018-05-18 10:12:29.397133\n",
      "Trained at 2018-05-18 10:18:09.583043\n",
      "TP 17144 FP 3135 FN 2185 F 0.8656837002625732 Precision 0.8454065782336407 Recall 0.8869574214910239\n",
      "Epoch 27 start at 2018-05-18 10:20:24.777268\n",
      "Trained at 2018-05-18 10:26:03.421624\n",
      "TP 17124 FP 3398 FN 2205 F 0.8594012697297433 Precision 0.8344215963356398 Recall 0.8859227068135962\n",
      "Epoch 28 start at 2018-05-18 10:28:18.714158\n",
      "Trained at 2018-05-18 10:33:58.345706\n",
      "TP 17135 FP 3148 FN 2194 F 0.8651418761991315 Precision 0.8447961346940788 Recall 0.8864917998861814\n",
      "Epoch 29 start at 2018-05-18 10:36:14.447776\n",
      "Trained at 2018-05-18 10:41:53.040688\n",
      "TP 17135 FP 3341 FN 2194 F 0.8609471171963321 Precision 0.8368333658917757 Recall 0.8864917998861814\n"
     ]
    }
   ],
   "source": [
    "from chemlistem import neominimodel_pred\n",
    "nmm = neominimodel_pred.PredNeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patful_a61k31a61_noct_s.txt\", 20000)\n",
    "del(nmm)\n",
    "nmm = neominimodel_pred.PredNeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patful_a61k31a61_noct_s.txt\", 100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-18 14:05:24.985869\n",
      "Shufflesplit at 2018-05-18 14:05:24.986324\n",
      "Read annots at 2018-05-18 14:05:25.043401\n",
      "Read train seqs at 2018-05-18 14:05:25.331376\n",
      "Read test seqs at 2018-05-18 14:06:08.537950\n",
      "Corpus read at 2018-05-18 14:06:21.098647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-18 14:06:25.245648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "5892\n",
      "0 0.6936265 0.6936265 2018-05-18 14:06:39.085528\n",
      "100 0.224411 0.001068146 2018-05-18 14:06:46.459174\n",
      "200 0.103735074 0.0005373661 2018-05-18 14:06:54.833089\n",
      "300 0.091227055 0.062267564 2018-05-18 14:07:00.882406\n",
      "400 0.08557668 0.14259303 2018-05-18 14:07:07.510888\n",
      "500 0.08401093 0.11730252 2018-05-18 14:07:14.876681\n",
      "600 0.0802719 0.3099702 2018-05-18 14:07:20.939944\n",
      "700 0.0735491 0.07416393 2018-05-18 14:07:28.463998\n",
      "800 0.08975423 0.05266884 2018-05-18 14:07:35.201286\n",
      "900 0.06336514 0.11011874 2018-05-18 14:07:41.277906\n",
      "1000 0.064495236 0.055088088 2018-05-18 14:07:46.717719\n",
      "1100 0.0662826 0.09186843 2018-05-18 14:07:54.265632\n",
      "1200 0.063415565 8.317992e-05 2018-05-18 14:08:01.457259\n",
      "1300 0.05981396 6.0098973e-05 2018-05-18 14:08:07.816851\n",
      "1400 0.06130339 0.10600555 2018-05-18 14:08:15.131377\n",
      "1500 0.055990905 0.05738253 2018-05-18 14:08:22.680213\n",
      "1600 0.04889333 5.519543e-05 2018-05-18 14:08:29.296306\n",
      "1700 0.061229963 0.13247897 2018-05-18 14:08:35.754668\n",
      "1800 0.061978765 0.03725454 2018-05-18 14:08:41.882479\n",
      "1900 0.057232782 0.5525013 2018-05-18 14:08:47.302593\n",
      "2000 0.053410616 5.3438518e-05 2018-05-18 14:08:54.932320\n",
      "2100 0.06253921 0.039551806 2018-05-18 14:09:01.997311\n",
      "2200 0.06217756 0.09656292 2018-05-18 14:09:08.966778\n",
      "2300 0.04575838 0.023088431 2018-05-18 14:09:16.258231\n",
      "2400 0.058275916 0.048096262 2018-05-18 14:09:23.057319\n",
      "2500 0.06149481 4.5777146e-05 2018-05-18 14:09:29.595899\n",
      "2600 0.046105266 6.854806e-05 2018-05-18 14:09:36.751370\n",
      "2700 0.058354326 0.0001214639 2018-05-18 14:09:43.124333\n",
      "2800 0.04876245 0.03395902 2018-05-18 14:09:48.986680\n",
      "2900 0.04348319 0.0034738851 2018-05-18 14:09:55.848466\n",
      "3000 0.05975669 0.08053383 2018-05-18 14:10:01.164155\n",
      "3100 0.039263077 1.7777233e-05 2018-05-18 14:10:08.125003\n",
      "3200 0.06202776 0.049060047 2018-05-18 14:10:13.241314\n",
      "3300 0.051462565 2.092321e-05 2018-05-18 14:10:19.245622\n",
      "3400 0.047869526 0.10086335 2018-05-18 14:10:27.735759\n",
      "3500 0.044890534 0.092677005 2018-05-18 14:10:34.714402\n",
      "3600 0.054455377 0.007199747 2018-05-18 14:10:41.696583\n",
      "3700 0.048133012 0.043174483 2018-05-18 14:10:48.779183\n",
      "3800 0.04973551 2.1202395e-05 2018-05-18 14:10:54.593069\n",
      "3900 0.051614318 0.15073979 2018-05-18 14:11:01.715439\n",
      "4000 0.05101689 0.08945191 2018-05-18 14:11:07.867717\n",
      "4100 0.036851108 1.1580378e-05 2018-05-18 14:11:15.967185\n",
      "4200 0.049502216 0.02015616 2018-05-18 14:11:22.205471\n",
      "4300 0.043064903 0.0669004 2018-05-18 14:11:28.495556\n",
      "4400 0.048462395 0.109786004 2018-05-18 14:11:35.459840\n",
      "4500 0.042181037 2.284058e-05 2018-05-18 14:11:42.755296\n",
      "4600 0.047460984 1.5815582e-05 2018-05-18 14:11:48.803297\n",
      "4700 0.046017226 0.01645407 2018-05-18 14:11:55.151331\n",
      "4800 0.05660059 0.012630446 2018-05-18 14:12:00.776317\n",
      "4900 0.04872104 0.03219746 2018-05-18 14:12:06.822003\n",
      "5000 0.05687288 0.059998337 2018-05-18 14:12:13.740832\n",
      "5100 0.04286001 0.019218024 2018-05-18 14:12:21.078033\n",
      "5200 0.044547364 0.030288767 2018-05-18 14:12:27.287888\n",
      "5300 0.040700957 4.9691724e-05 2018-05-18 14:12:34.865500\n",
      "5400 0.047764663 0.040275984 2018-05-18 14:12:41.047299\n",
      "5500 0.047183167 5.069912e-05 2018-05-18 14:12:46.987281\n",
      "5600 0.04010585 0.016074669 2018-05-18 14:12:53.550082\n",
      "5700 0.047680378 0.00022844067 2018-05-18 14:12:59.326268\n",
      "5800 0.055638682 0.053484432 2018-05-18 14:13:04.996260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-18 14:13:10.589245\n",
      "Trained at 2018-05-18 14:18:53.233578\n",
      "TP 6340 FP 3062 FN 12989 F 0.4413351432250879 Precision 0.6743246117847267 Recall 0.3280045527445807\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-18 14:21:20.664217\n",
      "Trained at 2018-05-18 14:27:01.500988\n",
      "TP 13164 FP 3817 FN 6165 F 0.7250895070228587 Precision 0.7752193628172663 Recall 0.6810492006829116\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-18 14:29:22.646508\n",
      "Trained at 2018-05-18 14:35:01.971307\n",
      "TP 14129 FP 2940 FN 5200 F 0.7763613385350844 Precision 0.8277579237213663 Recall 0.7309741838687982\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-18 14:37:19.572889\n",
      "Trained at 2018-05-18 14:42:58.332708\n",
      "TP 15154 FP 3080 FN 4175 F 0.8068578122088225 Precision 0.8310847866622793 Recall 0.7840033110869677\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-18 14:45:15.299937\n",
      "Trained at 2018-05-18 14:50:55.210841\n",
      "TP 15902 FP 3273 FN 3427 F 0.8259921047163931 Precision 0.8293089960886572 Recall 0.8227016400227637\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-18 14:53:07.377915\n",
      "Trained at 2018-05-18 14:58:47.260392\n",
      "TP 15962 FP 3043 FN 3367 F 0.8327855167736213 Precision 0.8398842409892133 Recall 0.8258057840550468\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-18 15:00:59.558395\n",
      "Trained at 2018-05-18 15:06:39.664965\n",
      "TP 16156 FP 3076 FN 3173 F 0.8379450740385364 Precision 0.8400582362728786 Recall 0.8358425164260955\n",
      "Best so far\n",
      "Epoch 7 start at 2018-05-18 15:08:51.756546\n",
      "Trained at 2018-05-18 15:14:30.602936\n",
      "TP 16150 FP 2910 FN 3179 F 0.8413868556096799 Precision 0.8473242392444911 Recall 0.8355321020228672\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-18 15:16:43.982462\n",
      "Trained at 2018-05-18 15:22:22.800146\n",
      "TP 16456 FP 3102 FN 2873 F 0.8463496798415923 Precision 0.8413948256467941 Recall 0.851363236587511\n",
      "Best so far\n",
      "Epoch 9 start at 2018-05-18 15:24:32.614048\n",
      "Trained at 2018-05-18 15:30:11.744863\n",
      "TP 16349 FP 2934 FN 2980 F 0.8468351807728167 Precision 0.8478452522947674 Recall 0.8458275130632728\n",
      "Best so far\n",
      "Epoch 10 start at 2018-05-18 15:32:23.027620\n",
      "Trained at 2018-05-18 15:38:01.862006\n",
      "TP 16611 FP 3315 FN 2718 F 0.846312571646924 Precision 0.8336344474555857 Recall 0.8593822753375757\n",
      "Epoch 11 start at 2018-05-18 15:40:12.794637\n",
      "Trained at 2018-05-18 15:45:53.088284\n",
      "TP 16500 FP 3111 FN 2829 F 0.847457627118644 Precision 0.8413645403090102 Recall 0.853639608877852\n",
      "Best so far\n",
      "Epoch 12 start at 2018-05-18 15:48:03.803867\n",
      "Trained at 2018-05-18 15:53:43.435132\n",
      "TP 16730 FP 3235 FN 2599 F 0.8515294955972922 Precision 0.8379664412722264 Recall 0.8655388276682705\n",
      "Best so far\n",
      "Epoch 13 start at 2018-05-18 15:55:53.862645\n",
      "Trained at 2018-05-18 16:01:32.838252\n",
      "TP 16646 FP 3243 FN 2683 F 0.8488959151410067 Precision 0.8369450449997486 Recall 0.8611930260230741\n",
      "Epoch 14 start at 2018-05-18 16:03:43.122141\n",
      "Trained at 2018-05-18 16:09:22.699578\n",
      "TP 16654 FP 3141 FN 2675 F 0.8513444433084552 Precision 0.8413235665572114 Recall 0.8616069118940453\n",
      "Epoch 15 start at 2018-05-18 16:11:33.990795\n",
      "Trained at 2018-05-18 16:17:13.668340\n",
      "TP 16637 FP 3078 FN 2692 F 0.852218010449749 Precision 0.8438752219122495 Recall 0.8607274044182317\n",
      "Best so far\n",
      "Epoch 16 start at 2018-05-18 16:19:23.179995\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-eeff6f72390f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneominimodel_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneominimodel_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPredNeoMiniModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../catted_patful_a61k31a61_noct_s.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel_pred.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname, unsupfile, nunsup)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;31m#    self.train_unsup_batch(self.usb.pop())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel_pred.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;31m#    self.train_unsup_batch(self.usb.pop())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel_pred.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;31m#    self.train_unsup_batch(self.usb.pop())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chemlistem/chemlistem/utils.py\u001b[0m in \u001b[0;36mtobits\u001b[0;34m(bit, bits)\u001b[0m\n\u001b[1;32m     16\u001b[0m \t\"\"\"\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbit\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msobie_scores_to_char_ents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from chemlistem import neominimodel_pred\n",
    "nmm = neominimodel_pred.PredNeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patful_a61k31a61_noct_s.txt\", 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-18 16:27:06.668893\n",
      "Shufflesplit at 2018-05-18 16:27:06.668991\n",
      "Read annots at 2018-05-18 16:27:06.725568\n",
      "Read train seqs at 2018-05-18 16:27:07.004655\n",
      "Read test seqs at 2018-05-18 16:27:49.564693\n",
      "Corpus read at 2018-05-18 16:28:02.063253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-18 16:28:06.251837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "3931 batches of unsupervised\n",
      "0 9.0223875 [9.0223875, 4.510597, 4.5117903] 2018-05-18 16:28:17.383493\n",
      "100 6.264867 [5.617977, 2.8156147, 2.8023622] 2018-05-18 16:28:36.971928\n",
      "200 5.2033234 [4.893625, 2.4395614, 2.4540634] 2018-05-18 16:28:58.221970\n",
      "300 4.60483 [3.9851265, 1.972824, 2.0123024] 2018-05-18 16:29:20.908429\n",
      "400 4.176756 [3.935158, 1.950841, 1.9843171] 2018-05-18 16:29:46.286612\n",
      "500 3.8612921 [3.731245, 1.828537, 1.9027082] 2018-05-18 16:30:11.578721\n",
      "600 3.6315615 [3.2511644, 1.6048248, 1.6463398] 2018-05-18 16:30:37.872566\n",
      "700 3.4641986 [3.4127655, 1.6901889, 1.7225766] 2018-05-18 16:31:00.668153\n",
      "800 3.2165728 [3.5909815, 1.7575796, 1.8334019] 2018-05-18 16:31:19.893561\n",
      "900 3.1244504 [3.003767, 1.4877436, 1.5160235] 2018-05-18 16:31:42.209774\n",
      "1000 3.0094736 [3.4732711, 1.7145653, 1.7587059] 2018-05-18 16:32:06.406429\n",
      "1100 2.895741 [2.966436, 1.4519229, 1.514513] 2018-05-18 16:32:26.262644\n",
      "1200 2.8796744 [3.048789, 1.5056921, 1.5430968] 2018-05-18 16:32:51.130831\n",
      "1300 2.767269 [2.3351936, 1.1525115, 1.1826823] 2018-05-18 16:33:12.398012\n",
      "1400 2.6668077 [2.6292434, 1.2979556, 1.3312876] 2018-05-18 16:33:30.700291\n",
      "1500 2.7001078 [3.0143776, 1.4896383, 1.5247393] 2018-05-18 16:33:54.736270\n",
      "1600 2.6402712 [2.2869196, 1.1329857, 1.153934] 2018-05-18 16:34:17.281709\n",
      "1700 2.632202 [2.7515993, 1.3641433, 1.3874562] 2018-05-18 16:34:48.170523\n",
      "1800 2.5088978 [2.3294144, 1.1459529, 1.1834615] 2018-05-18 16:35:08.136605\n",
      "1900 2.4539917 [2.3788297, 1.1701376, 1.2086921] 2018-05-18 16:35:24.211004\n",
      "2000 2.4578266 [2.4292135, 1.1993529, 1.2298605] 2018-05-18 16:35:46.226902\n",
      "2100 2.4092185 [3.0689125, 1.5337079, 1.5352046] 2018-05-18 16:36:05.489559\n",
      "2200 2.4336324 [2.544004, 1.2592266, 1.2847774] 2018-05-18 16:36:27.574857\n",
      "2300 2.3918395 [1.9533036, 0.9689495, 0.98435414] 2018-05-18 16:36:49.066173\n",
      "2400 2.4224489 [2.317937, 1.1450292, 1.1729078] 2018-05-18 16:37:11.944111\n",
      "2500 2.3921297 [2.3300266, 1.1419256, 1.1881009] 2018-05-18 16:37:33.082348\n",
      "2600 2.3792415 [2.3953257, 1.186485, 1.2088406] 2018-05-18 16:37:56.043726\n",
      "2700 2.2644527 [2.1041899, 1.0279257, 1.0762641] 2018-05-18 16:38:14.360664\n",
      "2800 2.307809 [2.9058146, 1.4487007, 1.4571141] 2018-05-18 16:38:39.464855\n",
      "2900 2.2687433 [2.0952034, 1.0334265, 1.0617769] 2018-05-18 16:39:03.976763\n",
      "3000 2.2966077 [1.9602437, 0.96614885, 0.9940948] 2018-05-18 16:39:29.637597\n",
      "3100 2.3298292 [2.2253141, 1.0998182, 1.1254958] 2018-05-18 16:39:58.402465\n",
      "3200 2.2664225 [2.1369576, 1.0482222, 1.0887353] 2018-05-18 16:40:25.449411\n",
      "3300 2.2967262 [2.0193207, 1.0035679, 1.0157528] 2018-05-18 16:40:47.940366\n",
      "3400 2.2129374 [1.6604977, 0.71701336, 0.9434843] 2018-05-18 16:41:13.129206\n",
      "3500 2.1628616 [2.0745292, 1.0190341, 1.055495] 2018-05-18 16:41:33.043546\n",
      "3600 2.2086153 [2.4286947, 1.2084969, 1.2201979] 2018-05-18 16:41:55.718169\n",
      "3700 2.1400056 [2.0692515, 1.0174743, 1.0517774] 2018-05-18 16:42:16.506274\n",
      "3800 2.1733205 [2.0269094, 1.0086975, 1.0182117] 2018-05-18 16:42:40.571988\n",
      "3900 2.1864736 [1.9901788, 0.9706254, 1.0195534] 2018-05-18 16:43:07.724661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-18 16:43:11.935455\n",
      "Trained at 2018-05-18 16:53:25.800113\n",
      "TP 15191 FP 3191 FN 4138 F 0.8056535228447933 Precision 0.8264062670003264 Recall 0.785917533240209\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-18 16:55:57.479412\n",
      "Trained at 2018-05-18 17:06:08.435946\n",
      "TP 15928 FP 2875 FN 3401 F 0.8354138256582399 Precision 0.8470988672020422 Recall 0.8240467691034198\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-18 17:08:41.140109\n",
      "Trained at 2018-05-18 17:18:51.190251\n",
      "TP 16446 FP 2955 FN 2883 F 0.8492641363284276 Precision 0.8476882634915726 Recall 0.8508458792487972\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-18 17:21:23.561033\n",
      "Trained at 2018-05-18 17:31:34.845813\n",
      "TP 16592 FP 2927 FN 2737 F 0.8542009884678748 Precision 0.8500435473128747 Recall 0.8583992963940194\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-18 17:34:06.445600\n",
      "Trained at 2018-05-18 17:44:16.432917\n",
      "TP 16929 FP 3143 FN 2400 F 0.8593182914139235 Precision 0.8434137106416899 Recall 0.875834238708676\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-18 17:46:48.825795\n",
      "Trained at 2018-05-18 17:56:58.402787\n",
      "TP 16497 FP 2608 FN 2832 F 0.8584586563979809 Precision 0.8634912326616069 Recall 0.8534844016762377\n",
      "Epoch 6 start at 2018-05-18 17:59:29.915654\n",
      "Trained at 2018-05-18 18:09:39.709641\n",
      "TP 16733 FP 2904 FN 2596 F 0.858851306267002 Precision 0.8521159036512705 Recall 0.8656940348698846\n",
      "Epoch 7 start at 2018-05-18 18:12:11.817388\n",
      "Trained at 2018-05-18 18:22:19.741669\n",
      "TP 16939 FP 2997 FN 2390 F 0.8628040239398956 Precision 0.8496689406099518 Recall 0.8763515960473899\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-18 18:24:51.868645\n",
      "Trained at 2018-05-18 18:34:59.507630\n",
      "TP 16754 FP 3179 FN 2575 F 0.8534460801793082 Precision 0.840515727687754 Recall 0.8667804852811837\n",
      "Epoch 9 start at 2018-05-18 18:37:33.075285\n",
      "Trained at 2018-05-18 18:47:42.833261\n",
      "TP 16925 FP 3166 FN 2404 F 0.858701166920345 Precision 0.8424170026379971 Recall 0.8756272957731905\n",
      "Epoch 10 start at 2018-05-18 18:50:17.295149\n",
      "Trained at 2018-05-18 19:00:25.730257\n",
      "TP 16495 FP 2583 FN 2834 F 0.8589580024474706 Precision 0.8646084495230107 Recall 0.853380930208495\n",
      "Epoch 11 start at 2018-05-18 19:02:58.327224\n",
      "Trained at 2018-05-18 19:13:07.101376\n",
      "TP 16679 FP 2841 FN 2650 F 0.8586578805117249 Precision 0.8544569672131147 Recall 0.8629003052408298\n",
      "Epoch 12 start at 2018-05-18 19:15:40.203119\n",
      "Trained at 2018-05-18 19:25:49.350646\n",
      "TP 16916 FP 3300 FN 2413 F 0.855531672777848 Precision 0.8367629600316581 Recall 0.8751616741683481\n",
      "Epoch 13 start at 2018-05-18 19:28:24.163914\n",
      "Trained at 2018-05-18 19:38:33.788844\n",
      "TP 16694 FP 2826 FN 2635 F 0.859430101160905 Precision 0.8552254098360655 Recall 0.8636763412489006\n",
      "Epoch 14 start at 2018-05-18 19:41:06.166898\n",
      "Trained at 2018-05-18 19:51:14.689812\n",
      "TP 16863 FP 3191 FN 2466 F 0.8563593428636721 Precision 0.8408796250124664 Recall 0.8724196802731646\n",
      "Epoch 15 start at 2018-05-18 19:53:48.137506\n",
      "Trained at 2018-05-18 20:03:57.946786\n",
      "TP 16810 FP 3055 FN 2519 F 0.8577843547481757 Precision 0.8462119305310848 Recall 0.8696776863779813\n",
      "Epoch 16 start at 2018-05-18 20:06:30.319496\n",
      "Trained at 2018-05-18 20:16:38.222108\n",
      "TP 16943 FP 3248 FN 2386 F 0.8574392712550607 Precision 0.8391362488237334 Recall 0.8765585389828755\n",
      "Epoch 17 start at 2018-05-18 20:19:10.122350\n",
      "Trained at 2018-05-18 20:29:18.301081\n",
      "TP 16851 FP 3041 FN 2478 F 0.8592845669411795 Precision 0.8471244721496078 Recall 0.8717988514667081\n",
      "Epoch 18 start at 2018-05-18 20:31:50.875664\n",
      "Trained at 2018-05-18 20:42:00.239500\n",
      "TP 16875 FP 3061 FN 2454 F 0.8595441232649943 Precision 0.8464586677367576 Recall 0.8730405090796213\n",
      "Epoch 19 start at 2018-05-18 20:44:31.610248\n",
      "Trained at 2018-05-18 20:54:41.024961\n",
      "TP 16787 FP 2974 FN 2542 F 0.8588897416218981 Precision 0.8495015434441577 Recall 0.8684877644989394\n",
      "Epoch 20 start at 2018-05-18 20:57:12.096144\n",
      "Trained at 2018-05-18 21:07:21.000520\n",
      "TP 16997 FP 3275 FN 2332 F 0.8584126663468095 Precision 0.8384471191791634 Recall 0.8793522686119303\n",
      "Epoch 21 start at 2018-05-18 21:09:53.795843\n",
      "Trained at 2018-05-18 21:20:03.176955\n",
      "TP 16945 FP 3090 FN 2384 F 0.8609389289706331 Precision 0.8457699026703269 Recall 0.8766620104506182\n",
      "Epoch 22 start at 2018-05-18 21:22:35.469737\n",
      "Trained at 2018-05-18 21:32:44.047097\n",
      "TP 16540 FP 2767 FN 2789 F 0.8561962936121752 Precision 0.856684104210908 Recall 0.8557090382327074\n",
      "Epoch 23 start at 2018-05-18 21:35:14.837330\n",
      "Trained at 2018-05-18 21:45:24.195569\n",
      "TP 16901 FP 3219 FN 2428 F 0.8568531521711577 Precision 0.8400099403578529 Recall 0.8743856381602773\n",
      "Epoch 24 start at 2018-05-18 21:47:54.803974\n",
      "Trained at 2018-05-18 21:58:04.730425\n",
      "TP 16634 FP 2932 FN 2695 F 0.8553284483866821 Precision 0.8501482162935705 Recall 0.8605721972166175\n",
      "Epoch 25 start at 2018-05-18 22:00:38.126716\n",
      "Trained at 2018-05-18 22:10:46.838908\n",
      "TP 16640 FP 2815 FN 2689 F 0.858085808580858 Precision 0.8553071189925469 Recall 0.8608826116198458\n",
      "Epoch 26 start at 2018-05-18 22:13:19.540362\n",
      "Trained at 2018-05-18 22:23:27.667038\n",
      "TP 16774 FP 2988 FN 2555 F 0.858202655342662 Precision 0.8488007286711872 Recall 0.8678151999586114\n",
      "Epoch 27 start at 2018-05-18 22:25:59.345666\n",
      "Trained at 2018-05-18 22:36:08.748642\n",
      "TP 16881 FP 3059 FN 2448 F 0.8597621533525173 Precision 0.8465897693079237 Recall 0.8733509234828496\n",
      "Epoch 28 start at 2018-05-18 22:38:42.092797\n",
      "Trained at 2018-05-18 22:48:51.015297\n",
      "TP 16707 FP 2934 FN 2622 F 0.8574287913779831 Precision 0.8506186039407362 Recall 0.8643489057892286\n",
      "Epoch 29 start at 2018-05-18 22:51:25.226349\n",
      "Trained at 2018-05-18 23:01:34.394831\n",
      "TP 16965 FP 3196 FN 2364 F 0.8592048619903773 Precision 0.8414761172560885 Recall 0.877696725128046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-18 23:04:08.187481\n",
      "Shufflesplit at 2018-05-18 23:04:08.187605\n",
      "Read annots at 2018-05-18 23:04:08.253969\n",
      "Read train seqs at 2018-05-18 23:04:08.471710\n",
      "Read test seqs at 2018-05-18 23:04:47.043507\n",
      "Corpus read at 2018-05-18 23:04:57.836684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-18 23:05:02.041070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3931 batches of unsupervised\n",
      "0 9.021431 [9.021431, 4.511057, 4.510374] 2018-05-18 23:05:06.484042\n",
      "100 6.21033 [5.458508, 2.7274303, 2.7310777] 2018-05-18 23:05:27.521870\n",
      "200 5.1785197 [4.6467423, 2.3235745, 2.3231678] 2018-05-18 23:05:46.676194\n",
      "300 4.6243043 [4.507566, 2.2519598, 2.2556062] 2018-05-18 23:06:08.611877\n",
      "400 4.1945076 [4.097545, 2.053132, 2.044413] 2018-05-18 23:06:34.108140\n",
      "500 3.8678358 [3.7377043, 1.8756709, 1.8620334] 2018-05-18 23:06:57.324653\n",
      "600 3.585979 [3.535663, 1.7681191, 1.7675438] 2018-05-18 23:07:19.180466\n",
      "700 3.39089 [3.3007336, 1.6579576, 1.6427761] 2018-05-18 23:07:41.231085\n",
      "800 3.2434628 [2.8928928, 1.4478966, 1.4449964] 2018-05-18 23:08:01.310520\n",
      "900 3.1238549 [2.7630854, 1.3806794, 1.382406] 2018-05-18 23:08:26.294844\n",
      "1000 3.031916 [3.324337, 1.6822048, 1.6421323] 2018-05-18 23:08:50.428437\n",
      "1100 2.94123 [3.2375414, 1.6169147, 1.6206267] 2018-05-18 23:09:13.522838\n",
      "1200 2.8394248 [2.2824419, 1.1465054, 1.1359365] 2018-05-18 23:09:36.384108\n",
      "1300 2.7376902 [2.9504914, 1.4789544, 1.4715369] 2018-05-18 23:09:54.282674\n",
      "1400 2.7609646 [2.6312244, 1.2982793, 1.3329451] 2018-05-18 23:10:18.885831\n",
      "1500 2.7395067 [2.9940615, 1.4942703, 1.4997913] 2018-05-18 23:10:43.231744\n",
      "1600 2.6234286 [2.4132338, 1.2078415, 1.2053921] 2018-05-18 23:11:05.105752\n",
      "1700 2.6516984 [2.8833952, 1.419287, 1.4641082] 2018-05-18 23:11:30.256141\n",
      "1800 2.553486 [2.9567323, 1.4733002, 1.483432] 2018-05-18 23:11:50.677008\n",
      "1900 2.4903772 [2.8995895, 1.4534407, 1.446149] 2018-05-18 23:12:07.710042\n",
      "2000 2.50586 [2.66152, 1.3317451, 1.3297747] 2018-05-18 23:12:33.034439\n",
      "2100 2.4109595 [2.1619344, 1.0546237, 1.1073105] 2018-05-18 23:12:54.198282\n",
      "2200 2.3969736 [2.80684, 1.405642, 1.4011978] 2018-05-18 23:13:13.716666\n",
      "2300 2.394416 [2.304765, 1.1634605, 1.1413045] 2018-05-18 23:13:37.314068\n",
      "2400 2.3764374 [2.2332804, 1.1131613, 1.1201191] 2018-05-18 23:13:56.142329\n",
      "2500 2.3016942 [2.2162254, 1.0967879, 1.1194375] 2018-05-18 23:14:14.020857\n",
      "2600 2.341275 [2.3213854, 1.1568836, 1.1645019] 2018-05-18 23:14:38.183958\n",
      "2700 2.35508 [2.5411901, 1.2507432, 1.2904469] 2018-05-18 23:15:01.823263\n",
      "2800 2.2939558 [2.2526584, 1.1086006, 1.1440579] 2018-05-18 23:15:22.348280\n",
      "2900 2.2989929 [2.2253623, 1.1045794, 1.1207827] 2018-05-18 23:15:45.158702\n",
      "3000 2.2818685 [1.8535187, 0.9128577, 0.94066095] 2018-05-18 23:16:06.759919\n",
      "3100 2.189266 [2.6497614, 1.3189199, 1.3308415] 2018-05-18 23:16:24.395620\n",
      "3200 2.285956 [2.7924204, 1.3954666, 1.3969538] 2018-05-18 23:16:48.498031\n",
      "3300 2.2250216 [1.9618075, 0.97214735, 0.9896601] 2018-05-18 23:17:14.003196\n",
      "3400 2.2495286 [2.0515807, 1.0324656, 1.0191151] 2018-05-18 23:17:37.653041\n",
      "3500 2.1856363 [2.1376197, 1.0719447, 1.065675] 2018-05-18 23:18:01.753673\n",
      "3600 2.2172582 [2.6266847, 1.3190345, 1.3076503] 2018-05-18 23:18:30.516051\n",
      "3700 2.174217 [2.0711162, 1.0358622, 1.035254] 2018-05-18 23:18:51.957179\n",
      "3800 2.1633954 [1.9987867, 1.0023384, 0.99644834] 2018-05-18 23:19:12.674482\n",
      "3900 2.12907 [1.7997453, 0.8883322, 0.91141313] 2018-05-18 23:19:32.265179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-18 23:19:38.483202\n",
      "Trained at 2018-05-18 23:38:49.270407\n",
      "TP 15438 FP 2860 FN 3891 F 0.8205809657958381 Precision 0.843698764892338 Recall 0.7986962595064411\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-18 23:42:07.654626\n",
      "Trained at 2018-05-19 00:01:15.514285\n",
      "TP 16355 FP 3102 FN 2974 F 0.8433455370494508 Precision 0.8405715166778024 Recall 0.8461379274665011\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-19 00:04:35.488126\n",
      "Trained at 2018-05-19 00:23:43.584136\n",
      "TP 16574 FP 2956 FN 2755 F 0.853032759463702 Precision 0.8486431131592422 Recall 0.8574680531843344\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-19 00:27:05.089559\n",
      "Trained at 2018-05-19 00:46:14.769964\n",
      "TP 16847 FP 3102 FN 2482 F 0.8578339019298334 Precision 0.844503483883904 Recall 0.8715919085312225\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-19 00:49:32.989417\n",
      "Trained at 2018-05-19 01:08:43.338721\n",
      "TP 16491 FP 2698 FN 2838 F 0.8562749883171504 Precision 0.8593986137891501 Recall 0.8531739872730094\n",
      "Epoch 5 start at 2018-05-19 01:12:01.864222\n",
      "Trained at 2018-05-19 01:31:10.853076\n",
      "TP 16764 FP 2904 FN 2565 F 0.8597584429571505 Precision 0.8523489932885906 Recall 0.8672978426198976\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-19 01:34:34.111892\n",
      "Trained at 2018-05-19 01:53:43.703422\n",
      "TP 16746 FP 2956 FN 2583 F 0.8580871614870231 Precision 0.8499644706121205 Recall 0.8663665994102127\n",
      "Epoch 7 start at 2018-05-19 01:57:03.603039\n",
      "Trained at 2018-05-19 02:16:12.724809\n",
      "TP 16450 FP 2689 FN 2879 F 0.8552563169387543 Precision 0.8595015413553477 Recall 0.8510528221842827\n",
      "Epoch 8 start at 2018-05-19 02:19:31.603859\n",
      "Trained at 2018-05-19 02:38:40.511808\n",
      "TP 16779 FP 3067 FN 2550 F 0.8566177409061901 Precision 0.8454600423259095 Recall 0.8680738786279684\n",
      "Epoch 9 start at 2018-05-19 02:42:03.214397\n",
      "Trained at 2018-05-19 03:01:12.628646\n",
      "TP 16695 FP 2878 FN 2634 F 0.8583106267029973 Precision 0.8529607111837736 Recall 0.863728076982772\n",
      "Epoch 10 start at 2018-05-19 03:04:32.788132\n",
      "Trained at 2018-05-19 03:23:41.753760\n",
      "TP 16857 FP 3059 FN 2472 F 0.8590648490253535 Precision 0.8464049005824463 Recall 0.8721092658699363\n",
      "Epoch 11 start at 2018-05-19 03:26:59.762806\n",
      "Trained at 2018-05-19 03:46:08.882454\n",
      "TP 16758 FP 2954 FN 2571 F 0.8584821085525474 Precision 0.8501420454545454 Recall 0.8669874282166693\n",
      "Epoch 12 start at 2018-05-19 03:49:28.697448\n",
      "Trained at 2018-05-19 04:08:39.052551\n",
      "TP 16869 FP 3098 FN 2460 F 0.858560667752443 Precision 0.8448439925877698 Recall 0.872730094676393\n",
      "Epoch 13 start at 2018-05-19 04:11:58.822895\n",
      "Trained at 2018-05-19 04:31:08.107388\n",
      "TP 16661 FP 2760 FN 2668 F 0.8599225806451612 Precision 0.8578857937284383 Recall 0.8619690620311449\n",
      "Best so far\n",
      "Epoch 14 start at 2018-05-19 04:34:29.123109\n",
      "Trained at 2018-05-19 04:53:38.305058\n",
      "TP 16872 FP 3165 FN 2457 F 0.8571864045115074 Precision 0.8420422218895044 Recall 0.8728853018780072\n",
      "Epoch 15 start at 2018-05-19 04:57:00.985186\n",
      "Trained at 2018-05-19 05:16:10.274208\n",
      "TP 16801 FP 3082 FN 2528 F 0.8569315515658472 Precision 0.8449932102801389 Recall 0.8692120647731388\n",
      "Epoch 16 start at 2018-05-19 05:19:30.551796\n",
      "Trained at 2018-05-19 05:38:38.865298\n",
      "TP 16828 FP 2996 FN 2501 F 0.8596020739151534 Precision 0.8488700564971752 Recall 0.8706089295876662\n",
      "Epoch 17 start at 2018-05-19 05:41:59.057620\n",
      "Trained at 2018-05-19 06:01:09.200321\n",
      "TP 16616 FP 2780 FN 2713 F 0.8581536475145255 Precision 0.8566714786553928 Recall 0.8596409540069326\n",
      "Epoch 18 start at 2018-05-19 06:04:29.957920\n",
      "Trained at 2018-05-19 06:23:40.107745\n",
      "TP 16884 FP 3049 FN 2445 F 0.8600682593856656 Precision 0.8470375758791953 Recall 0.8735061306844638\n",
      "Best so far\n",
      "Epoch 19 start at 2018-05-19 06:27:01.823322\n",
      "Trained at 2018-05-19 06:46:10.555872\n",
      "TP 16781 FP 2910 FN 2548 F 0.8601230138390569 Precision 0.8522167487684729 Recall 0.8681773500957111\n",
      "Best so far\n",
      "Epoch 20 start at 2018-05-19 06:49:31.918917\n",
      "Trained at 2018-05-19 07:08:42.362111\n",
      "TP 16665 FP 2752 FN 2664 F 0.8602178289371806 Precision 0.8582685275789257 Recall 0.8621760049666305\n",
      "Best so far\n",
      "Epoch 21 start at 2018-05-19 07:12:02.044265\n",
      "Trained at 2018-05-19 07:31:11.918332\n",
      "TP 16781 FP 2922 FN 2548 F 0.8598585775773724 Precision 0.8516977110084759 Recall 0.8681773500957111\n",
      "Epoch 22 start at 2018-05-19 07:34:31.885164\n",
      "Trained at 2018-05-19 07:53:41.894854\n",
      "TP 16716 FP 2978 FN 2613 F 0.8567255208466802 Precision 0.8487864324159643 Recall 0.864814527394071\n",
      "Epoch 23 start at 2018-05-19 07:57:02.110108\n",
      "Trained at 2018-05-19 08:16:10.305463\n",
      "TP 16818 FP 2942 FN 2511 F 0.86049783826652 Precision 0.8511133603238866 Recall 0.8700915722489524\n",
      "Best so far\n",
      "Epoch 24 start at 2018-05-19 08:19:29.509645\n",
      "Trained at 2018-05-19 08:38:39.247802\n",
      "TP 16872 FP 3057 FN 2457 F 0.8595445514290081 Precision 0.8466054493451753 Recall 0.8728853018780072\n",
      "Epoch 25 start at 2018-05-19 08:41:58.629527\n",
      "Trained at 2018-05-19 09:01:05.375958\n",
      "TP 16821 FP 3054 FN 2508 F 0.8581267217630854 Precision 0.8463396226415094 Recall 0.8702467794505665\n",
      "Epoch 26 start at 2018-05-19 09:04:28.263870\n",
      "Trained at 2018-05-19 09:23:35.880139\n",
      "TP 16632 FP 2725 FN 2697 F 0.8598459390994158 Precision 0.8592240533140466 Recall 0.8604687257488748\n",
      "Epoch 27 start at 2018-05-19 09:26:57.588229\n",
      "Trained at 2018-05-19 09:46:04.451504\n",
      "TP 16700 FP 2899 FN 2629 F 0.8579942457870942 Precision 0.8520842900147967 Recall 0.8639867556521289\n",
      "Epoch 28 start at 2018-05-19 09:49:23.282632\n",
      "Trained at 2018-05-19 10:08:30.667400\n",
      "TP 16995 FP 3227 FN 2334 F 0.8593967282748856 Precision 0.8404213233112452 Recall 0.8792487971441875\n",
      "Epoch 29 start at 2018-05-19 10:11:50.184322\n",
      "Trained at 2018-05-19 10:30:57.937894\n",
      "TP 16613 FP 2824 FN 2716 F 0.8570912655419698 Precision 0.8547100890055049 Recall 0.8594857468053184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-19 10:34:19.475888\n",
      "Shufflesplit at 2018-05-19 10:34:19.476021\n",
      "Read annots at 2018-05-19 10:34:19.550372\n",
      "Read train seqs at 2018-05-19 10:34:19.763058\n",
      "Read test seqs at 2018-05-19 10:34:56.855416\n",
      "Corpus read at 2018-05-19 10:35:06.750819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-19 10:35:10.931110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3931 batches of unsupervised\n",
      "0 9.023195 [9.023195, 4.5126753, 4.5105205] 2018-05-19 10:35:15.430733\n",
      "100 6.137813 [5.3887825, 2.6823003, 2.706482] 2018-05-19 10:35:34.752389\n",
      "200 5.114913 [5.1307564, 2.5570216, 2.5737348] 2018-05-19 10:35:54.759765\n",
      "300 4.5649014 [4.278202, 2.0975695, 2.1806324] 2018-05-19 10:36:10.864420\n",
      "400 4.160021 [3.9047484, 1.9246521, 1.9800963] 2018-05-19 10:36:33.572746\n",
      "500 3.8489544 [4.327224, 2.1524267, 2.174797] 2018-05-19 10:36:55.419300\n",
      "600 3.5649526 [3.216637, 1.5859885, 1.6306484] 2018-05-19 10:37:15.247086\n",
      "700 3.4311504 [3.199997, 1.573804, 1.626193] 2018-05-19 10:37:39.203434\n",
      "800 3.3052416 [3.6750169, 1.8191917, 1.8558252] 2018-05-19 10:38:08.153925\n",
      "900 3.0876727 [2.7733345, 1.365248, 1.4080864] 2018-05-19 10:38:25.075959\n",
      "1000 2.9901304 [3.2927122, 1.6273191, 1.665393] 2018-05-19 10:38:46.872808\n",
      "1100 2.8988693 [2.3506029, 1.1558971, 1.1947057] 2018-05-19 10:39:08.488701\n",
      "1200 2.8741324 [2.854813, 1.4119654, 1.4428476] 2018-05-19 10:39:33.592851\n",
      "1300 2.7568164 [2.6628628, 1.3111061, 1.3517567] 2018-05-19 10:39:57.003745\n",
      "1400 2.6796649 [2.4869883, 1.2337168, 1.2532715] 2018-05-19 10:40:17.642633\n",
      "1500 2.619552 [2.5296378, 1.2427179, 1.2869201] 2018-05-19 10:40:37.700265\n",
      "1600 2.5337174 [2.133729, 1.0385326, 1.0951964] 2018-05-19 10:40:58.113274\n",
      "1700 2.5722082 [2.4998827, 1.2280624, 1.2718202] 2018-05-19 10:41:22.915057\n",
      "1800 2.5139012 [2.4555345, 1.2130535, 1.2424809] 2018-05-19 10:41:43.992218\n",
      "1900 2.49454 [2.4425416, 1.2100519, 1.2324896] 2018-05-19 10:42:05.845575\n",
      "2000 2.4839818 [2.2563944, 1.1162577, 1.1401367] 2018-05-19 10:42:28.130307\n",
      "2100 2.4374857 [2.4271865, 1.1819106, 1.2452759] 2018-05-19 10:42:51.562005\n",
      "2200 2.3500595 [3.0353127, 1.5135293, 1.5217832] 2018-05-19 10:43:11.389928\n",
      "2300 2.3663256 [2.1408236, 1.0553287, 1.0854949] 2018-05-19 10:43:30.557462\n",
      "2400 2.364569 [2.6272433, 1.2957922, 1.331451] 2018-05-19 10:43:52.420148\n",
      "2500 2.3473845 [1.8152709, 0.9003596, 0.91491127] 2018-05-19 10:44:16.456218\n",
      "2600 2.3611982 [2.065756, 1.0264578, 1.0392983] 2018-05-19 10:44:42.965512\n",
      "2700 2.3046613 [3.0421393, 1.5038013, 1.538338] 2018-05-19 10:45:03.047490\n",
      "2800 2.2682846 [2.317835, 1.1446757, 1.1731594] 2018-05-19 10:45:25.547153\n",
      "2900 2.2523887 [2.194048, 1.0756108, 1.118437] 2018-05-19 10:45:44.990541\n",
      "3000 2.2860534 [2.1353993, 1.0629467, 1.0724525] 2018-05-19 10:46:10.969022\n",
      "3100 2.2141423 [1.950331, 0.9642776, 0.9860533] 2018-05-19 10:46:30.302843\n",
      "3200 2.2393887 [2.2821727, 1.1142457, 1.167927] 2018-05-19 10:46:51.673127\n",
      "3300 2.2233047 [2.4098496, 1.1948533, 1.2149963] 2018-05-19 10:47:14.998215\n",
      "3400 2.194601 [1.6059843, 0.7910944, 0.8148899] 2018-05-19 10:47:36.940380\n",
      "3500 2.1925976 [2.0247624, 0.9962139, 1.0285485] 2018-05-19 10:47:57.324726\n",
      "3600 2.2238593 [2.6920123, 1.323758, 1.3682544] 2018-05-19 10:48:18.943000\n",
      "3700 2.2146988 [1.8823377, 0.92951727, 0.9528204] 2018-05-19 10:48:41.772581\n",
      "3800 2.182207 [2.3194852, 1.1448177, 1.1746676] 2018-05-19 10:49:05.986582\n",
      "3900 2.1424077 [1.7268425, 0.8635535, 0.863289] 2018-05-19 10:49:30.383042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-19 10:49:37.418056\n",
      "Trained at 2018-05-19 11:26:29.201407\n",
      "TP 16462 FP 3632 FN 2867 F 0.8351469954087716 Precision 0.8192495272220564 Recall 0.8516736509907393\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-19 11:31:18.775024\n",
      "Trained at 2018-05-19 12:08:09.199431\n",
      "TP 16643 FP 3206 FN 2686 F 0.8496094747051917 Precision 0.8384805279862966 Recall 0.86103781882146\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-19 12:12:57.941734\n",
      "Trained at 2018-05-19 12:49:48.455072\n",
      "TP 16512 FP 2950 FN 2817 F 0.8513314944188085 Precision 0.8484225670537457 Recall 0.8542604376843086\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-19 12:54:38.851875\n",
      "Trained at 2018-05-19 13:31:27.781609\n",
      "TP 16745 FP 3297 FN 2584 F 0.8506260953493688 Precision 0.8354954595349765 Recall 0.8663148636763413\n",
      "Epoch 4 start at 2018-05-19 13:36:19.392133\n",
      "Trained at 2018-05-19 14:13:09.999826\n",
      "TP 16651 FP 2988 FN 2678 F 0.8545986450420858 Precision 0.8478537603747645 Recall 0.861451704692431\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-19 14:17:59.668349\n",
      "Trained at 2018-05-19 14:54:51.047722\n",
      "TP 16799 FP 3018 FN 2530 F 0.8582741531701834 Precision 0.8477065146086693 Recall 0.869108593305396\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-19 14:59:40.441834\n",
      "Trained at 2018-05-19 15:36:30.180635\n",
      "TP 16504 FP 2657 FN 2825 F 0.8575733956871915 Precision 0.8613329158185898 Recall 0.8538465518133375\n",
      "Epoch 7 start at 2018-05-19 15:41:21.264404\n",
      "Trained at 2018-05-19 16:18:12.377410\n",
      "TP 16847 FP 3049 FN 2482 F 0.8589929891650733 Precision 0.8467531162042622 Recall 0.8715919085312225\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-19 16:23:01.947116\n",
      "Trained at 2018-05-19 16:59:52.633045\n",
      "TP 16738 FP 2817 FN 2591 F 0.8609196584713507 Precision 0.8559447711582715 Recall 0.8659527135392415\n",
      "Best so far\n",
      "Epoch 9 start at 2018-05-19 17:04:42.598588\n",
      "Trained at 2018-05-19 17:41:33.498388\n",
      "TP 16736 FP 2966 FN 2593 F 0.8575747482770106 Precision 0.8494569079281291 Recall 0.8658492420714988\n",
      "Epoch 10 start at 2018-05-19 17:46:19.970497\n",
      "Trained at 2018-05-19 18:23:10.097403\n",
      "TP 16443 FP 2606 FN 2886 F 0.8568971806764292 Precision 0.8631949183684183 Recall 0.850690672047183\n",
      "Epoch 11 start at 2018-05-19 18:27:59.000881\n",
      "Trained at 2018-05-19 19:04:49.955955\n",
      "TP 16441 FP 2611 FN 2888 F 0.8567259842109377 Precision 0.8629540205752677 Recall 0.8505872005794403\n",
      "Epoch 12 start at 2018-05-19 19:09:37.527398\n",
      "Trained at 2018-05-19 19:46:30.295263\n",
      "TP 16861 FP 3002 FN 2468 F 0.8604307001428863 Precision 0.8488647233549816 Recall 0.8723162088054219\n",
      "Epoch 13 start at 2018-05-19 19:51:22.293349\n",
      "Trained at 2018-05-19 20:28:16.493789\n",
      "TP 16762 FP 2942 FN 2567 F 0.8588630133476801 Precision 0.8506902151847341 Recall 0.8671943711521548\n",
      "Epoch 14 start at 2018-05-19 20:33:05.533881\n",
      "Trained at 2018-05-19 21:10:00.823496\n",
      "TP 16552 FP 2723 FN 2777 F 0.8575277173349912 Precision 0.8587289234760052 Recall 0.8563298670391639\n",
      "Epoch 15 start at 2018-05-19 21:14:49.547759\n",
      "Trained at 2018-05-19 21:51:45.879283\n",
      "TP 16569 FP 2835 FN 2760 F 0.855549531407327 Precision 0.8538961038961039 Recall 0.8572093745149775\n",
      "Epoch 16 start at 2018-05-19 21:56:37.075783\n",
      "Trained at 2018-05-19 22:33:32.012902\n",
      "TP 16949 FP 3064 FN 2380 F 0.8616237100299934 Precision 0.8468995153150453 Recall 0.8768689533861038\n",
      "Best so far\n",
      "Epoch 17 start at 2018-05-19 22:38:22.440979\n",
      "Trained at 2018-05-19 23:15:18.718622\n",
      "TP 16529 FP 2652 FN 2800 F 0.8584263827577253 Precision 0.861738178405714 Recall 0.8551399451601222\n",
      "Epoch 18 start at 2018-05-19 23:20:08.650439\n",
      "Trained at 2018-05-19 23:57:05.893139\n",
      "TP 16884 FP 3011 FN 2445 F 0.8609014888843565 Precision 0.8486554410655943 Recall 0.8735061306844638\n",
      "Epoch 19 start at 2018-05-20 00:01:58.265814\n",
      "Trained at 2018-05-20 00:38:53.173807\n",
      "TP 16651 FP 2666 FN 2678 F 0.8617191947420173 Precision 0.861986850960294 Recall 0.861451704692431\n",
      "Best so far\n",
      "Epoch 20 start at 2018-05-20 00:43:43.868170\n",
      "Trained at 2018-05-20 01:20:46.449379\n",
      "TP 16603 FP 2655 FN 2726 F 0.860548889522378 Precision 0.8621352165333888 Recall 0.8589683894666046\n",
      "Epoch 21 start at 2018-05-20 01:25:37.565001\n",
      "Trained at 2018-05-20 02:02:34.261331\n",
      "TP 16802 FP 2968 FN 2527 F 0.8594593212102611 Precision 0.8498735457764289 Recall 0.8692638005070102\n",
      "Epoch 22 start at 2018-05-20 02:07:24.458407\n",
      "Trained at 2018-05-20 02:44:17.754625\n",
      "TP 16561 FP 2782 FN 2768 F 0.8564853123707075 Precision 0.8561753605955643 Recall 0.8567954886440065\n",
      "Epoch 23 start at 2018-05-20 02:49:06.667702\n",
      "Trained at 2018-05-20 03:25:58.617366\n",
      "TP 16585 FP 2825 FN 2744 F 0.8562430625467875 Precision 0.8544564657393097 Recall 0.8580371462569196\n",
      "Epoch 24 start at 2018-05-20 03:30:47.310097\n",
      "Trained at 2018-05-20 04:07:40.010903\n",
      "TP 16886 FP 3070 FN 2443 F 0.8596665393916253 Precision 0.8461615554219283 Recall 0.8736096021522065\n",
      "Epoch 25 start at 2018-05-20 04:12:29.313743\n",
      "Trained at 2018-05-20 04:49:22.717658\n",
      "TP 16692 FP 2836 FN 2637 F 0.8591502174640347 Precision 0.8547726341663253 Recall 0.8635728697811579\n",
      "Epoch 26 start at 2018-05-20 04:54:11.938375\n",
      "Trained at 2018-05-20 05:31:05.307985\n",
      "TP 16937 FP 3117 FN 2392 F 0.8601173094990224 Precision 0.8445696619128353 Recall 0.8762481245796472\n",
      "Epoch 27 start at 2018-05-20 05:35:53.411284\n",
      "Trained at 2018-05-20 06:12:46.661700\n",
      "TP 16682 FP 2829 FN 2647 F 0.8590113285272915 Precision 0.8550048690482293 Recall 0.863055512442444\n",
      "Epoch 28 start at 2018-05-20 06:17:34.463910\n",
      "Trained at 2018-05-20 06:54:27.915141\n",
      "TP 16879 FP 3055 FN 2450 F 0.8597916613605685 Precision 0.8467442560449483 Recall 0.8732474520151068\n",
      "Epoch 29 start at 2018-05-20 06:59:17.610004\n",
      "Trained at 2018-05-20 07:36:10.002075\n",
      "TP 16792 FP 2948 FN 2537 F 0.8596073613350739 Precision 0.8506585612968591 Recall 0.8687464431682963\n"
     ]
    }
   ],
   "source": [
    "from chemlistem import neominimodel_pred\n",
    "nmm = neominimodel_pred.PredNeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patabs_a61k31a61_noct_s.txt\", -1, 16)\n",
    "del(nmm)\n",
    "nmm = neominimodel_pred.PredNeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patabs_a61k31a61_noct_s.txt\", -1, 8)\n",
    "del(nmm)\n",
    "nmm = neominimodel_pred.PredNeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patabs_a61k31a61_noct_s.txt\", -1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-21 09:25:56.294378\n",
      "Shufflesplit at 2018-05-21 09:25:56.294517\n",
      "Read annots at 2018-05-21 09:25:56.352040\n",
      "Read train seqs at 2018-05-21 09:25:56.635854\n",
      "Read test seqs at 2018-05-21 09:26:39.668390\n",
      "Corpus read at 2018-05-21 09:26:52.239613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make train dict at 2018-05-21 09:26:55.756211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-21 09:27:03.598157\n",
      "Trained at 2018-05-21 09:43:32.063362\n",
      "TP 9257 FP 1941 FN 10072 F 0.6064795099420185 Precision 0.8266654759778532 Recall 0.4789176884474106\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-21 09:58:42.187512\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-73beea46e9b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminimodel_cudnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimodel_cudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMiniModelCuDNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/minimodel_cudnn.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# Train in batches of different sizes - randomize the order of sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# Except for the first few epochs - train on the smallest examples first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# For unknown reasons we can't train on a single token (i.e. character)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# as close as possible to original\n",
    "from chemlistem import minimodel_cudnn\n",
    "nmm = minimodel_cudnn.MiniModelCuDNN()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-21 10:05:55.049807\n",
      "Shufflesplit at 2018-05-21 10:05:55.050443\n",
      "Read annots at 2018-05-21 10:05:55.106950\n",
      "Read train seqs at 2018-05-21 10:05:55.395180\n",
      "Read test seqs at 2018-05-21 10:06:38.535296\n",
      "Corpus read at 2018-05-21 10:06:51.108205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make train dict at 2018-05-21 10:06:55.497925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-21 10:07:02.716331\n",
      "Trained at 2018-05-21 10:23:28.617337\n",
      "TP 10199 FP 2483 FN 9130 F 0.6372184561556965 Precision 0.8042106923198233 Recall 0.5276527497542552\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-21 10:28:17.279017\n",
      "Trained at 2018-05-21 10:44:34.857010\n",
      "TP 14138 FP 2582 FN 5191 F 0.7843768204388472 Precision 0.8455741626794259 Recall 0.7314398054736406\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-21 10:49:35.102491\n",
      "Trained at 2018-05-21 11:05:54.572527\n",
      "TP 15171 FP 2541 FN 4158 F 0.8191463513404066 Precision 0.8565379403794038 Recall 0.7848828185627813\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-21 11:10:53.447389\n",
      "Trained at 2018-05-21 11:27:10.997599\n",
      "TP 15267 FP 2210 FN 4062 F 0.8295930011411183 Precision 0.873548091777765 Recall 0.7898494490144343\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-21 11:32:12.075856\n",
      "Trained at 2018-05-21 11:48:30.582754\n",
      "TP 15907 FP 2471 FN 3422 F 0.8437160208979765 Precision 0.8655457612362607 Recall 0.8229603186921206\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-21 11:53:31.313281\n",
      "Trained at 2018-05-21 12:09:47.494199\n",
      "TP 16780 FP 2813 FN 2549 F 0.8622372951030266 Precision 0.8564283162353902 Recall 0.8681256143618398\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-21 12:14:55.217793\n",
      "Trained at 2018-05-21 12:31:12.552157\n",
      "TP 16555 FP 2533 FN 2774 F 0.8618580316005935 Precision 0.8672988264878457 Recall 0.8564850742407781\n",
      "Epoch 7 start at 2018-05-21 12:36:19.236678\n",
      "Trained at 2018-05-21 12:52:36.851815\n",
      "TP 16902 FP 2701 FN 2427 F 0.8682831603822049 Precision 0.862214967096873 Recall 0.8744373738941487\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-21 12:57:43.284203\n",
      "Trained at 2018-05-21 13:14:01.984005\n",
      "TP 16988 FP 2858 FN 2341 F 0.8672878111040204 Precision 0.8559911317141994 Recall 0.8788866470070877\n",
      "Epoch 9 start at 2018-05-21 13:19:09.731709\n",
      "Trained at 2018-05-21 13:35:28.491860\n",
      "TP 16763 FP 2606 FN 2566 F 0.8663496821541165 Precision 0.8654551086788167 Recall 0.8672461068860262\n",
      "Epoch 10 start at 2018-05-21 13:40:33.115945\n",
      "Trained at 2018-05-21 13:56:50.337402\n",
      "TP 16901 FP 2751 FN 2428 F 0.8671404017341782 Precision 0.8600142479136984 Recall 0.8743856381602773\n",
      "Epoch 11 start at 2018-05-21 14:01:55.164758\n",
      "Trained at 2018-05-21 14:18:11.371856\n",
      "TP 16938 FP 2922 FN 2391 F 0.8644262420577202 Precision 0.8528700906344411 Recall 0.8762998603135186\n",
      "Epoch 12 start at 2018-05-21 14:23:16.084175\n",
      "Trained at 2018-05-21 14:39:32.946203\n",
      "TP 17007 FP 2979 FN 2322 F 0.8651659671880961 Precision 0.8509456619633744 Recall 0.8798696259506441\n",
      "Epoch 13 start at 2018-05-21 14:44:40.525342\n",
      "Trained at 2018-05-21 15:00:57.670941\n",
      "TP 17046 FP 3005 FN 2283 F 0.8657186389029965 Precision 0.8501321629843898 Recall 0.8818873195716281\n",
      "Epoch 14 start at 2018-05-21 15:06:05.143307\n",
      "Trained at 2018-05-21 15:22:23.632723\n",
      "TP 17046 FP 3032 FN 2283 F 0.865125485319867 Precision 0.8489889431218249 Recall 0.8818873195716281\n",
      "Epoch 15 start at 2018-05-21 15:27:30.861745\n",
      "Trained at 2018-05-21 15:43:49.463112\n",
      "TP 17097 FP 3152 FN 2232 F 0.8639648289453737 Precision 0.8443379919996049 Recall 0.8845258419990688\n",
      "Epoch 16 start at 2018-05-21 15:48:56.144470\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-73beea46e9b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminimodel_cudnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimodel_cudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMiniModelCuDNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/minimodel_cudnn.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mcr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCorpusReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharbychar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainseqs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestseqs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mseqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# as close as possible to original\n",
    "from chemlistem import minimodel_cudnn\n",
    "nmm = minimodel_cudnn.MiniModelCuDNN()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-21 15:59:20.117012\n",
      "Shufflesplit at 2018-05-21 15:59:20.117475\n",
      "Read annots at 2018-05-21 15:59:20.173500\n",
      "Read train seqs at 2018-05-21 15:59:20.455357\n",
      "Read test seqs at 2018-05-21 16:00:03.534528\n",
      "Corpus read at 2018-05-21 16:00:16.102921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make train dict at 2018-05-21 16:00:20.245307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "3931 batches of unsupervised\n",
      "0 9.02071 [9.02071, 4.510255, 4.510455] 2018-05-21 16:00:30.936656\n",
      "100 6.1942406 [5.5955296, 2.794262, 2.8012679] 2018-05-21 16:00:53.804358\n",
      "200 5.1946235 [5.1503673, 2.5763068, 2.5740604] 2018-05-21 16:01:14.810998\n",
      "300 4.642289 [4.6502113, 2.2962866, 2.3539248] 2018-05-21 16:01:35.869589\n",
      "400 4.229396 [3.942741, 1.9623297, 1.9804113] 2018-05-21 16:02:00.064548\n",
      "500 3.8336937 [3.5493011, 1.7590156, 1.7902856] 2018-05-21 16:02:17.113737\n",
      "600 3.5686715 [3.4237747, 1.6971767, 1.726598] 2018-05-21 16:02:37.347149\n",
      "700 3.3618226 [3.1207294, 1.553941, 1.5667883] 2018-05-21 16:02:57.948462\n",
      "800 3.2721517 [3.223525, 1.5796206, 1.6439044] 2018-05-21 16:03:18.644045\n",
      "900 3.1227083 [2.8173327, 1.3967215, 1.4206113] 2018-05-21 16:03:39.019121\n",
      "1000 2.980715 [2.8870368, 1.4143033, 1.4727336] 2018-05-21 16:04:02.050688\n",
      "1100 2.8789022 [2.9140906, 1.4328399, 1.4812508] 2018-05-21 16:04:21.160167\n",
      "1200 2.8531165 [2.4959507, 1.2391124, 1.2568383] 2018-05-21 16:04:45.038997\n",
      "1300 2.7962847 [3.0041018, 1.4802649, 1.523837] 2018-05-21 16:05:11.293446\n",
      "1400 2.669399 [2.736141, 1.3432503, 1.3928907] 2018-05-21 16:05:29.379161\n",
      "1500 2.6596231 [2.4616854, 1.2136166, 1.2480688] 2018-05-21 16:05:49.428526\n",
      "1600 2.6673782 [2.080923, 1.0248005, 1.0561225] 2018-05-21 16:06:17.515598\n",
      "1700 2.5566554 [2.684957, 1.3264999, 1.3584571] 2018-05-21 16:06:40.901835\n",
      "1800 2.5449648 [2.3486586, 1.156795, 1.1918635] 2018-05-21 16:07:05.395575\n",
      "1900 2.5160894 [2.2445374, 1.10552, 1.1390175] 2018-05-21 16:07:26.439215\n",
      "2000 2.4489944 [2.5279472, 1.224663, 1.3032842] 2018-05-21 16:07:46.425399\n",
      "2100 2.4544048 [2.451334, 1.2057161, 1.2456177] 2018-05-21 16:08:10.437439\n",
      "2200 2.4201696 [2.56153, 1.2540174, 1.3075128] 2018-05-21 16:08:32.423065\n",
      "2300 2.3463686 [2.6224022, 1.2944236, 1.3279785] 2018-05-21 16:08:56.397526\n",
      "2400 2.385536 [2.1239204, 1.0524778, 1.0714425] 2018-05-21 16:09:16.441099\n",
      "2500 2.3864563 [3.0692997, 1.5201455, 1.549154] 2018-05-21 16:09:41.360628\n",
      "2600 2.3073046 [2.2762618, 1.1125664, 1.1636955] 2018-05-21 16:10:01.735628\n",
      "2700 2.32943 [2.099924, 1.0236497, 1.0762745] 2018-05-21 16:10:27.119550\n",
      "2800 2.3083205 [2.0992153, 1.0307043, 1.068511] 2018-05-21 16:10:53.635134\n",
      "2900 2.2558131 [2.3196907, 1.1490004, 1.1706903] 2018-05-21 16:11:15.826223\n",
      "3000 2.301143 [2.0795295, 1.0260797, 1.0534499] 2018-05-21 16:11:42.091592\n",
      "3100 2.2299552 [2.109429, 1.0322778, 1.0771511] 2018-05-21 16:12:05.514009\n",
      "3200 2.2503555 [2.3357275, 1.1552522, 1.1804752] 2018-05-21 16:12:32.680638\n",
      "3300 2.1935198 [1.8539581, 0.91056526, 0.94339293] 2018-05-21 16:12:55.765015\n",
      "3400 2.2094374 [1.8739257, 0.91670597, 0.9572198] 2018-05-21 16:13:19.580696\n",
      "3500 2.2305055 [2.8936596, 1.4490943, 1.4445653] 2018-05-21 16:13:45.154779\n",
      "3600 2.230355 [1.7461805, 0.85971296, 0.88646764] 2018-05-21 16:14:12.146554\n",
      "3700 2.2138784 [1.9803795, 0.96113575, 1.0192437] 2018-05-21 16:14:39.214384\n",
      "3800 2.1031253 [1.9572189, 0.98195547, 0.9752635] 2018-05-21 16:14:58.345843\n",
      "3900 2.1475732 [2.4155672, 1.1796443, 1.2359228] 2018-05-21 16:15:23.047625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-21 16:15:29.474884\n",
      "Trained at 2018-05-21 16:31:52.820962\n",
      "TP 13581 FP 1981 FN 5748 F 0.778481556848471 Precision 0.8727027374373474 Recall 0.7026230017072792\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-21 16:36:56.805705\n",
      "Trained at 2018-05-21 16:53:15.914292\n",
      "TP 15374 FP 2388 FN 3955 F 0.8289881642446955 Precision 0.8655556806665916 Recall 0.7953851725386725\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-21 16:58:25.382121\n",
      "Trained at 2018-05-21 17:14:46.042036\n",
      "TP 15997 FP 2351 FN 3332 F 0.8491652732436235 Precision 0.8718661434488773 Recall 0.8276165347405453\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-21 17:20:00.676637\n",
      "Trained at 2018-05-21 17:36:21.162585\n",
      "TP 16601 FP 2573 FN 2728 F 0.862322416435083 Precision 0.8658078648169396 Recall 0.8588649179988618\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-21 17:41:36.596271\n",
      "Trained at 2018-05-21 17:57:55.224689\n",
      "TP 16845 FP 2795 FN 2484 F 0.8645333470194257 Precision 0.8576883910386965 Recall 0.8714884370634798\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-21 18:03:12.250968\n",
      "Trained at 2018-05-21 18:19:30.219587\n",
      "TP 16561 FP 2588 FN 2768 F 0.8608035760694422 Precision 0.8648493393910909 Recall 0.8567954886440065\n",
      "Epoch 6 start at 2018-05-21 18:24:55.644003\n",
      "Trained at 2018-05-21 18:41:15.931464\n",
      "TP 17279 FP 3164 FN 2050 F 0.8689027456502062 Precision 0.8452281954703321 Recall 0.8939417455636608\n",
      "Best so far\n",
      "Epoch 7 start at 2018-05-21 18:46:37.314717\n",
      "Trained at 2018-05-21 19:02:58.094050\n",
      "TP 17092 FP 2884 FN 2237 F 0.8697112326675995 Precision 0.855626752102523 Recall 0.8842671633297118\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-21 19:08:17.949861\n",
      "Trained at 2018-05-21 19:24:37.516131\n",
      "TP 17318 FP 3117 FN 2011 F 0.8710391308721457 Precision 0.8474675801321263 Recall 0.8959594391846448\n",
      "Best so far\n",
      "Epoch 9 start at 2018-05-21 19:29:55.940492\n",
      "Trained at 2018-05-21 19:46:15.914163\n",
      "TP 17311 FP 3295 FN 2018 F 0.8669588080631025 Precision 0.8400951179268175 Recall 0.8955972890475451\n",
      "Epoch 10 start at 2018-05-21 19:51:36.561376\n",
      "Trained at 2018-05-21 20:07:56.728327\n",
      "TP 17184 FP 3079 FN 2145 F 0.868054152354011 Precision 0.8480481666090904 Recall 0.8890268508458793\n",
      "Epoch 11 start at 2018-05-21 20:13:13.706016\n",
      "Trained at 2018-05-21 20:29:34.153064\n",
      "TP 17164 FP 3008 FN 2165 F 0.8690412900939217 Precision 0.850882411263137 Recall 0.8879921361684515\n",
      "Epoch 12 start at 2018-05-21 20:34:52.825651\n",
      "Trained at 2018-05-21 20:51:08.897896\n",
      "TP 17038 FP 2779 FN 2291 F 0.8704848515812599 Precision 0.8597668668315083 Recall 0.881473433700657\n",
      "Epoch 13 start at 2018-05-21 20:56:25.350988\n",
      "Trained at 2018-05-21 21:12:40.345475\n",
      "TP 16991 FP 2819 FN 2338 F 0.8682388410536804 Precision 0.8576981322564361 Recall 0.879041854208702\n",
      "Epoch 14 start at 2018-05-21 21:17:57.392370\n",
      "Trained at 2018-05-21 21:34:13.024588\n",
      "TP 17182 FP 3030 FN 2147 F 0.8690726081788523 Precision 0.8500890560063329 Recall 0.8889233793781365\n",
      "Epoch 15 start at 2018-05-21 21:39:31.266208\n",
      "Trained at 2018-05-21 21:55:46.921220\n",
      "TP 17099 FP 2878 FN 2230 F 0.8700452857070168 Precision 0.8559343244731441 Recall 0.8846293134668115\n",
      "Epoch 16 start at 2018-05-21 22:01:03.073457\n",
      "Trained at 2018-05-21 22:17:18.103289\n",
      "TP 16990 FP 2934 FN 2339 F 0.8656663184979492 Precision 0.8527404135715719 Recall 0.8789901184748306\n",
      "Epoch 17 start at 2018-05-21 22:22:35.638740\n",
      "Trained at 2018-05-21 22:38:51.623524\n",
      "TP 17019 FP 2906 FN 2310 F 0.8671218219799256 Precision 0.8541530740276035 Recall 0.8804904547571007\n",
      "Epoch 18 start at 2018-05-21 22:44:08.264784\n",
      "Trained at 2018-05-21 23:00:23.145992\n",
      "TP 16856 FP 2684 FN 2473 F 0.867323574056446 Precision 0.8626407369498464 Recall 0.872057530136065\n",
      "Epoch 19 start at 2018-05-21 23:05:39.953434\n",
      "Trained at 2018-05-21 23:21:55.923002\n",
      "TP 17117 FP 2956 FN 2212 F 0.8688391452210548 Precision 0.8527375080954516 Recall 0.8855605566764965\n",
      "Epoch 20 start at 2018-05-21 23:27:10.618742\n",
      "Trained at 2018-05-21 23:43:26.258011\n",
      "TP 17204 FP 3240 FN 2125 F 0.8651094963920247 Precision 0.8415182938759538 Recall 0.890061565523307\n",
      "Epoch 21 start at 2018-05-21 23:48:44.484591\n",
      "Trained at 2018-05-22 00:04:59.377316\n",
      "TP 17296 FP 3276 FN 2033 F 0.8669456905841959 Precision 0.8407544234882365 Recall 0.8948212530394744\n",
      "Epoch 22 start at 2018-05-22 00:10:18.280905\n",
      "Trained at 2018-05-22 00:26:31.641425\n",
      "TP 16910 FP 2665 FN 2419 F 0.8693193501953527 Precision 0.8638569604086845 Recall 0.8748512597651198\n",
      "Epoch 23 start at 2018-05-22 00:31:47.266528\n",
      "Trained at 2018-05-22 00:48:01.479749\n",
      "TP 17100 FP 3010 FN 2229 F 0.8671619462968128 Precision 0.8503232222774739 Recall 0.8846810492006829\n",
      "Epoch 24 start at 2018-05-22 00:53:16.210705\n",
      "Trained at 2018-05-22 01:09:30.518187\n",
      "TP 17201 FP 3095 FN 2128 F 0.8681892744479496 Precision 0.8475068979109184 Recall 0.8899063583216928\n",
      "Epoch 25 start at 2018-05-22 01:14:50.720947\n",
      "Trained at 2018-05-22 01:31:06.857467\n",
      "TP 16785 FP 2683 FN 2544 F 0.8652730881253705 Precision 0.8621840969796589 Recall 0.8683842930311967\n",
      "Epoch 26 start at 2018-05-22 01:36:21.396999\n",
      "Trained at 2018-05-22 01:52:36.196260\n",
      "TP 17154 FP 3306 FN 2175 F 0.862248360099525 Precision 0.83841642228739 Recall 0.8874747788297377\n",
      "Epoch 27 start at 2018-05-22 01:57:56.035686\n",
      "Trained at 2018-05-22 02:14:11.140835\n",
      "TP 16866 FP 2763 FN 2463 F 0.8658555367318651 Precision 0.8592388812471343 Recall 0.8725748874747788\n",
      "Epoch 28 start at 2018-05-22 02:19:28.090592\n",
      "Trained at 2018-05-22 02:35:43.003706\n",
      "TP 17167 FP 3365 FN 2162 F 0.8613431675070872 Precision 0.8361094876290668 Recall 0.8881473433700657\n",
      "Epoch 29 start at 2018-05-22 02:40:58.706201\n",
      "Trained at 2018-05-22 02:57:12.781987\n",
      "TP 16982 FP 2941 FN 2347 F 0.8652807500254764 Precision 0.8523816694272951 Recall 0.8785762326038595\n"
     ]
    }
   ],
   "source": [
    "# as close as possible to original plus unsup\n",
    "from chemlistem import minimodel_cudnn\n",
    "nmm = minimodel_cudnn.MiniModelCuDNN()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patabs_a61k31a61_noct_s.txt\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-22 09:34:08.000927\n",
      "Shufflesplit at 2018-05-22 09:34:08.001044\n",
      "Read annots at 2018-05-22 09:34:08.069080\n",
      "Read train seqs at 2018-05-22 09:34:08.355697\n",
      "Read test seqs at 2018-05-22 09:34:51.660609\n",
      "Corpus read at 2018-05-22 09:35:04.328418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-22 09:35:08.633257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-22 09:35:18.198353\n",
      "Trained at 2018-05-22 09:41:59.874745\n",
      "TP 8322 FP 3551 FN 11007 F 0.5334273444009999 Precision 0.7009180493556809 Recall 0.43054477727766566\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-22 09:44:22.778275\n",
      "Trained at 2018-05-22 09:50:57.633098\n",
      "TP 13595 FP 3324 FN 5734 F 0.7501103509159126 Precision 0.803534487853892 Recall 0.7033473019814787\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-22 09:53:12.302580\n",
      "Trained at 2018-05-22 09:59:42.869895\n",
      "TP 14737 FP 3344 FN 4592 F 0.7878642074311681 Precision 0.815054477075383 Recall 0.7624295100626003\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-22 10:01:58.016925\n",
      "Trained at 2018-05-22 10:08:30.405425\n",
      "TP 15659 FP 3169 FN 3670 F 0.8207668317739865 Precision 0.8316868493732739 Recall 0.8101298566920172\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-22 10:10:45.169621\n",
      "Trained at 2018-05-22 10:17:16.522082\n",
      "TP 16035 FP 3224 FN 3294 F 0.8310873846791749 Precision 0.8325977465081261 Recall 0.8295824926276579\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-22 10:19:29.473413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Dict 2018-05-22 10:23:26.137909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-22 10:25:42.635854\n",
      "TP 16505 FP 3248 FN 2824 F 0.8446343585282227 Precision 0.8355692806156026 Recall 0.8538982875472089\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-22 10:27:56.845161\n",
      "Trained at 2018-05-22 10:33:38.416051\n",
      "TP 16686 FP 3156 FN 2643 F 0.8519568047790457 Precision 0.8409434532809192 Recall 0.8632624553779296\n",
      "Best so far\n",
      "Epoch 7 start at 2018-05-22 10:35:53.153809\n",
      "Trained at 2018-05-22 10:41:34.887343\n",
      "TP 16590 FP 2949 FN 2739 F 0.8536585365853658 Precision 0.8490710885920467 Recall 0.8582958249262765\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-22 10:43:47.779557\n",
      "Trained at 2018-05-22 10:49:29.124993\n",
      "TP 16773 FP 3098 FN 2556 F 0.8557653061224489 Precision 0.8440944089376479 Recall 0.86776346422474\n",
      "Best so far\n",
      "Epoch 9 start at 2018-05-22 10:51:40.538606\n",
      "Trained at 2018-05-22 10:57:22.196385\n",
      "TP 16680 FP 2903 FN 2649 F 0.8573190789473685 Precision 0.8517591788796405 Recall 0.8629520409747012\n",
      "Best so far\n",
      "Epoch 10 start at 2018-05-22 10:59:34.276338\n",
      "Trained at 2018-05-22 11:05:15.852152\n",
      "TP 16839 FP 3059 FN 2490 F 0.858541310831825 Precision 0.8462659563775253 Recall 0.8711780226602515\n",
      "Best so far\n",
      "Epoch 11 start at 2018-05-22 11:07:30.289202\n",
      "Trained at 2018-05-22 11:13:11.850427\n",
      "TP 16764 FP 2935 FN 2565 F 0.859075535512965 Precision 0.8510076653637241 Recall 0.8672978426198976\n",
      "Best so far\n",
      "Epoch 12 start at 2018-05-22 11:15:26.514831\n",
      "Trained at 2018-05-22 11:21:07.466644\n",
      "TP 16721 FP 2956 FN 2608 F 0.8573552786750757 Precision 0.8497738476393759 Recall 0.865073206063428\n",
      "Epoch 13 start at 2018-05-22 11:23:22.000710\n",
      "Trained at 2018-05-22 11:29:03.284059\n",
      "TP 16766 FP 2983 FN 2563 F 0.8580787143661395 Precision 0.8489543774368322 Recall 0.8674013140876403\n",
      "Epoch 14 start at 2018-05-22 11:31:15.816851\n",
      "Trained at 2018-05-22 11:36:57.156005\n",
      "TP 16613 FP 2901 FN 2716 F 0.8553922199624128 Precision 0.8513375012811315 Recall 0.8594857468053184\n",
      "Epoch 15 start at 2018-05-22 11:39:08.479978\n",
      "Trained at 2018-05-22 11:44:51.371656\n",
      "TP 16806 FP 2891 FN 2523 F 0.8612719725311331 Precision 0.8532263796517237 Recall 0.8694707434424958\n",
      "Best so far\n",
      "Epoch 16 start at 2018-05-22 11:47:03.558776\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-49ee8e7accc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneominimodel_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneominimodel_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPredNeoMiniModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../catted_patful_a61k31a61_noct_s.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel_pred.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname, unsupfile, nunsup, bsize)\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0;31m#model.fit(tx, ty, verbose=0, epochs=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#interleave dict training\n",
    "from chemlistem import neominimodel_pred\n",
    "nmm = neominimodel_pred.PredNeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patful_a61k31a61_noct_s.txt\", 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-22 11:49:36.244923\n",
      "Shufflesplit at 2018-05-22 11:49:36.245383\n",
      "Read annots at 2018-05-22 11:49:36.302044\n",
      "Read train seqs at 2018-05-22 11:49:36.583000\n",
      "Read test seqs at 2018-05-22 11:50:19.652356\n",
      "Corpus read at 2018-05-22 11:50:32.190467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-22 11:50:36.459845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-22 11:50:46.175730\n",
      "Trained at 2018-05-22 11:57:08.466279\n",
      "TP 9002 FP 3276 FN 10327 F 0.5696206536526719 Precision 0.7331812998859749 Recall 0.4657250763102075\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-22 11:59:21.169423\n",
      "Trained at 2018-05-22 12:05:36.112127\n",
      "TP 13439 FP 3004 FN 5890 F 0.751369786425137 Precision 0.817308277078392 Recall 0.6952765274975425\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-22 12:07:47.174487\n",
      "Trained at 2018-05-22 12:13:59.232947\n",
      "TP 14968 FP 3155 FN 4361 F 0.7993164584000855 Precision 0.8259118247530762 Recall 0.7743804645868901\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-22 12:16:10.916384\n",
      "Trained at 2018-05-22 12:22:24.162368\n",
      "TP 15596 FP 3111 FN 3733 F 0.8200652013881586 Precision 0.8336986154915272 Recall 0.8068705054581199\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-22 12:24:35.182465\n",
      "Trained at 2018-05-22 12:30:48.155181\n",
      "TP 15915 FP 2994 FN 3414 F 0.8324180134944297 Precision 0.8416627003014437 Recall 0.8233742045630917\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-22 12:33:00.647958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Dict 2018-05-22 12:36:46.735742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-22 12:39:02.930524\n",
      "TP 16351 FP 3078 FN 2978 F 0.8437483874296919 Precision 0.8415770240362345 Recall 0.8459309845310156\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-22 12:41:12.871328\n",
      "Trained at 2018-05-22 12:46:54.634252\n",
      "TP 16524 FP 3123 FN 2805 F 0.8479064039408867 Precision 0.8410444342647733 Recall 0.8548812664907651\n",
      "Best so far\n",
      "Epoch 7 start at 2018-05-22 12:49:02.775992\n",
      "Trained at 2018-05-22 12:54:44.929948\n",
      "TP 16532 FP 3046 FN 2797 F 0.8498213689053383 Precision 0.84441720298294 Recall 0.8552951523617363\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-22 12:56:53.475992\n",
      "Trained at 2018-05-22 13:02:34.755849\n",
      "TP 16668 FP 3143 FN 2661 F 0.851711803781298 Precision 0.841350764726667 Recall 0.8623312121682446\n",
      "Best so far\n",
      "Epoch 9 start at 2018-05-22 13:04:43.718606\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b716d54f6d9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneominimodel_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneominimodel_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPredNeoMiniModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../catted_patful_a61k31a61_noct_s.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel_pred.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname, unsupfile, nunsup, bsize)\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0;31m#model.fit(tx, ty, verbose=0, epochs=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#interleave dict training, 2 layers not 3\n",
    "from chemlistem import neominimodel_pred\n",
    "nmm = neominimodel_pred.PredNeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patful_a61k31a61_noct_s.txt\", 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-22 13:06:01.372276\n",
      "Shufflesplit at 2018-05-22 13:06:01.372373\n",
      "Read annots at 2018-05-22 13:06:01.441093\n",
      "Read train seqs at 2018-05-22 13:06:02.405780\n",
      "Read test seqs at 2018-05-22 13:06:47.551308\n",
      "Corpus read at 2018-05-22 13:07:00.413645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-22 13:07:04.064470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3931 batches of unsupervised\n",
      "0 9.020487 [9.020487, 4.5100045, 4.5104823] 2018-05-22 13:07:09.104638\n",
      "100 6.142597 [5.4037523, 2.7019982, 2.701754] 2018-05-22 13:07:34.190134\n",
      "200 5.074463 [4.5422564, 2.2563193, 2.2859373] 2018-05-22 13:07:53.573326\n",
      "300 4.5298476 [4.268976, 2.132359, 2.1366172] 2018-05-22 13:08:15.112313\n",
      "400 4.126973 [4.266499, 2.125958, 2.140541] 2018-05-22 13:08:38.556914\n",
      "500 3.8553457 [3.7365587, 1.8593581, 1.8772006] 2018-05-22 13:09:06.371312\n",
      "600 3.6227722 [3.6178665, 1.8097855, 1.808081] 2018-05-22 13:09:28.134148\n",
      "700 3.4229429 [2.757225, 1.3904097, 1.3668153] 2018-05-22 13:09:54.897910\n",
      "800 3.2262704 [3.306056, 1.6400831, 1.665973] 2018-05-22 13:10:15.762841\n",
      "900 3.04357 [3.0154343, 1.4992502, 1.5161841] 2018-05-22 13:10:34.920482\n",
      "1000 2.9081194 [2.798739, 1.3760691, 1.4226699] 2018-05-22 13:10:52.862462\n",
      "1100 2.8663511 [2.6914983, 1.3262482, 1.36525] 2018-05-22 13:11:13.294084\n",
      "1200 2.8506951 [2.9951482, 1.4826844, 1.5124638] 2018-05-22 13:11:36.965208\n",
      "1300 2.782936 [2.4129033, 1.204059, 1.2088444] 2018-05-22 13:11:58.109761\n",
      "1400 2.6777081 [2.624288, 1.3024305, 1.3218577] 2018-05-22 13:12:17.696186\n",
      "1500 2.6825352 [3.2827473, 1.6358129, 1.6469343] 2018-05-22 13:12:40.196867\n",
      "1600 2.5421083 [2.5109913, 1.2561653, 1.2548261] 2018-05-22 13:13:02.147652\n",
      "1700 2.5715127 [1.9555557, 0.9624354, 0.9931203] 2018-05-22 13:13:25.680687\n",
      "1800 2.5288196 [2.272943, 1.1247121, 1.1482309] 2018-05-22 13:13:48.799784\n",
      "1900 2.4395292 [2.3639479, 1.1782424, 1.1857054] 2018-05-22 13:14:07.556865\n",
      "2000 2.477991 [2.52676, 1.2533052, 1.2734548] 2018-05-22 13:14:30.962108\n",
      "2100 2.4678092 [2.8615344, 1.4277108, 1.4338236] 2018-05-22 13:14:57.245151\n",
      "2200 2.404409 [2.1701384, 1.0767478, 1.0933905] 2018-05-22 13:15:19.265474\n",
      "2300 2.3540995 [2.1135268, 1.0346446, 1.0788823] 2018-05-22 13:15:43.140599\n",
      "2400 2.3163993 [2.2923083, 1.1275859, 1.1647224] 2018-05-22 13:16:03.635683\n",
      "2500 2.301277 [2.2208107, 1.1041467, 1.1166639] 2018-05-22 13:16:22.188123\n",
      "2600 2.2944367 [2.3630025, 1.1765213, 1.1864812] 2018-05-22 13:16:42.099926\n",
      "2700 2.318138 [2.347316, 1.1688616, 1.1784544] 2018-05-22 13:17:04.483093\n",
      "2800 2.3424811 [2.1146533, 1.0542791, 1.0603743] 2018-05-22 13:17:29.392996\n",
      "2900 2.293146 [1.652893, 0.81020534, 0.8426876] 2018-05-22 13:17:54.343822\n",
      "3000 2.2500305 [2.3561826, 1.1680001, 1.1881826] 2018-05-22 13:18:17.401448\n",
      "3100 2.2293122 [2.1311855, 1.0605466, 1.0706389] 2018-05-22 13:18:39.779122\n",
      "3200 2.2362056 [2.0708508, 1.030378, 1.0404727] 2018-05-22 13:19:05.683934\n",
      "3300 2.187169 [2.2659564, 1.1246037, 1.1413528] 2018-05-22 13:19:30.068138\n",
      "3400 2.2099998 [2.4056401, 1.2007735, 1.2048668] 2018-05-22 13:19:55.446303\n",
      "3500 2.1474533 [2.3095179, 1.1403874, 1.1691304] 2018-05-22 13:20:13.978348\n",
      "3600 2.154384 [2.5582626, 1.2735684, 1.2846942] 2018-05-22 13:20:36.553572\n",
      "3700 2.189254 [1.9903502, 0.98800766, 1.0023426] 2018-05-22 13:21:01.416502\n",
      "3800 2.1918654 [2.495543, 1.232191, 1.2633522] 2018-05-22 13:21:27.189126\n",
      "3900 2.173137 [2.694297, 1.3404989, 1.3537982] 2018-05-22 13:21:50.595840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-22 13:22:00.608838\n",
      "Trained at 2018-05-22 13:28:24.273639\n",
      "TP 14221 FP 3243 FN 5108 F 0.7730274780528905 Precision 0.8143037104901512 Recall 0.7357338713849656\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-22 13:30:36.899343\n",
      "Trained at 2018-05-22 13:36:56.801192\n",
      "TP 15488 FP 3190 FN 3841 F 0.8150077617281027 Precision 0.8292108362779741 Recall 0.8012830462000103\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-22 13:39:08.836104\n",
      "Trained at 2018-05-22 13:45:23.353449\n",
      "TP 16329 FP 3365 FN 3000 F 0.8368910642441637 Precision 0.8291357773941302 Recall 0.8447927983858451\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-22 13:47:32.985167\n",
      "Trained at 2018-05-22 13:53:48.711319\n",
      "TP 16415 FP 3301 FN 2914 F 0.8408246894608785 Precision 0.832572529924934 Recall 0.8492420714987842\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-22 13:55:59.561327\n",
      "Trained at 2018-05-22 14:02:16.033988\n",
      "TP 16584 FP 3201 FN 2745 F 0.847982819450836 Precision 0.8382107657316149 Recall 0.8579854105230482\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-22 14:04:26.127579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Dict 2018-05-22 14:08:12.292274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-22 14:10:28.235928\n",
      "TP 16690 FP 2923 FN 2639 F 0.8571722048174207 Precision 0.8509661958904808 Recall 0.8634693983134151\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-22 14:12:37.150167\n",
      "Trained at 2018-05-22 14:18:20.513083\n",
      "TP 16701 FP 2959 FN 2628 F 0.856703172689733 Precision 0.8494913530010173 Recall 0.8640384913860003\n",
      "Epoch 7 start at 2018-05-22 14:20:29.713038\n",
      "Trained at 2018-05-22 14:26:13.662292\n",
      "TP 16924 FP 3044 FN 2405 F 0.8613380156246024 Precision 0.8475560897435898 Recall 0.8755755600393191\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-22 14:28:21.113032\n",
      "Trained at 2018-05-22 14:34:05.445617\n",
      "TP 17001 FP 3028 FN 2328 F 0.8639158493825906 Precision 0.8488192121423935 Recall 0.8795592115474158\n",
      "Best so far\n",
      "Epoch 9 start at 2018-05-22 14:36:13.706081\n",
      "Trained at 2018-05-22 14:41:58.083259\n",
      "TP 16755 FP 2751 FN 2574 F 0.8628814213982232 Precision 0.8589664718548139 Recall 0.8668322210150551\n",
      "Epoch 10 start at 2018-05-22 14:44:08.247847\n",
      "Trained at 2018-05-22 14:49:53.963824\n",
      "TP 16932 FP 3051 FN 2397 F 0.8614163614163615 Precision 0.8473202221888605 Recall 0.8759894459102903\n",
      "Epoch 11 start at 2018-05-22 14:52:04.196003\n",
      "Trained at 2018-05-22 14:57:46.762496\n",
      "TP 16917 FP 2971 FN 2412 F 0.8627380982737078 Precision 0.850613435237329 Recall 0.8752134099022194\n",
      "Epoch 12 start at 2018-05-22 14:59:55.164386\n",
      "Trained at 2018-05-22 15:05:39.168521\n",
      "TP 16855 FP 2974 FN 2474 F 0.8608713417437049 Precision 0.850017650915326 Recall 0.8720057944021936\n",
      "Epoch 13 start at 2018-05-22 15:07:48.750453\n",
      "Trained at 2018-05-22 15:13:32.709783\n",
      "TP 16937 FP 2992 FN 2392 F 0.8628559783993072 Precision 0.8498670279492198 Recall 0.8762481245796472\n",
      "Epoch 14 start at 2018-05-22 15:15:43.530917\n",
      "Trained at 2018-05-22 15:21:30.598623\n",
      "TP 16875 FP 2985 FN 2454 F 0.8612110541223302 Precision 0.8496978851963746 Recall 0.8730405090796213\n",
      "Epoch 15 start at 2018-05-22 15:23:40.127992\n",
      "Trained at 2018-05-22 15:29:25.845677\n",
      "TP 16825 FP 2987 FN 2504 F 0.8597123221174727 Precision 0.8492327882091661 Recall 0.870453722386052\n",
      "Epoch 16 start at 2018-05-22 15:31:35.954922\n",
      "Trained at 2018-05-22 15:37:21.067567\n",
      "TP 16908 FP 3096 FN 2421 F 0.85973609945847 Precision 0.8452309538092382 Recall 0.874747788297377\n",
      "Epoch 17 start at 2018-05-22 15:39:30.673313\n",
      "Trained at 2018-05-22 15:45:15.374080\n",
      "TP 16924 FP 3084 FN 2405 F 0.86046216030709 Precision 0.8458616553378648 Recall 0.8755755600393191\n",
      "Epoch 18 start at 2018-05-22 15:47:24.485399\n",
      "Trained at 2018-05-22 15:53:09.401095\n",
      "TP 17120 FP 3304 FN 2209 F 0.8613186426181672 Precision 0.8382295338817078 Recall 0.8857157638781106\n",
      "Epoch 19 start at 2018-05-22 15:55:19.072670\n",
      "Trained at 2018-05-22 16:01:04.113318\n",
      "TP 17009 FP 3134 FN 2320 F 0.861826104580462 Precision 0.844412450975525 Recall 0.8799730974183869\n",
      "Epoch 20 start at 2018-05-22 16:03:13.832980\n",
      "Trained at 2018-05-22 16:08:59.260633\n",
      "TP 16940 FP 2984 FN 2389 F 0.8631187425164956 Precision 0.8502308773338687 Recall 0.8764033317812613\n",
      "Epoch 21 start at 2018-05-22 16:11:08.010072\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f7cdbaaaf8ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneominimodel_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneominimodel_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPredNeoMiniModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../catted_patabs_a61k31a61_noct_s.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel_pred.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname, unsupfile, nunsup, bsize)\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0;31m#model.fit(tx, ty, verbose=0, epochs=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#interleave dict training, 2 layers not 3, with pretraining\n",
    "from chemlistem import neominimodel_pred\n",
    "nmm = neominimodel_pred.PredNeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patabs_a61k31a61_noct_s.txt\", -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-22 16:20:17.365936\n",
      "Shufflesplit at 2018-05-22 16:20:17.366073\n",
      "Read annots at 2018-05-22 16:20:17.422646\n",
      "Read train seqs at 2018-05-22 16:20:17.709195\n",
      "Read test seqs at 2018-05-22 16:21:00.589796\n",
      "Corpus read at 2018-05-22 16:21:13.240842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make train dict at 2018-05-22 16:21:17.458383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "3931 batches of unsupervised\n",
      "0 9.0231085 [9.0231085, 4.510953, 4.5121555] 2018-05-22 16:21:29.547738\n",
      "100 6.152367 [5.3467846, 2.6366653, 2.710119] 2018-05-22 16:21:48.165874\n",
      "200 5.1077924 [4.708867, 2.3460615, 2.3628058] 2018-05-22 16:22:05.469078\n",
      "300 4.577441 [3.9949565, 1.9800922, 2.0148644] 2018-05-22 16:22:27.220693\n",
      "400 4.1441584 [4.387436, 2.1898632, 2.1975727] 2018-05-22 16:22:50.396756\n",
      "500 3.8209174 [4.1143117, 2.0522037, 2.062108] 2018-05-22 16:23:16.045420\n",
      "600 3.5147185 [3.518558, 1.7504787, 1.7680794] 2018-05-22 16:23:36.477485\n",
      "700 3.3370771 [3.6434183, 1.8199847, 1.8234338] 2018-05-22 16:23:58.748567\n",
      "800 3.22397 [2.9773617, 1.4395714, 1.5377904] 2018-05-22 16:24:24.570244\n",
      "900 3.1013868 [2.8203094, 1.4060197, 1.4142897] 2018-05-22 16:24:50.722450\n",
      "1000 2.979358 [2.7534926, 1.3592689, 1.3942237] 2018-05-22 16:25:13.964372\n",
      "1100 2.868648 [3.5164473, 1.7445155, 1.7719318] 2018-05-22 16:25:36.522480\n",
      "1200 2.8188248 [3.510357, 1.748051, 1.762306] 2018-05-22 16:25:59.368132\n",
      "1300 2.7623916 [2.261692, 1.0986146, 1.1630775] 2018-05-22 16:26:25.289262\n",
      "1400 2.7108078 [2.6656303, 1.3227421, 1.3428881] 2018-05-22 16:26:45.822025\n",
      "1500 2.6276977 [2.4804409, 1.2222028, 1.2582381] 2018-05-22 16:27:06.738156\n",
      "1600 2.6292057 [3.0572555, 1.5215108, 1.5357447] 2018-05-22 16:27:32.644770\n",
      "1700 2.541271 [2.918903, 1.4522009, 1.4667022] 2018-05-22 16:27:51.895411\n",
      "1800 2.512988 [2.2138588, 1.0880201, 1.1258388] 2018-05-22 16:28:12.155092\n",
      "1900 2.4827583 [3.231758, 1.6122526, 1.6195055] 2018-05-22 16:28:34.832858\n",
      "2000 2.4527311 [1.8919821, 0.91536593, 0.97661614] 2018-05-22 16:28:58.055465\n",
      "2100 2.4216921 [2.4553428, 1.2180177, 1.2373252] 2018-05-22 16:29:20.202588\n",
      "2200 2.3350806 [3.1457357, 1.5661044, 1.5796314] 2018-05-22 16:29:39.959059\n",
      "2300 2.3430367 [2.3215704, 1.1502495, 1.1713208] 2018-05-22 16:30:01.124432\n",
      "2400 2.3322124 [2.1734638, 1.0812978, 1.092166] 2018-05-22 16:30:24.205976\n",
      "2500 2.3678966 [2.0887518, 1.0320325, 1.0567194] 2018-05-22 16:30:48.200671\n",
      "2600 2.327655 [2.2980704, 1.1481686, 1.1499019] 2018-05-22 16:31:09.392635\n",
      "2700 2.2465997 [2.2515168, 1.1119988, 1.139518] 2018-05-22 16:31:29.790307\n",
      "2800 2.299049 [2.3336668, 1.1511527, 1.1825142] 2018-05-22 16:31:56.581821\n",
      "2900 2.2405925 [2.5426626, 1.2696444, 1.2730181] 2018-05-22 16:32:17.712549\n",
      "3000 2.280997 [2.2913103, 1.1289468, 1.1623635] 2018-05-22 16:32:39.526608\n",
      "3100 2.2080915 [2.4803843, 1.2390994, 1.241285] 2018-05-22 16:33:03.089028\n",
      "3200 2.2130733 [2.3365731, 1.1386944, 1.1978786] 2018-05-22 16:33:24.511301\n",
      "3300 2.2353094 [2.0295224, 1.0045326, 1.0249897] 2018-05-22 16:33:51.953207\n",
      "3400 2.2134264 [2.459507, 1.1703508, 1.2891561] 2018-05-22 16:34:12.587310\n",
      "3500 2.1390707 [2.0759704, 1.0355033, 1.0404671] 2018-05-22 16:34:34.523489\n",
      "3600 2.177299 [1.833591, 0.9046465, 0.9289445] 2018-05-22 16:34:59.874031\n",
      "3700 2.1723135 [2.032748, 1.0002267, 1.0325212] 2018-05-22 16:35:24.132274\n",
      "3800 2.1855245 [2.1837049, 1.0624148, 1.12129] 2018-05-22 16:35:48.585132\n",
      "3900 2.1434362 [2.5272598, 1.2634382, 1.2638215] 2018-05-22 16:36:12.598064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-22 16:36:17.454917\n",
      "Trained at 2018-05-22 16:54:11.783831\n",
      "TP 13880 FP 2378 FN 5449 F 0.7800601343187119 Precision 0.8537335465616928 Recall 0.7180919861348233\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-22 16:59:22.739843\n",
      "Trained at 2018-05-22 17:17:00.445266\n",
      "TP 15537 FP 2589 FN 3792 F 0.8296355626752102 Precision 0.8571665011585567 Recall 0.8038180971597082\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-22 17:22:14.932758\n",
      "Trained at 2018-05-22 17:39:53.507774\n",
      "TP 14870 FP 2037 FN 4459 F 0.8207307649850977 Precision 0.879517359673508 Recall 0.7693103626674944\n",
      "Epoch 3 start at 2018-05-22 17:45:02.296514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Dict 2018-05-22 17:54:04.968170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-22 18:02:06.181771\n",
      "TP 16169 FP 2508 FN 3160 F 0.8508656527916645 Precision 0.8657171922685656 Recall 0.8365150809664235\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-22 18:07:17.724853\n",
      "Trained at 2018-05-22 18:23:35.769181\n",
      "TP 16623 FP 2549 FN 2706 F 0.8635100387002935 Precision 0.8670456916336324 Recall 0.8600031041440322\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-22 18:28:48.053007\n",
      "Trained at 2018-05-22 18:45:07.197579\n",
      "TP 17056 FP 2932 FN 2273 F 0.8676145178930234 Precision 0.8533119871923154 Recall 0.882404676910342\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-22 18:50:25.019029\n",
      "Trained at 2018-05-22 19:06:43.586723\n",
      "TP 17163 FP 3126 FN 2166 F 0.866424352567015 Precision 0.8459263640396274 Recall 0.8879404004345801\n",
      "Epoch 7 start at 2018-05-22 19:11:58.108398\n",
      "Trained at 2018-05-22 19:28:16.048465\n",
      "TP 17047 FP 2902 FN 2282 F 0.8680177198431692 Precision 0.8545290490751416 Recall 0.8819390553054995\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-22 19:33:31.397588\n",
      "Trained at 2018-05-22 19:49:50.643304\n",
      "TP 17100 FP 3026 FN 2229 F 0.8668102902040299 Precision 0.849647222498261 Recall 0.8846810492006829\n",
      "Epoch 9 start at 2018-05-22 19:55:05.250048\n",
      "Trained at 2018-05-22 20:11:24.594066\n",
      "TP 16808 FP 2572 FN 2521 F 0.868428530832623 Precision 0.8672858617131063 Recall 0.8695742149102385\n",
      "Best so far\n",
      "Epoch 10 start at 2018-05-22 20:16:40.770714\n",
      "Trained at 2018-05-22 20:33:01.046365\n",
      "TP 17213 FP 3063 FN 2116 F 0.8692336826158313 Precision 0.8489347011244821 Recall 0.8905271871281494\n",
      "Best so far\n",
      "Epoch 11 start at 2018-05-22 20:38:15.431305\n",
      "Trained at 2018-05-22 20:54:34.327598\n",
      "TP 17242 FP 3226 FN 2087 F 0.8664974746840214 Precision 0.8423881180379128 Recall 0.8920275234104196\n",
      "Epoch 12 start at 2018-05-22 20:59:50.846274\n",
      "Trained at 2018-05-22 21:16:09.986334\n",
      "TP 16976 FP 2705 FN 2353 F 0.870340938220969 Precision 0.8625577968599156 Recall 0.8782658182006312\n",
      "Best so far\n",
      "Epoch 13 start at 2018-05-22 21:21:24.287555\n",
      "Trained at 2018-05-22 21:37:42.459004\n",
      "TP 16956 FP 2973 FN 2373 F 0.8638239339752407 Precision 0.8508204124642481 Recall 0.8772311035232034\n",
      "Epoch 14 start at 2018-05-22 21:43:01.310845\n",
      "Trained at 2018-05-22 21:59:20.152764\n",
      "TP 17383 FP 3093 FN 1946 F 0.8734078633337521 Precision 0.8489451064661067 Recall 0.8993222618862848\n",
      "Best so far\n",
      "Epoch 15 start at 2018-05-22 22:04:34.082116\n",
      "Trained at 2018-05-22 22:20:52.698009\n",
      "TP 17263 FP 3208 FN 2066 F 0.8674874371859297 Precision 0.8432905085242538 Recall 0.8931139738217186\n",
      "Epoch 16 start at 2018-05-22 22:26:08.690141\n",
      "Trained at 2018-05-22 22:42:25.548926\n",
      "TP 17089 FP 3114 FN 2240 F 0.8645654153597085 Precision 0.8458644755729348 Recall 0.8841119561280977\n",
      "Epoch 17 start at 2018-05-22 22:47:43.476256\n",
      "Trained at 2018-05-22 23:04:00.840620\n",
      "TP 17328 FP 3192 FN 2001 F 0.8696830535270647 Precision 0.8444444444444444 Recall 0.8964767965233587\n",
      "Epoch 18 start at 2018-05-22 23:09:17.601838\n",
      "Trained at 2018-05-22 23:25:35.001346\n",
      "TP 17122 FP 2933 FN 2207 F 0.869490148283567 Precision 0.8537521815008726 Recall 0.8858192353458534\n",
      "Epoch 19 start at 2018-05-22 23:30:49.714582\n",
      "Trained at 2018-05-22 23:47:06.706990\n",
      "TP 17312 FP 3168 FN 2017 F 0.8697530709136125 Precision 0.8453125 Recall 0.8956490247814165\n",
      "Epoch 20 start at 2018-05-22 23:52:23.183123\n",
      "Trained at 2018-05-23 00:08:41.779942\n",
      "TP 17358 FP 3331 FN 1971 F 0.8675096206706981 Precision 0.8389965682246604 Recall 0.8980288685395003\n",
      "Epoch 21 start at 2018-05-23 00:14:00.815106\n",
      "Trained at 2018-05-23 00:30:18.655107\n",
      "TP 17223 FP 3149 FN 2106 F 0.8676355759300773 Precision 0.845425093265266 Recall 0.8910445444668632\n",
      "Epoch 22 start at 2018-05-23 00:35:34.122979\n",
      "Trained at 2018-05-23 00:51:53.085175\n",
      "TP 17063 FP 2911 FN 2266 F 0.8682797750807827 Precision 0.8542605387003104 Recall 0.8827668270474417\n",
      "Epoch 23 start at 2018-05-23 00:57:09.419562\n",
      "Trained at 2018-05-23 01:13:26.151321\n",
      "TP 16686 FP 2614 FN 2643 F 0.86391053353698 Precision 0.864559585492228 Recall 0.8632624553779296\n",
      "Epoch 24 start at 2018-05-23 01:18:39.894314\n",
      "Trained at 2018-05-23 01:34:57.334157\n",
      "TP 16896 FP 2753 FN 2433 F 0.8669505875109036 Precision 0.8598910886050181 Recall 0.8741269594909203\n",
      "Epoch 25 start at 2018-05-23 01:40:12.943585\n",
      "Trained at 2018-05-23 01:56:31.842618\n",
      "TP 16670 FP 2621 FN 2659 F 0.8632832729155878 Precision 0.8641335337722253 Recall 0.8624346836359874\n",
      "Epoch 26 start at 2018-05-23 02:01:48.050330\n",
      "Trained at 2018-05-23 02:18:05.589215\n",
      "TP 16899 FP 2851 FN 2430 F 0.8648634816653445 Precision 0.8556455696202532 Recall 0.8742821666925346\n",
      "Epoch 27 start at 2018-05-23 02:23:18.292454\n",
      "Trained at 2018-05-23 02:39:37.785583\n",
      "TP 17065 FP 3077 FN 2264 F 0.8646854652783056 Precision 0.8472346340979049 Recall 0.8828702985151844\n",
      "Epoch 28 start at 2018-05-23 02:44:57.095141\n",
      "Trained at 2018-05-23 03:01:16.921696\n",
      "TP 16410 FP 2537 FN 2919 F 0.8574563695265963 Precision 0.8661001741700534 Recall 0.8489833928294273\n",
      "Epoch 29 start at 2018-05-23 03:06:32.080320\n",
      "Trained at 2018-05-23 03:22:49.915426\n",
      "TP 16806 FP 2846 FN 2523 F 0.8622662322670018 Precision 0.855180134337472 Recall 0.8694707434424958\n"
     ]
    }
   ],
   "source": [
    "# as close as possible to original plus unsup plus dict\n",
    "from chemlistem import minimodel_cudnn\n",
    "nmm = minimodel_cudnn.MiniModelCuDNN()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patabs_a61k31a61_noct_s.txt\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-23 10:57:00.528754\n",
      "Shufflesplit at 2018-05-23 10:57:00.528862\n",
      "Read annots at 2018-05-23 10:57:00.586091\n",
      "Read train seqs at 2018-05-23 10:57:00.870259\n",
      "Read test seqs at 2018-05-23 10:57:43.947870\n",
      "Corpus read at 2018-05-23 10:57:56.405363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-23 10:58:00.639078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "3931 batches of unsupervised\n",
      "0 9.021872 [9.021872, 4.5118017, 4.5100703] 2018-05-23 10:58:13.738607\n",
      "100 6.396768 [6.0358524, 3.0292566, 3.0065956] 2018-05-23 10:58:45.983151\n",
      "200 5.628336 [5.120054, 2.549078, 2.5709758] 2018-05-23 10:59:11.399961\n",
      "300 5.039028 [4.748557, 2.3937612, 2.354796] 2018-05-23 10:59:40.387082\n",
      "400 4.605242 [4.6052094, 2.3074522, 2.2977571] 2018-05-23 11:00:12.009944\n",
      "500 4.292568 [4.1510134, 2.0849838, 2.0660293] 2018-05-23 11:00:48.137113\n",
      "600 4.0523257 [4.014925, 2.0260491, 1.988876] 2018-05-23 11:01:17.326236\n",
      "700 3.8361683 [3.2350147, 1.6581022, 1.5769125] 2018-05-23 11:01:51.159690\n",
      "800 3.6253214 [3.662393, 1.8455039, 1.816889] 2018-05-23 11:02:19.144084\n",
      "900 3.4266977 [3.3780088, 1.694302, 1.683707] 2018-05-23 11:02:44.890056\n",
      "1000 3.2758975 [3.1421165, 1.5908539, 1.5512626] 2018-05-23 11:03:07.971466\n",
      "1100 3.2110229 [3.0356064, 1.5301399, 1.5054665] 2018-05-23 11:03:35.585813\n",
      "1200 3.1817427 [3.3095715, 1.6650655, 1.644506] 2018-05-23 11:04:07.671326\n",
      "1300 3.0993133 [2.7301493, 1.384821, 1.3453283] 2018-05-23 11:04:36.317527\n",
      "1400 2.989977 [2.961911, 1.502914, 1.458997] 2018-05-23 11:05:02.711554\n",
      "1500 2.9907649 [3.5832863, 1.8070128, 1.7762735] 2018-05-23 11:05:33.001946\n",
      "1600 2.8309925 [2.8039649, 1.4214306, 1.3825343] 2018-05-23 11:06:00.418919\n",
      "1700 2.8596897 [2.2080715, 1.1143101, 1.0937613] 2018-05-23 11:06:31.547269\n",
      "1800 2.8103194 [2.561907, 1.2948868, 1.2670202] 2018-05-23 11:07:02.346827\n",
      "1900 2.7148242 [2.6187038, 1.3146278, 1.304076] 2018-05-23 11:07:27.513005\n",
      "2000 2.7495403 [2.796307, 1.4135618, 1.3827453] 2018-05-23 11:07:58.962972\n",
      "2100 2.733548 [3.118681, 1.5662582, 1.5524228] 2018-05-23 11:08:32.200382\n",
      "2200 2.6624994 [2.4403696, 1.2171791, 1.2231905] 2018-05-23 11:09:01.503316\n",
      "2300 2.6060922 [2.3508158, 1.1772358, 1.17358] 2018-05-23 11:09:31.495908\n",
      "2400 2.5687602 [2.541043, 1.2766206, 1.2644224] 2018-05-23 11:09:58.857916\n",
      "2500 2.54889 [2.45969, 1.2410343, 1.2186558] 2018-05-23 11:10:22.975207\n",
      "2600 2.5375779 [2.5812485, 1.3171518, 1.2640967] 2018-05-23 11:10:49.688556\n",
      "2700 2.5611825 [2.6004684, 1.3064892, 1.2939792] 2018-05-23 11:11:19.509441\n",
      "2800 2.581867 [2.359286, 1.1881247, 1.1711614] 2018-05-23 11:11:52.904210\n",
      "2900 2.5329869 [1.8714013, 0.94120824, 0.93019307] 2018-05-23 11:12:25.222232\n",
      "3000 2.4866958 [2.5869539, 1.3046348, 1.2823191] 2018-05-23 11:12:56.054502\n",
      "3100 2.4618096 [2.3790102, 1.1804597, 1.1985503] 2018-05-23 11:13:25.916601\n",
      "3200 2.4686573 [2.3139815, 1.177176, 1.1368055] 2018-05-23 11:13:58.429677\n",
      "3300 2.4138002 [2.5236695, 1.2689474, 1.2547221] 2018-05-23 11:14:30.991347\n",
      "3400 2.441534 [2.664517, 1.3376215, 1.3268954] 2018-05-23 11:15:03.844057\n",
      "3500 2.3735578 [2.524611, 1.2629101, 1.2617007] 2018-05-23 11:15:27.929965\n",
      "3600 2.3727632 [2.7957816, 1.4034936, 1.392288] 2018-05-23 11:15:58.115377\n",
      "3700 2.4165432 [2.2113297, 1.1137366, 1.0975931] 2018-05-23 11:16:32.383474\n",
      "3800 2.4098487 [2.7402768, 1.3727489, 1.367528] 2018-05-23 11:17:04.849396\n",
      "3900 2.3960247 [2.9322066, 1.4732742, 1.4589324] 2018-05-23 11:17:36.346157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-23 11:17:48.697151\n",
      "Trained at 2018-05-23 11:24:46.154953\n",
      "TP 14084 FP 3003 FN 5245 F 0.773506151142355 Precision 0.8242523555919705 Recall 0.7286460758445858\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-23 11:26:57.544598\n",
      "Trained at 2018-05-23 11:33:46.853415\n",
      "TP 15075 FP 2859 FN 4254 F 0.8091135979389743 Precision 0.8405821344931416 Recall 0.7799161881111284\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-23 11:35:59.742067\n",
      "Trained at 2018-05-23 11:42:45.872366\n",
      "TP 15741 FP 2757 FN 3588 F 0.8322626695217702 Precision 0.8509568602011028 Recall 0.8143721868694708\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-23 11:44:59.117383\n",
      "Trained at 2018-05-23 11:51:46.930246\n",
      "TP 16394 FP 3165 FN 2935 F 0.843139271754783 Precision 0.8381819111406513 Recall 0.8481556210874851\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-23 11:53:57.977714\n",
      "Trained at 2018-05-23 12:00:47.334780\n",
      "TP 16780 FP 3353 FN 2549 F 0.8504383964320106 Precision 0.8334575075746287 Recall 0.8681256143618398\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-23 12:02:58.770083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Dict 2018-05-23 12:07:05.278591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-23 12:09:27.240273\n",
      "TP 16695 FP 3027 FN 2634 F 0.8550357225167089 Precision 0.8465165804685123 Recall 0.863728076982772\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-23 12:11:38.721390\n",
      "Trained at 2018-05-23 12:17:34.421998\n",
      "TP 16685 FP 3033 FN 2644 F 0.8546111096883243 Precision 0.8461811542752815 Recall 0.8632107196440582\n",
      "Epoch 7 start at 2018-05-23 12:19:44.500263\n",
      "Trained at 2018-05-23 12:25:40.695845\n",
      "TP 16775 FP 2954 FN 2554 F 0.8589789543755441 Precision 0.8502711744133002 Recall 0.8678669356924827\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-23 12:27:51.664347\n",
      "Trained at 2018-05-23 12:33:47.126011\n",
      "TP 16728 FP 2915 FN 2601 F 0.8584624858873037 Precision 0.851601079264878 Recall 0.8654353562005277\n",
      "Epoch 9 start at 2018-05-23 12:35:57.766511\n",
      "Trained at 2018-05-23 12:41:54.417406\n",
      "TP 16783 FP 2941 FN 2546 F 0.8594986300668322 Precision 0.8508923139322653 Recall 0.8682808215634539\n",
      "Best so far\n",
      "Epoch 10 start at 2018-05-23 12:44:05.580499\n",
      "Trained at 2018-05-23 12:50:02.227503\n",
      "TP 17028 FP 3255 FN 2301 F 0.8597394728870039 Precision 0.8395207809495637 Recall 0.8809560763619432\n",
      "Best so far\n",
      "Epoch 11 start at 2018-05-23 12:52:13.128055\n",
      "Trained at 2018-05-23 12:58:08.621485\n",
      "TP 16836 FP 3054 FN 2493 F 0.8585634513883577 Precision 0.8464555052790347 Recall 0.8710228154586372\n",
      "Epoch 12 start at 2018-05-23 13:00:19.272164\n",
      "Trained at 2018-05-23 13:06:16.143097\n",
      "TP 16832 FP 2974 FN 2497 F 0.8602018653379327 Precision 0.8498434817732 Recall 0.8708158725231517\n",
      "Best so far\n",
      "Epoch 13 start at 2018-05-23 13:08:27.297909\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-869be3c21afc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneominimodel_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneominimodel_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPredNeoMiniModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../catted_patabs_a61k31a61_noct_s.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel_pred.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname, unsupfile, nunsup, bsize)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;31m#model.fit(tx, ty, verbose=0, epochs=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#interleave dict training, with pretraining, 2 layers pretraining\n",
    "from chemlistem import neominimodel_pred\n",
    "nmm = neominimodel_pred.PredNeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patabs_a61k31a61_noct_s.txt\", -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-23 13:24:05.267248\n",
      "Shufflesplit at 2018-05-23 13:24:05.267370\n",
      "Read annots at 2018-05-23 13:24:05.324342\n",
      "Read train seqs at 2018-05-23 13:24:05.613916\n",
      "Read test seqs at 2018-05-23 13:24:48.909251\n",
      "Corpus read at 2018-05-23 13:25:01.488502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-23 13:25:05.681578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "3931 batches of unsupervised\n",
      "0 9.021765 [9.021765, 4.511097, 4.510668] 2018-05-23 13:25:18.915009\n",
      "100 6.5663743 [6.0821314, 3.0628839, 3.0192478] 2018-05-23 13:25:50.618830\n",
      "200 5.6965833 [5.1026573, 2.5420523, 2.560605] 2018-05-23 13:26:17.215042\n",
      "300 5.0552235 [4.740237, 2.3565187, 2.3837183] 2018-05-23 13:26:46.337992\n",
      "400 4.6464534 [4.678955, 2.3390238, 2.3399315] 2018-05-23 13:27:18.150389\n",
      "500 4.3646946 [4.2205095, 2.1079535, 2.112556] 2018-05-23 13:27:54.519376\n",
      "600 4.161886 [4.108256, 2.0545197, 2.0537362] 2018-05-23 13:28:24.000028\n",
      "700 3.9683104 [3.3888867, 1.6957983, 1.6930884] 2018-05-23 13:28:58.254723\n",
      "800 3.7732513 [3.81185, 1.9023093, 1.9095407] 2018-05-23 13:29:26.613472\n",
      "900 3.59654 [3.596367, 1.7838719, 1.812495] 2018-05-23 13:29:51.774629\n",
      "1000 3.4635122 [3.3341944, 1.6541164, 1.680078] 2018-05-23 13:30:15.807395\n",
      "1100 3.4156349 [3.2027655, 1.5856748, 1.6170907] 2018-05-23 13:30:43.576639\n",
      "1200 3.3988996 [3.523086, 1.7597626, 1.7633233] 2018-05-23 13:31:15.636332\n",
      "1300 3.3297634 [2.9355142, 1.4645259, 1.4709883] 2018-05-23 13:31:44.453028\n",
      "1400 3.222169 [3.184739, 1.5956905, 1.5890485] 2018-05-23 13:32:09.926042\n",
      "1500 3.2385383 [3.866245, 1.9350293, 1.9312158] 2018-05-23 13:32:40.264571\n",
      "1600 3.0806692 [3.0705342, 1.5207312, 1.549803] 2018-05-23 13:33:07.955818\n",
      "1700 3.119873 [2.441574, 1.2265142, 1.21506] 2018-05-23 13:33:39.475084\n",
      "1800 3.07393 [2.8093998, 1.4062455, 1.4031544] 2018-05-23 13:34:11.618117\n",
      "1900 2.982396 [2.8940034, 1.4516735, 1.4423299] 2018-05-23 13:34:36.241823\n",
      "2000 3.019809 [3.0716815, 1.5349143, 1.5367671] 2018-05-23 13:35:08.041739\n",
      "2100 3.0052254 [3.3816583, 1.685854, 1.6958044] 2018-05-23 13:35:42.401429\n",
      "2200 2.9423008 [2.7334514, 1.3680072, 1.3654442] 2018-05-23 13:36:11.958376\n",
      "2300 2.8859468 [2.6510768, 1.3460859, 1.3049909] 2018-05-23 13:36:41.464568\n",
      "2400 2.8498805 [2.8203516, 1.4110588, 1.4092929] 2018-05-23 13:37:09.060446\n",
      "2500 2.8360398 [2.774479, 1.3941944, 1.3802845] 2018-05-23 13:37:34.180152\n",
      "2600 2.8283415 [2.86231, 1.4279264, 1.4343834] 2018-05-23 13:38:00.410857\n",
      "2700 2.8529558 [2.9077601, 1.4568095, 1.4509506] 2018-05-23 13:38:30.363778\n",
      "2800 2.870454 [2.614205, 1.3159099, 1.298295] 2018-05-23 13:39:04.834666\n",
      "2900 2.832532 [2.1996126, 1.0842338, 1.1153789] 2018-05-23 13:39:36.432554\n",
      "3000 2.78107 [2.8829477, 1.4376594, 1.4452883] 2018-05-23 13:40:07.465595\n",
      "3100 2.7542193 [2.6337156, 1.308277, 1.3254385] 2018-05-23 13:40:37.557812\n",
      "3200 2.7652636 [2.6418693, 1.3296472, 1.3122221] 2018-05-23 13:41:11.004880\n",
      "3300 2.713263 [2.8298635, 1.4108989, 1.4189646] 2018-05-23 13:41:43.868997\n",
      "3400 2.7416766 [2.9716744, 1.4867156, 1.4849589] 2018-05-23 13:42:16.200144\n",
      "3500 2.6739285 [2.8478608, 1.4233658, 1.4244949] 2018-05-23 13:42:41.397986\n",
      "3600 2.6719818 [3.1228294, 1.5615172, 1.5613123] 2018-05-23 13:43:11.849356\n",
      "3700 2.7177165 [2.4859333, 1.2425766, 1.2433568] 2018-05-23 13:43:45.431085\n",
      "3800 2.717923 [3.0338287, 1.5004352, 1.5333934] 2018-05-23 13:44:19.005233\n",
      "3900 2.7018096 [3.2501183, 1.6235167, 1.6266017] 2018-05-23 13:44:50.813105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-23 13:45:03.228032\n",
      "Trained at 2018-05-23 13:51:59.117895\n",
      "TP 14275 FP 3256 FN 5054 F 0.7745523602821487 Precision 0.8142718612743141 Recall 0.7385276010140204\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-23 13:54:10.632059\n",
      "Trained at 2018-05-23 14:00:59.217317\n",
      "TP 15651 FP 3159 FN 3678 F 0.8207346810351609 Precision 0.8320574162679426 Recall 0.8097159708210461\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-23 14:03:09.448294\n",
      "Trained at 2018-05-23 14:09:55.502030\n",
      "TP 16211 FP 3034 FN 3118 F 0.8405143360812983 Precision 0.8423486619901273 Recall 0.8386879817890217\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-23 14:12:04.834437\n",
      "Trained at 2018-05-23 14:18:52.190178\n",
      "TP 16521 FP 3182 FN 2808 F 0.8465361754457881 Precision 0.8385017510023854 Recall 0.854726059289151\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-23 14:21:00.577849\n",
      "Trained at 2018-05-23 14:27:46.779018\n",
      "TP 16649 FP 3072 FN 2680 F 0.8527016645326504 Precision 0.8442269661781857 Recall 0.8613482332246883\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-23 14:29:55.848773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Dict 2018-05-23 14:34:02.182588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-23 14:36:23.749141\n",
      "TP 16843 FP 3109 FN 2486 F 0.8575647259489321 Precision 0.8441760224538893 Recall 0.871384965595737\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-23 14:38:31.821365\n",
      "Trained at 2018-05-23 14:44:27.572933\n",
      "TP 16877 FP 3082 FN 2452 F 0.8591427407859906 Precision 0.845583446064432 Recall 0.8731439805473641\n",
      "Best so far\n",
      "Epoch 7 start at 2018-05-23 14:46:34.746761\n",
      "Trained at 2018-05-23 14:52:31.071358\n",
      "TP 16780 FP 2888 FN 2549 F 0.8605790188988897 Precision 0.8531624974577995 Recall 0.8681256143618398\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-23 14:54:39.400644\n",
      "Trained at 2018-05-23 15:00:34.993232\n",
      "TP 17067 FP 3040 FN 2262 F 0.8655543158535348 Precision 0.848808872531954 Recall 0.8829737699829272\n",
      "Best so far\n",
      "Epoch 9 start at 2018-05-23 15:02:41.859183\n",
      "Trained at 2018-05-23 15:08:37.933873\n",
      "TP 16902 FP 2842 FN 2427 F 0.8651498477209326 Precision 0.8560575364667747 Recall 0.8744373738941487\n",
      "Epoch 10 start at 2018-05-23 15:10:47.142561\n",
      "Trained at 2018-05-23 15:16:41.940698\n",
      "TP 16876 FP 3001 FN 2453 F 0.860888639493955 Precision 0.8490214821150073 Recall 0.8730922448134927\n",
      "Epoch 11 start at 2018-05-23 15:18:50.336707\n",
      "Trained at 2018-05-23 15:24:45.663833\n",
      "TP 16779 FP 2835 FN 2550 F 0.8617209768122641 Precision 0.8554603854389722 Recall 0.8680738786279684\n",
      "Epoch 12 start at 2018-05-23 15:26:54.434447\n",
      "Trained at 2018-05-23 15:32:50.145872\n",
      "TP 16837 FP 2835 FN 2492 F 0.8634137586215738 Precision 0.8558865392435949 Recall 0.8710745511925087\n",
      "Epoch 13 start at 2018-05-23 15:34:58.851604\n",
      "Trained at 2018-05-23 15:40:54.284447\n",
      "TP 16872 FP 2970 FN 2457 F 0.8614536264072911 Precision 0.850317508315694 Recall 0.8728853018780072\n",
      "Epoch 14 start at 2018-05-23 15:43:03.486889\n",
      "Trained at 2018-05-23 15:49:00.072003\n",
      "TP 16954 FP 2949 FN 2375 F 0.8642944535073409 Precision 0.8518313822036879 Recall 0.8771276320554607\n",
      "Epoch 15 start at 2018-05-23 15:51:09.119148\n",
      "Trained at 2018-05-23 15:57:06.095410\n",
      "TP 17034 FP 3128 FN 2295 F 0.8626775721050366 Precision 0.8448566610455311 Recall 0.8812664907651715\n",
      "Epoch 16 start at 2018-05-23 15:59:15.581404\n",
      "Trained at 2018-05-23 16:05:10.775839\n",
      "TP 16895 FP 2920 FN 2434 F 0.8632229715920703 Precision 0.8526368912440071 Recall 0.874075223757049\n",
      "Epoch 17 start at 2018-05-23 16:07:17.214368\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d027aa80b221>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneominimodel_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneominimodel_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPredNeoMiniModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../catted_patabs_a61k31a61_noct_s.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel_pred.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname, unsupfile, nunsup, bsize)\u001b[0m\n\u001b[1;32m    400\u001b[0m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;31m#model.fit(tx, ty, verbose=0, epochs=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#interleave dict training, with pretraining, 2 layers pretraining, full dropout\n",
    "from chemlistem import neominimodel_pred\n",
    "nmm = neominimodel_pred.PredNeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patabs_a61k31a61_noct_s.txt\", -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-23 16:30:44.592737\n",
      "Shufflesplit at 2018-05-23 16:30:44.593394\n",
      "Read annots at 2018-05-23 16:30:44.651034\n",
      "Read train seqs at 2018-05-23 16:30:44.931168\n",
      "Read test seqs at 2018-05-23 16:31:28.176826\n",
      "Corpus read at 2018-05-23 16:31:40.740358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make train dict at 2018-05-23 16:31:45.153826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "40000 batches of unsupervised\n",
      "0 9.02054 [9.02054, 4.510281, 4.5102587] 2018-05-23 16:32:02.514589\n",
      "100 7.175796 [6.690226, 3.3464198, 3.3438063] 2018-05-23 16:32:37.332886\n",
      "200 5.9873133 [5.5326705, 2.7648335, 2.767837] 2018-05-23 16:33:10.901108\n",
      "300 5.5094995 [5.1669617, 2.5765173, 2.590444] 2018-05-23 16:33:44.329890\n",
      "400 5.3363996 [5.176631, 2.5889091, 2.5877218] 2018-05-23 16:34:15.621732\n",
      "500 4.968512 [5.576868, 2.7380612, 2.838807] 2018-05-23 16:34:52.413290\n",
      "Skipped batch, length 37590\n",
      "600 4.8254128 [5.3972826, 2.6839564, 2.7133265] 2018-05-23 16:35:22.239192\n",
      "700 4.468356 [4.7070932, 2.3499928, 2.3571005] 2018-05-23 16:36:00.874519\n",
      "Skipped batch, length 16605\n",
      "800 4.350594 [1.6896434, 0.9463252, 0.7433182] 2018-05-23 16:36:28.401114\n",
      "900 4.217 [2.036062, 1.1014638, 0.93459815] 2018-05-23 16:36:59.932971\n",
      "Skipped batch, length 1233816\n",
      "1000 4.2242084 [4.0355873, 2.0044863, 2.0311007] 2018-05-23 16:37:36.437586\n",
      "1100 4.185959 [1.665766, 0.9290196, 0.73674643] 2018-05-23 16:38:09.552319\n",
      "1200 4.0999217 [4.2081084, 2.105689, 2.1024194] 2018-05-23 16:38:37.335903\n",
      "1300 3.982179 [4.6293626, 2.3375244, 2.2918382] 2018-05-23 16:39:15.950342\n",
      "1400 3.9040003 [3.8103874, 1.9127998, 1.8975875] 2018-05-23 16:39:43.742638\n",
      "1500 3.897318 [3.543569, 1.7643683, 1.7792008] 2018-05-23 16:40:18.977469\n",
      "1600 3.810886 [4.190939, 2.093672, 2.0972667] 2018-05-23 16:40:56.988706\n",
      "1700 3.5787275 [3.4336114, 1.7141958, 1.7194157] 2018-05-23 16:41:31.919527\n",
      "Skipped batch, length 46205\n",
      "1800 3.591999 [3.4299083, 1.7016656, 1.7282426] 2018-05-23 16:42:06.121241\n",
      "Skipped batch, length 17955\n",
      "1900 3.6365252 [3.270319, 1.6280172, 1.6423017] 2018-05-23 16:42:58.314690\n",
      "2000 3.7168045 [3.9454749, 1.9707661, 1.9747088] 2018-05-23 16:43:34.768812\n",
      "2100 3.598194 [3.410461, 1.6959436, 1.7145174] 2018-05-23 16:44:27.209191\n",
      "2200 3.5563507 [1.165522, 0.699234, 0.46628797] 2018-05-23 16:44:56.332744\n",
      "2300 3.6148598 [3.341548, 1.6503465, 1.6912013] 2018-05-23 16:45:26.390336\n",
      "Skipped batch, length 59770\n",
      "2400 3.5266726 [3.345913, 1.6491857, 1.6967273] 2018-05-23 16:45:56.541720\n",
      "2500 3.5198257 [3.0672626, 1.5293937, 1.5378689] 2018-05-23 16:46:28.983694\n",
      "2600 3.4826124 [3.92561, 1.9619676, 1.9636426] 2018-05-23 16:47:01.812340\n",
      "2700 3.4922554 [3.4152641, 1.7001851, 1.7150792] 2018-05-23 16:47:37.062318\n",
      "2800 3.4319282 [4.029102, 2.0127673, 2.0163345] 2018-05-23 16:48:04.745831\n",
      "2900 3.4280236 [1.1797141, 0.69917893, 0.48053515] 2018-05-23 16:48:32.190932\n",
      "3000 3.3330905 [1.3549517, 0.7732711, 0.58168066] 2018-05-23 16:49:11.323920\n",
      "3100 3.3985355 [3.4599192, 1.735467, 1.7244523] 2018-05-23 16:49:43.952905\n",
      "3200 3.2951715 [3.5511742, 1.7693369, 1.7818371] 2018-05-23 16:50:13.890181\n",
      "3300 3.3267815 [3.7664487, 1.8812602, 1.8851886] 2018-05-23 16:50:43.495961\n",
      "3400 3.4190552 [3.7236848, 1.8336383, 1.8900464] 2018-05-23 16:51:20.840941\n",
      "3500 3.3489242 [3.6101713, 1.8011708, 1.8090005] 2018-05-23 16:51:59.461704\n",
      "3600 3.297321 [2.92701, 1.4392719, 1.4877381] 2018-05-23 16:52:39.411136\n",
      "Skipped batch, length 25655\n",
      "3700 3.207652 [1.6006509, 0.88772786, 0.71292305] 2018-05-23 16:53:06.646332\n",
      "3800 3.2172575 [3.1335213, 1.5400252, 1.5934961] 2018-05-23 16:53:31.948117\n",
      "3900 3.3290122 [3.7918515, 1.888788, 1.9030635] 2018-05-23 16:54:05.270225\n",
      "4000 3.4066973 [3.0772262, 1.5431266, 1.5340996] 2018-05-23 16:54:43.623817\n",
      "4100 3.2923696 [3.8863573, 1.9354577, 1.9508996] 2018-05-23 16:55:18.306120\n",
      "4200 3.2680807 [3.302122, 1.648722, 1.6534001] 2018-05-23 16:55:50.627367\n",
      "4300 3.163201 [2.9135947, 1.4421883, 1.4714063] 2018-05-23 16:56:23.541930\n",
      "4400 3.256112 [3.722324, 1.8578651, 1.8644588] 2018-05-23 16:56:58.530363\n",
      "4500 3.2212195 [3.591529, 1.7901073, 1.8014216] 2018-05-23 16:57:32.229649\n",
      "4600 3.2625551 [2.7867417, 1.3912101, 1.3955318] 2018-05-23 16:58:04.265020\n",
      "4700 3.1655283 [2.8044438, 1.3929906, 1.4114532] 2018-05-23 16:58:39.889299\n",
      "4800 3.2197328 [2.7946243, 1.3832777, 1.4113466] 2018-05-23 16:59:17.232282\n",
      "4900 3.1757438 [3.4326982, 1.7139839, 1.7187142] 2018-05-23 16:59:46.881847\n",
      "5000 3.219142 [3.5320106, 1.7589564, 1.7730541] 2018-05-23 17:00:33.395303\n",
      "5100 3.2311602 [3.6032329, 1.7993343, 1.8038987] 2018-05-23 17:01:06.439274\n",
      "5200 3.2473037 [3.2158058, 1.602179, 1.6136267] 2018-05-23 17:01:48.582401\n",
      "5300 3.1346743 [3.065568, 1.5267895, 1.5387783] 2018-05-23 17:02:20.554336\n",
      "5400 3.1316118 [3.110598, 1.5554655, 1.5551327] 2018-05-23 17:02:52.785994\n",
      "5500 3.0309114 [3.23072, 1.6070998, 1.6236202] 2018-05-23 17:03:33.865549\n",
      "5600 3.1094089 [2.7536855, 1.3597947, 1.3938909] 2018-05-23 17:04:07.870900\n",
      "Skipped batch, length 27589\n",
      "5700 3.2145598 [3.207641, 1.5934668, 1.6141741] 2018-05-23 17:04:50.922450\n",
      "5800 3.2013385 [3.3987064, 1.6908273, 1.7078793] 2018-05-23 17:05:29.248692\n",
      "5900 3.2342882 [2.8542156, 1.4204235, 1.4337922] 2018-05-23 17:05:59.105593\n",
      "6000 3.1315908 [2.7366033, 1.3651636, 1.3714397] 2018-05-23 17:06:29.547654\n",
      "6100 3.176636 [3.1666167, 1.5782046, 1.588412] 2018-05-23 17:07:02.577999\n",
      "6200 3.1109018 [1.4013605, 0.84101164, 0.5603488] 2018-05-23 17:07:38.003487\n",
      "6300 3.11864 [3.120536, 1.555826, 1.5647101] 2018-05-23 17:08:18.037169\n",
      "6400 3.089193 [3.0126343, 1.4901406, 1.5224937] 2018-05-23 17:08:52.939770\n",
      "6500 3.0910597 [3.282885, 1.6369044, 1.6459806] 2018-05-23 17:09:26.423171\n",
      "6600 3.0795107 [3.4186807, 1.7100756, 1.708605] 2018-05-23 17:10:04.418127\n",
      "6700 3.0961196 [3.311403, 1.6485679, 1.6628351] 2018-05-23 17:10:33.221683\n",
      "6800 3.1496096 [3.2644663, 1.6224948, 1.6419716] 2018-05-23 17:11:03.783411\n",
      "6900 3.0654097 [3.510262, 1.7513281, 1.758934] 2018-05-23 17:11:44.487119\n",
      "7000 3.1510606 [3.5802822, 1.7826838, 1.7975982] 2018-05-23 17:12:19.346184\n",
      "7100 3.1221883 [2.5447185, 1.2666149, 1.2781036] 2018-05-23 17:12:47.198704\n",
      "7200 3.1353626 [3.247638, 1.615952, 1.631686] 2018-05-23 17:13:28.545020\n",
      "7300 3.1093183 [2.744524, 1.3585694, 1.3859546] 2018-05-23 17:14:05.129558\n",
      "7400 3.0135589 [3.1689436, 1.5850658, 1.5838778] 2018-05-23 17:14:43.088238\n",
      "7500 3.140361 [3.7040699, 1.7839131, 1.9201567] 2018-05-23 17:15:22.332281\n",
      "7600 2.987297 [1.4553512, 0.83998597, 0.61536527] 2018-05-23 17:15:47.332815\n",
      "7700 3.033059 [3.0559022, 1.532031, 1.5238712] 2018-05-23 17:16:23.087568\n",
      "7800 3.028713 [2.6324596, 1.3158406, 1.316619] 2018-05-23 17:17:00.369335\n",
      "7900 3.0490618 [2.7377586, 1.3545682, 1.3831904] 2018-05-23 17:17:32.539872\n",
      "8000 3.0215774 [2.5442057, 1.2246649, 1.3195407] 2018-05-23 17:17:57.140070\n",
      "8100 3.0481782 [3.0436282, 1.5096853, 1.5339429] 2018-05-23 17:18:33.645631\n",
      "8200 2.9932349 [3.5810018, 1.8646239, 1.716378] 2018-05-23 17:19:07.953872\n",
      "8300 3.0116193 [3.0688171, 1.5309258, 1.5378914] 2018-05-23 17:19:36.126311\n",
      "Skipped batch, length 19891\n",
      "8400 3.0445008 [3.3784719, 1.6799572, 1.6985146] 2018-05-23 17:20:22.427193\n",
      "8500 2.986385 [3.1764326, 1.5860627, 1.5903699] 2018-05-23 17:20:52.840593\n",
      "8600 3.131798 [3.4385877, 1.709893, 1.7286948] 2018-05-23 17:21:25.239103\n",
      "8700 2.9879158 [2.5847826, 1.281083, 1.3036996] 2018-05-23 17:21:57.021533\n",
      "8800 2.946869 [2.817734, 1.3908556, 1.4268785] 2018-05-23 17:22:25.668491\n",
      "8900 2.9607596 [2.862733, 1.4231629, 1.4395701] 2018-05-23 17:22:56.281451\n",
      "9000 3.0553298 [3.2163522, 1.6001089, 1.6162434] 2018-05-23 17:23:34.661860\n",
      "9100 3.0535958 [3.4070342, 1.6964189, 1.7106153] 2018-05-23 17:24:11.200360\n",
      "9200 2.9813468 [3.3883128, 1.6813365, 1.7069763] 2018-05-23 17:24:43.785825\n",
      "9300 2.9568045 [2.914274, 1.451005, 1.463269] 2018-05-23 17:25:11.596664\n",
      "9400 2.9403849 [3.4202912, 1.6970346, 1.7232566] 2018-05-23 17:25:45.211397\n",
      "9500 2.866708 [3.5275989, 1.7397504, 1.7878486] 2018-05-23 17:26:16.796551\n",
      "9600 3.0374725 [3.3401887, 1.6662973, 1.6738914] 2018-05-23 17:26:47.765334\n",
      "9700 2.9167576 [3.58664, 1.7535757, 1.8330641] 2018-05-23 17:27:20.495668\n",
      "9800 2.949986 [2.5700383, 1.2721704, 1.297868] 2018-05-23 17:27:47.324148\n",
      "9900 2.9662488 [3.3814366, 1.6897087, 1.6917279] 2018-05-23 17:28:16.348410\n",
      "10000 3.0259225 [2.8515286, 1.4224379, 1.4290909] 2018-05-23 17:28:53.170502\n",
      "10100 2.9921765 [2.4097269, 1.2167194, 1.1930075] 2018-05-23 17:29:28.550650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10200 2.9539835 [3.3668988, 1.6722523, 1.6946465] 2018-05-23 17:30:08.687700\n",
      "10300 3.0212307 [2.8786905, 1.4320273, 1.4466631] 2018-05-23 17:30:39.299048\n",
      "10400 3.0691302 [1.9067457, 1.016624, 0.8901217] 2018-05-23 17:31:29.652352\n",
      "10500 2.9723847 [3.2420201, 1.608401, 1.6336191] 2018-05-23 17:32:00.445181\n",
      "Skipped batch, length 18757\n",
      "10600 2.9805186 [3.2446213, 1.6115592, 1.6330621] 2018-05-23 17:32:26.798667\n",
      "10700 3.0501163 [3.0438948, 1.515931, 1.5279636] 2018-05-23 17:33:02.643484\n",
      "10800 3.0355442 [3.354246, 1.6698284, 1.6844175] 2018-05-23 17:33:44.575749\n",
      "10900 2.9788995 [2.7763305, 1.3697631, 1.4065673] 2018-05-23 17:34:16.588257\n",
      "11000 2.929068 [3.0514317, 1.5161365, 1.535295] 2018-05-23 17:34:50.327824\n",
      "11100 2.997684 [2.8150802, 1.3929813, 1.4220988] 2018-05-23 17:35:26.339809\n",
      "11200 2.9227898 [2.7519197, 1.3664298, 1.3854898] 2018-05-23 17:36:01.367870\n",
      "11300 2.9208415 [2.6096213, 1.3018293, 1.307792] 2018-05-23 17:36:30.334637\n",
      "11400 2.9437046 [1.4056418, 0.80734533, 0.5982965] 2018-05-23 17:37:01.942455\n",
      "11500 3.0359247 [3.472746, 1.7280697, 1.7446761] 2018-05-23 17:37:34.891072\n",
      "11600 2.9081094 [3.118042, 1.546643, 1.5713991] 2018-05-23 17:38:01.289122\n",
      "11700 2.9664276 [2.8672976, 1.4208326, 1.446465] 2018-05-23 17:38:29.753154\n",
      "11800 2.9086802 [1.3401971, 0.7530839, 0.58711314] 2018-05-23 17:39:13.425426\n",
      "11900 2.9384928 [3.044921, 1.5112102, 1.5337107] 2018-05-23 17:39:42.963973\n",
      "12000 2.92928 [3.2131934, 1.5983119, 1.6148815] 2018-05-23 17:40:14.775481\n",
      "12100 2.8965793 [2.9956222, 1.4860078, 1.5096145] 2018-05-23 17:40:48.206797\n",
      "12200 2.8990633 [3.275469, 1.6322513, 1.6432178] 2018-05-23 17:41:27.171438\n",
      "12300 2.868928 [3.1681225, 1.5717316, 1.596391] 2018-05-23 17:42:00.704514\n",
      "12400 2.9015558 [2.9211493, 1.4588832, 1.4622662] 2018-05-23 17:42:38.257269\n",
      "12500 2.8274267 [3.129013, 1.5517178, 1.5772954] 2018-05-23 17:43:04.581838\n",
      "12600 2.907616 [2.8466766, 1.4137015, 1.432975] 2018-05-23 17:43:31.297518\n",
      "12700 2.9166558 [3.4107618, 1.6945293, 1.7162325] 2018-05-23 17:44:03.845946\n",
      "12800 2.895995 [2.431667, 1.219568, 1.2120991] 2018-05-23 17:44:37.804406\n",
      "12900 2.9789867 [3.3644068, 1.6734109, 1.6909959] 2018-05-23 17:45:11.802956\n",
      "13000 2.8969948 [2.914815, 1.455949, 1.4588659] 2018-05-23 17:45:49.238721\n",
      "13100 3.0068026 [1.8836663, 1.0261633, 0.8575029] 2018-05-23 17:46:27.446481\n",
      "13200 2.9238474 [2.894678, 1.4583801, 1.4362979] 2018-05-23 17:47:04.115645\n",
      "13300 2.9277315 [2.8614933, 1.401865, 1.4596283] 2018-05-23 17:47:39.232235\n",
      "13400 2.9223993 [3.0226326, 1.5018823, 1.5207503] 2018-05-23 17:48:11.106766\n",
      "13500 3.025629 [3.2958255, 1.6484952, 1.6473303] 2018-05-23 17:48:57.324050\n",
      "13600 2.9055538 [3.0916915, 1.5327789, 1.5589125] 2018-05-23 17:49:29.879640\n",
      "13700 2.8341558 [3.6432488, 1.8113472, 1.8319016] 2018-05-23 17:50:04.511300\n",
      "13800 2.8796864 [3.216638, 1.6074111, 1.6092271] 2018-05-23 17:50:41.112403\n",
      "13900 2.915937 [2.926929, 1.4486735, 1.4782555] 2018-05-23 17:51:12.636325\n",
      "14000 2.812117 [3.217181, 1.5974622, 1.6197188] 2018-05-23 17:51:45.612823\n",
      "14100 2.9815245 [3.2280984, 1.6035318, 1.6245666] 2018-05-23 17:52:20.911943\n",
      "14200 2.961947 [3.1207247, 1.554895, 1.5658295] 2018-05-23 17:52:52.015974\n",
      "14300 2.8720345 [3.1689835, 1.5826862, 1.5862972] 2018-05-23 17:53:24.607686\n",
      "14400 2.8325105 [2.6109972, 1.3077197, 1.3032776] 2018-05-23 17:53:58.691956\n",
      "14500 2.8856833 [3.3123374, 1.6475868, 1.6647507] 2018-05-23 17:54:28.226643\n",
      "14600 2.7955925 [3.2385662, 1.6159418, 1.6226244] 2018-05-23 17:55:00.932051\n",
      "14700 2.8221498 [3.5243073, 1.7752461, 1.749061] 2018-05-23 17:55:28.305768\n",
      "14800 2.9679587 [3.1784327, 1.5852456, 1.5931871] 2018-05-23 17:56:05.423639\n",
      "14900 2.908326 [2.8534536, 1.4157498, 1.437704] 2018-05-23 17:56:41.726535\n",
      "Skipped batch, length 22191\n",
      "15000 2.8430548 [2.9215994, 1.3901874, 1.5314119] 2018-05-23 17:57:12.896821\n",
      "15100 2.9279418 [2.5123, 1.2536434, 1.2586567] 2018-05-23 17:57:41.003958\n",
      "15200 2.828697 [2.5032325, 1.2467014, 1.2565312] 2018-05-23 17:58:23.380462\n",
      "15300 2.971054 [3.3645673, 1.6740614, 1.6905059] 2018-05-23 17:58:55.164787\n",
      "15400 2.845667 [2.7217903, 1.3566035, 1.3651869] 2018-05-23 17:59:33.107193\n",
      "15500 2.8297153 [2.9951751, 1.48546, 1.5097151] 2018-05-23 18:00:04.020886\n",
      "15600 2.9389172 [2.9645932, 1.4795711, 1.4850221] 2018-05-23 18:00:42.588744\n",
      "15700 2.8287573 [3.027788, 1.5062169, 1.521571] 2018-05-23 18:01:20.025216\n",
      "15800 2.8621523 [2.9034126, 1.441601, 1.4618115] 2018-05-23 18:01:52.365398\n",
      "15900 2.885895 [3.2419372, 1.6201627, 1.6217744] 2018-05-23 18:02:17.654579\n",
      "16000 2.9540243 [6.3746557, 3.051542, 3.3231134] 2018-05-23 18:02:49.169044\n",
      "16100 2.9020352 [3.0700884, 1.5260606, 1.5440279] 2018-05-23 18:03:28.489539\n",
      "16200 2.8487144 [3.0960202, 1.5469187, 1.5491014] 2018-05-23 18:03:58.258555\n",
      "16300 2.8204226 [3.0480273, 1.5154878, 1.5325395] 2018-05-23 18:04:34.418791\n",
      "16400 2.9311633 [3.0484183, 1.5217582, 1.5266601] 2018-05-23 18:05:03.168176\n",
      "16500 2.9549692 [3.128696, 1.5544442, 1.5742518] 2018-05-23 18:05:39.326749\n",
      "16600 2.9684124 [2.9259083, 1.4539921, 1.4719162] 2018-05-23 18:06:16.118838\n",
      "16700 2.838873 [3.1035075, 1.5362424, 1.5672653] 2018-05-23 18:06:44.946809\n",
      "16800 2.836404 [2.9892683, 1.4909632, 1.4983051] 2018-05-23 18:07:13.971104\n",
      "16900 2.768468 [3.204585, 1.5885687, 1.6160164] 2018-05-23 18:07:37.705260\n",
      "17000 2.8636074 [3.3053818, 1.6470389, 1.6583428] 2018-05-23 18:08:11.842576\n",
      "17100 2.8081164 [2.8324425, 1.4321818, 1.4002607] 2018-05-23 18:08:51.652124\n",
      "17200 2.963515 [3.341084, 1.6553165, 1.6857675] 2018-05-23 18:09:29.221524\n",
      "17300 2.831872 [3.2412426, 1.6231883, 1.6180544] 2018-05-23 18:09:59.126601\n",
      "17400 2.8516064 [3.1817112, 1.583808, 1.5979031] 2018-05-23 18:10:27.683105\n",
      "17500 2.8597095 [2.934007, 1.4567946, 1.4772123] 2018-05-23 18:10:55.553437\n",
      "17600 2.9061937 [2.8853416, 1.4277688, 1.4575727] 2018-05-23 18:11:27.943687\n",
      "17700 2.8800378 [3.510282, 1.7440281, 1.7662541] 2018-05-23 18:12:00.479029\n",
      "17800 2.8414516 [2.6183505, 1.329027, 1.2893236] 2018-05-23 18:12:40.869603\n",
      "17900 2.9169173 [2.7496624, 1.375082, 1.3745803] 2018-05-23 18:13:14.427043\n",
      "18000 2.7715604 [2.467599, 1.2812095, 1.1863896] 2018-05-23 18:13:49.346358\n",
      "18100 2.8514025 [2.4761934, 1.1764419, 1.2997515] 2018-05-23 18:14:19.778128\n",
      "18200 2.8674154 [2.67372, 1.33936, 1.3343599] 2018-05-23 18:14:56.123887\n",
      "18300 2.7713294 [1.638729, 0.90858376, 0.7301452] 2018-05-23 18:15:29.109474\n",
      "18400 2.752258 [2.4420748, 1.21382, 1.2282548] 2018-05-23 18:15:54.326990\n",
      "18500 2.9713802 [2.825899, 1.3928556, 1.4330432] 2018-05-23 18:16:34.371125\n",
      "18600 2.7627294 [2.8749151, 1.4358646, 1.4390506] 2018-05-23 18:17:04.860727\n",
      "18700 2.8243122 [2.7453935, 1.3757386, 1.3696549] 2018-05-23 18:17:45.435003\n",
      "18800 2.787229 [3.0577629, 1.5126414, 1.5451214] 2018-05-23 18:18:16.012842\n",
      "Skipped batch, length 20989\n",
      "18900 2.760882 [2.9588614, 1.4289981, 1.5298634] 2018-05-23 18:18:58.731965\n",
      "19000 2.7556891 [3.0803194, 1.5375888, 1.5427306] 2018-05-23 18:19:34.506888\n",
      "19100 2.9110062 [3.2867906, 1.6469517, 1.6398389] 2018-05-23 18:20:15.692349\n",
      "19200 2.8787766 [3.174335, 1.5735171, 1.600818] 2018-05-23 18:20:45.467703\n",
      "19300 2.852498 [2.9851713, 1.4876986, 1.4974728] 2018-05-23 18:21:18.925172\n",
      "19400 2.9289505 [2.798822, 1.4077998, 1.391022] 2018-05-23 18:21:52.932886\n",
      "19500 2.7198794 [3.2507157, 1.6096904, 1.6410254] 2018-05-23 18:22:23.049052\n",
      "19600 2.815565 [3.3289957, 1.6156598, 1.713336] 2018-05-23 18:22:47.208979\n",
      "19700 2.804953 [3.1038647, 1.5431067, 1.560758] 2018-05-23 18:23:18.804664\n",
      "19800 2.6499822 [3.2106905, 1.5967686, 1.6139219] 2018-05-23 18:23:48.853215\n",
      "19900 2.8996868 [2.8592153, 1.4344546, 1.4247608] 2018-05-23 18:24:18.054536\n",
      "20000 2.9644315 [2.5134406, 1.2620119, 1.2514286] 2018-05-23 18:24:55.862546\n",
      "20100 2.7824054 [3.0272365, 1.5408653, 1.4863712] 2018-05-23 18:25:27.608836\n",
      "20200 2.7502275 [2.6885946, 1.3391757, 1.3494189] 2018-05-23 18:25:54.029897\n",
      "20300 2.7691622 [2.9148617, 1.4485117, 1.4663501] 2018-05-23 18:26:19.993381\n",
      "20400 2.753622 [2.6428144, 1.319387, 1.3234274] 2018-05-23 18:26:57.554576\n",
      "20500 2.8481421 [2.686026, 1.3469664, 1.3390598] 2018-05-23 18:27:33.168251\n",
      "20600 2.8159058 [1.6966364, 0.92536145, 0.7712749] 2018-05-23 18:28:03.951567\n",
      "20700 2.899753 [2.9273994, 1.4571805, 1.4702189] 2018-05-23 18:28:41.180868\n",
      "20800 2.8573914 [3.1466565, 1.5636414, 1.583015] 2018-05-23 18:29:11.627323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20900 2.8068328 [2.6313362, 1.2966602, 1.334676] 2018-05-23 18:29:45.952426\n",
      "21000 2.878341 [1.7544699, 0.9703971, 0.78407276] 2018-05-23 18:30:21.137990\n",
      "21100 2.8816674 [1.6041582, 0.92306626, 0.6810919] 2018-05-23 18:30:52.954394\n",
      "21200 2.8243985 [3.0446267, 1.5156006, 1.5290263] 2018-05-23 18:31:24.748590\n",
      "21300 2.8191636 [1.1778492, 0.6626178, 0.5152314] 2018-05-23 18:31:54.392354\n",
      "21400 2.6060743 [1.4963933, 0.8541317, 0.6422616] 2018-05-23 18:32:19.708973\n",
      "21500 2.8075244 [2.328748, 1.1523037, 1.1764443] 2018-05-23 18:32:48.491491\n",
      "21600 2.8875616 [3.0485713, 1.5203786, 1.5281928] 2018-05-23 18:33:16.912760\n",
      "21700 2.7561092 [2.972653, 1.4839652, 1.4886878] 2018-05-23 18:33:49.260063\n",
      "Skipped batch, length 32374\n",
      "21800 2.905345 [3.0876584, 1.535918, 1.5517404] 2018-05-23 18:34:28.832313\n",
      "21900 2.846782 [3.2246585, 1.6071182, 1.6175401] 2018-05-23 18:35:02.490965\n",
      "22000 2.8912508 [3.0341904, 1.5079641, 1.5262263] 2018-05-23 18:35:32.050238\n",
      "22100 2.847718 [3.0423565, 1.5117183, 1.5306381] 2018-05-23 18:36:08.528003\n",
      "22200 2.8455267 [3.0782635, 1.5379837, 1.5402799] 2018-05-23 18:36:38.755219\n",
      "22300 2.8291225 [2.9509532, 1.4522564, 1.4986968] 2018-05-23 18:37:10.420241\n",
      "22400 2.8186963 [2.9188442, 1.4538805, 1.4649636] 2018-05-23 18:37:40.530391\n",
      "22500 2.8597622 [2.6876824, 1.3322868, 1.3553956] 2018-05-23 18:38:10.716458\n",
      "22600 2.8975055 [3.13601, 1.5577044, 1.5783055] 2018-05-23 18:38:39.849342\n",
      "22700 2.9110086 [2.767912, 1.3849835, 1.3829284] 2018-05-23 18:39:22.985334\n",
      "22800 2.8830426 [3.0595088, 1.5171667, 1.542342] 2018-05-23 18:40:08.322074\n",
      "22900 2.8253381 [3.1314638, 1.5448403, 1.5866234] 2018-05-23 18:40:40.471756\n",
      "23000 2.8406549 [1.2134948, 0.705366, 0.5081287] 2018-05-23 18:41:08.892790\n",
      "23100 2.8309045 [0.9765213, 0.6183844, 0.3581369] 2018-05-23 18:41:52.295467\n",
      "23200 2.7489908 [3.1112041, 1.5288895, 1.5823145] 2018-05-23 18:42:14.209144\n",
      "23300 2.7485642 [2.673583, 1.318435, 1.3551481] 2018-05-23 18:42:44.779269\n",
      "23400 2.762789 [2.7526264, 1.3755307, 1.3770958] 2018-05-23 18:43:17.569243\n",
      "23500 2.8322 [3.0449648, 1.5189757, 1.5259889] 2018-05-23 18:43:50.205763\n",
      "23600 2.7063763 [3.171522, 1.5783293, 1.5931926] 2018-05-23 18:44:21.009121\n",
      "23700 2.887816 [2.8440356, 1.412388, 1.4316475] 2018-05-23 18:44:59.273157\n",
      "23800 2.9274883 [3.0735638, 1.5642955, 1.5092683] 2018-05-23 18:45:32.627707\n",
      "23900 2.901218 [1.3645232, 0.7787434, 0.58577985] 2018-05-23 18:46:07.024370\n",
      "24000 2.8219397 [3.2334926, 1.6083555, 1.6251371] 2018-05-23 18:46:42.688115\n",
      "24100 2.8235493 [3.1183672, 1.5448339, 1.5735333] 2018-05-23 18:47:15.067930\n",
      "24200 2.786651 [2.4267128, 1.2100692, 1.2166436] 2018-05-23 18:47:52.284729\n",
      "24300 2.8226821 [3.0516207, 1.6436281, 1.4079926] 2018-05-23 18:48:27.004880\n",
      "24400 2.7968862 [2.486715, 1.2318792, 1.2548358] 2018-05-23 18:48:58.656331\n",
      "24500 2.793823 [3.1252942, 1.5492316, 1.5760624] 2018-05-23 18:49:26.438191\n",
      "24600 2.780511 [2.8399272, 1.4270966, 1.4128306] 2018-05-23 18:50:03.478539\n",
      "24700 2.746089 [2.375721, 1.1780279, 1.1976931] 2018-05-23 18:50:32.536092\n",
      "24800 2.81479 [3.074789, 1.5215001, 1.5532889] 2018-05-23 18:50:59.334861\n",
      "24900 2.8991609 [2.7272248, 1.3600827, 1.367142] 2018-05-23 18:51:35.235779\n",
      "25000 2.7399085 [2.9998555, 1.4907291, 1.5091263] 2018-05-23 18:52:03.283850\n",
      "25100 2.840816 [3.3121104, 1.6569365, 1.6551738] 2018-05-23 18:52:35.676826\n",
      "25200 2.7061117 [1.0610929, 0.63430023, 0.42679268] 2018-05-23 18:53:08.578286\n",
      "25300 2.9120274 [2.4907823, 1.2481077, 1.2426746] 2018-05-23 18:53:44.045302\n",
      "25400 2.7481418 [2.5531945, 1.2689278, 1.2842668] 2018-05-23 18:54:11.161182\n",
      "25500 2.753415 [3.0240822, 1.5027394, 1.5213428] 2018-05-23 18:54:37.817306\n",
      "25600 2.802302 [2.6135778, 1.2994959, 1.3140819] 2018-05-23 18:55:08.579999\n",
      "25700 2.89116 [2.6750817, 1.3402051, 1.3348768] 2018-05-23 18:55:51.802836\n",
      "25800 2.7987864 [2.9474325, 1.4645412, 1.4828912] 2018-05-23 18:56:27.532577\n",
      "25900 2.8744776 [2.4966297, 1.2377305, 1.2588992] 2018-05-23 18:56:58.663979\n",
      "26000 2.7959442 [3.1193058, 1.5450296, 1.5742762] 2018-05-23 18:57:38.017777\n",
      "26100 2.827514 [3.0566907, 1.5144739, 1.5422168] 2018-05-23 18:58:13.660859\n",
      "26200 2.7563763 [2.7663872, 1.3897578, 1.3766295] 2018-05-23 18:58:42.606050\n",
      "26300 2.7686646 [2.650537, 1.3207767, 1.3297603] 2018-05-23 18:59:10.294694\n",
      "26400 2.8852324 [3.0520263, 1.5164413, 1.5355848] 2018-05-23 18:59:48.244379\n",
      "26500 2.7759082 [3.1424966, 1.5650629, 1.5774338] 2018-05-23 19:00:21.113154\n",
      "26600 2.8380115 [2.9850087, 1.584729, 1.4002796] 2018-05-23 19:00:58.723637\n",
      "26700 2.9008822 [3.170247, 1.5870383, 1.5832088] 2018-05-23 19:01:28.752855\n",
      "26800 2.8129828 [1.5600166, 0.9012953, 0.6587213] 2018-05-23 19:02:03.631491\n",
      "26900 2.8566566 [2.5931036, 1.2803102, 1.3127935] 2018-05-23 19:02:39.182185\n",
      "27000 2.8421857 [3.1743932, 1.5902288, 1.5841643] 2018-05-23 19:03:23.009991\n",
      "27100 2.8156528 [2.3993726, 1.1860425, 1.21333] 2018-05-23 19:03:55.226699\n",
      "27200 2.8241591 [3.1034267, 1.5372639, 1.5661628] 2018-05-23 19:04:50.950788\n",
      "27300 2.8471005 [2.5285378, 1.2432288, 1.2853088] 2018-05-23 19:05:31.308325\n",
      "27400 2.735935 [2.3969212, 1.1864495, 1.2104717] 2018-05-23 19:06:00.921617\n",
      "27500 2.696905 [2.7058246, 1.3450176, 1.3608071] 2018-05-23 19:06:28.861331\n",
      "27600 2.8254526 [2.0357137, 1.0863407, 0.9493729] 2018-05-23 19:07:02.123915\n",
      "27700 2.814466 [2.6507874, 1.3153043, 1.3354832] 2018-05-23 19:07:45.173147\n",
      "27800 2.8181772 [2.553594, 1.2993586, 1.2542355] 2018-05-23 19:08:18.880857\n",
      "27900 2.8201885 [2.795063, 1.395675, 1.3993881] 2018-05-23 19:08:54.425419\n",
      "28000 2.7125452 [1.3558316, 0.78145576, 0.5743758] 2018-05-23 19:09:22.341758\n",
      "28100 2.7657158 [1.5803955, 0.8881147, 0.6922807] 2018-05-23 19:09:54.688486\n",
      "28200 2.82518 [2.5970845, 1.2843351, 1.3127494] 2018-05-23 19:10:22.169272\n",
      "28300 2.73299 [3.2451086, 1.612124, 1.6329848] 2018-05-23 19:10:49.942990\n",
      "28400 2.8576145 [2.5767431, 1.2746382, 1.302105] 2018-05-23 19:11:27.688437\n",
      "Skipped batch, length 24384\n",
      "28500 2.8608518 [2.937498, 1.458062, 1.4794362] 2018-05-23 19:12:07.536903\n",
      "28600 2.670684 [2.4016016, 1.1891854, 1.2124162] 2018-05-23 19:12:32.703576\n",
      "28700 2.776412 [2.665292, 1.3172293, 1.3480628] 2018-05-23 19:13:10.916244\n",
      "28800 2.7918458 [3.0587413, 1.5121669, 1.5465745] 2018-05-23 19:13:37.899857\n",
      "28900 2.7712893 [2.7528267, 1.3793504, 1.3734763] 2018-05-23 19:14:11.595606\n",
      "29000 2.750069 [3.00075, 1.4978297, 1.5029204] 2018-05-23 19:14:37.137728\n",
      "29100 2.9354792 [3.3075027, 1.6487894, 1.6587135] 2018-05-23 19:15:15.088332\n",
      "29200 2.7273364 [2.4674163, 1.2138039, 1.2536125] 2018-05-23 19:15:41.604601\n",
      "29300 2.8653324 [2.536767, 1.2718011, 1.2649659] 2018-05-23 19:16:18.670362\n",
      "29400 2.8008056 [2.5809035, 1.2619942, 1.3189092] 2018-05-23 19:17:03.453598\n",
      "29500 2.8555377 [1.5454879, 0.8678496, 0.67763823] 2018-05-23 19:17:35.248243\n",
      "29600 2.7180228 [2.5394182, 1.2541647, 1.2852534] 2018-05-23 19:18:02.145479\n",
      "29700 2.762279 [2.703527, 1.3385364, 1.3649907] 2018-05-23 19:18:28.110418\n",
      "29800 2.8972864 [2.7526433, 1.3765215, 1.3761219] 2018-05-23 19:19:05.349806\n",
      "29900 2.7533777 [1.5464597, 0.87947893, 0.66698074] 2018-05-23 19:19:39.937368\n",
      "30000 2.8035183 [2.9655275, 1.4688171, 1.4967105] 2018-05-23 19:20:14.256209\n",
      "30100 2.765392 [3.2353258, 1.6071035, 1.6282222] 2018-05-23 19:20:44.005337\n",
      "30200 2.7187796 [2.8820932, 1.4331067, 1.4489865] 2018-05-23 19:21:13.657455\n",
      "30300 2.7357266 [2.9983711, 1.489656, 1.5087152] 2018-05-23 19:21:45.927640\n",
      "30400 2.7560859 [2.8855395, 1.4094248, 1.4761147] 2018-05-23 19:22:10.592126\n",
      "30500 2.802183 [2.6956081, 1.3361179, 1.3594904] 2018-05-23 19:22:47.992404\n",
      "30600 2.7567477 [2.4015994, 1.1901597, 1.2114398] 2018-05-23 19:23:11.652116\n",
      "30700 2.786665 [2.9572587, 1.4732301, 1.4840286] 2018-05-23 19:23:44.658569\n",
      "30800 2.803205 [3.154892, 1.5814877, 1.5734042] 2018-05-23 19:24:17.815297\n",
      "30900 2.8570893 [2.8066902, 1.3977154, 1.4089748] 2018-05-23 19:24:54.784456\n",
      "31000 2.7944367 [2.8133047, 1.3952644, 1.4180403] 2018-05-23 19:25:32.432891\n",
      "31100 2.822175 [3.256473, 1.6234494, 1.6330236] 2018-05-23 19:26:22.765225\n",
      "31200 2.8439875 [3.1045783, 1.5356615, 1.5689168] 2018-05-23 19:26:57.911086\n",
      "31300 2.741702 [2.7577856, 1.3649471, 1.3928385] 2018-05-23 19:27:25.791836\n",
      "31400 2.7264807 [3.2664242, 1.624661, 1.6417633] 2018-05-23 19:28:05.634048\n",
      "31500 2.7707062 [2.5348716, 1.2548018, 1.28007] 2018-05-23 19:28:41.189970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31600 2.737602 [3.006531, 1.4970173, 1.5095137] 2018-05-23 19:29:09.208887\n",
      "31700 2.7670226 [2.7971578, 1.3965813, 1.4005764] 2018-05-23 19:29:45.714077\n",
      "31800 2.8160357 [3.190215, 1.5836629, 1.6065521] 2018-05-23 19:30:27.990501\n",
      "31900 2.7840905 [2.9488983, 1.5458876, 1.4030107] 2018-05-23 19:30:56.969443\n",
      "32000 2.7721844 [3.2315612, 1.5963109, 1.6352503] 2018-05-23 19:31:25.478173\n",
      "32100 2.6198914 [3.1019554, 1.5396512, 1.5623043] 2018-05-23 19:31:53.640853\n",
      "32200 2.8704348 [2.0147614, 0.98178715, 1.0329742] 2018-05-23 19:32:24.914889\n",
      "32300 2.7736185 [3.2943397, 1.6414855, 1.6528542] 2018-05-23 19:32:58.485450\n",
      "32400 2.8488722 [2.9025712, 1.4470059, 1.4555653] 2018-05-23 19:33:31.810485\n",
      "32500 2.732981 [2.9500506, 1.4550204, 1.4950302] 2018-05-23 19:33:57.661622\n",
      "32600 2.7396898 [3.180264, 1.6116728, 1.5685912] 2018-05-23 19:34:24.855176\n",
      "32700 2.8495884 [3.1251636, 1.5487981, 1.5763655] 2018-05-23 19:35:03.664602\n",
      "32800 2.8428495 [2.6744359, 1.339459, 1.3349769] 2018-05-23 19:35:51.567875\n",
      "32900 2.7930071 [2.945582, 1.4654185, 1.4801633] 2018-05-23 19:36:19.890557\n",
      "33000 2.7913342 [3.324346, 1.6531022, 1.6712439] 2018-05-23 19:36:57.639813\n",
      "33100 2.6783774 [2.7700534, 1.3754327, 1.3946207] 2018-05-23 19:37:22.443884\n",
      "33200 2.7749097 [2.9320087, 1.4584382, 1.4735705] 2018-05-23 19:37:58.508514\n",
      "33300 2.8282075 [2.474277, 1.2414633, 1.2328138] 2018-05-23 19:38:27.096990\n",
      "33400 2.7245746 [3.2481954, 1.606466, 1.6417294] 2018-05-23 19:38:55.592245\n",
      "33500 2.7895532 [2.8277514, 1.4090556, 1.4186958] 2018-05-23 19:39:25.640521\n",
      "33600 2.8822012 [3.141935, 1.5609752, 1.5809599] 2018-05-23 19:39:53.132626\n",
      "33700 2.7273986 [1.0105444, 0.6312214, 0.37932304] 2018-05-23 19:40:20.718080\n",
      "33800 2.896798 [2.998537, 1.4950705, 1.5034666] 2018-05-23 19:40:54.472550\n",
      "33900 2.7921379 [2.7615914, 1.3800609, 1.3815304] 2018-05-23 19:41:34.247113\n",
      "34000 2.6979144 [2.4806347, 1.2317011, 1.2489337] 2018-05-23 19:42:00.767974\n",
      "34100 2.8185828 [2.8475473, 1.4102414, 1.4373059] 2018-05-23 19:42:34.014165\n",
      "34200 2.657938 [2.5540385, 1.2627573, 1.2912811] 2018-05-23 19:43:04.304315\n",
      "34300 2.7548995 [2.8199584, 1.4033337, 1.4166248] 2018-05-23 19:43:40.829789\n",
      "34400 2.7440622 [2.8802648, 1.4265313, 1.4537334] 2018-05-23 19:44:26.223043\n",
      "34500 2.765156 [2.8076851, 1.39627, 1.4114151] 2018-05-23 19:45:07.021955\n",
      "34600 2.7968955 [2.4193454, 1.2115641, 1.2077813] 2018-05-23 19:45:40.309027\n",
      "34700 2.7719762 [2.8267117, 1.4066015, 1.4201102] 2018-05-23 19:46:15.347312\n",
      "34800 2.779846 [2.8344815, 1.4136302, 1.4208512] 2018-05-23 19:46:46.457135\n",
      "34900 2.9155412 [5.60376, 2.8280628, 2.7756972] 2018-05-23 19:47:27.728397\n",
      "Skipped batch, length 87498\n",
      "35000 2.751802 [2.7003984, 1.3468347, 1.3535637] 2018-05-23 19:48:02.994847\n",
      "35100 2.7512243 [2.909615, 1.4504724, 1.4591427] 2018-05-23 19:48:42.043920\n",
      "35200 2.761356 [2.8053312, 1.3782701, 1.4270611] 2018-05-23 19:49:07.657414\n",
      "35300 2.7400208 [3.1907942, 1.5866091, 1.6041851] 2018-05-23 19:49:45.959533\n",
      "35400 2.7160382 [2.7973733, 1.3866796, 1.4106936] 2018-05-23 19:50:22.488818\n",
      "35500 2.7128925 [2.9082782, 1.476435, 1.4318433] 2018-05-23 19:50:49.989913\n",
      "35600 2.7180064 [2.8062596, 1.4000007, 1.4062588] 2018-05-23 19:51:17.371390\n",
      "35700 2.7194312 [2.9860268, 1.4835647, 1.5024621] 2018-05-23 19:51:50.578729\n",
      "35800 2.7733326 [3.110276, 1.5534513, 1.5568247] 2018-05-23 19:52:23.697775\n",
      "35900 2.760557 [2.892998, 1.433291, 1.459707] 2018-05-23 19:52:58.589352\n",
      "36000 2.7399402 [1.4976052, 0.8436279, 0.6539773] 2018-05-23 19:53:25.654178\n",
      "36100 2.790835 [2.382052, 1.2017217, 1.1803302] 2018-05-23 19:53:59.611455\n",
      "36200 2.7679582 [2.5049233, 1.2394326, 1.2654907] 2018-05-23 19:54:29.422582\n",
      "36300 2.7260652 [3.758739, 1.8646855, 1.8940535] 2018-05-23 19:54:54.319838\n",
      "36400 2.8126497 [3.0251272, 1.5145614, 1.5105658] 2018-05-23 19:55:40.007472\n",
      "Skipped batch, length 17272\n",
      "36500 2.7821267 [2.363632, 1.1816008, 1.1820312] 2018-05-23 19:56:14.805010\n",
      "36600 2.737112 [1.079855, 0.67198944, 0.40786558] 2018-05-23 19:56:47.606258\n",
      "36700 2.7432644 [2.036156, 1.1564057, 0.87975025] 2018-05-23 19:57:23.857061\n",
      "36800 2.7429628 [2.5193033, 1.2528561, 1.2664471] 2018-05-23 19:57:56.628550\n",
      "36900 2.7387862 [3.7432156, 1.8669224, 1.8762932] 2018-05-23 19:58:27.225536\n",
      "37000 2.6348336 [2.6146865, 1.2907805, 1.323906] 2018-05-23 19:58:55.167688\n",
      "37100 2.7370827 [2.5871453, 1.2980455, 1.2890999] 2018-05-23 19:59:27.583451\n",
      "37200 2.8057325 [2.9247365, 1.4469562, 1.4777802] 2018-05-23 19:59:57.227972\n",
      "37300 2.7774343 [2.9381018, 1.4604902, 1.4776115] 2018-05-23 20:00:33.721370\n",
      "37400 2.7101257 [2.7155437, 1.3604566, 1.3550873] 2018-05-23 20:01:04.822444\n",
      "37500 2.7269335 [2.5162086, 1.2605448, 1.255664] 2018-05-23 20:01:35.859279\n",
      "37600 2.7850442 [2.736401, 1.363286, 1.3731151] 2018-05-23 20:02:22.296138\n",
      "37700 2.8030126 [2.8533618, 1.4219377, 1.4314241] 2018-05-23 20:03:00.127398\n",
      "37800 2.7910495 [2.39598, 1.1854969, 1.2104828] 2018-05-23 20:03:36.956915\n",
      "37900 2.7263377 [2.421632, 1.1969886, 1.2246435] 2018-05-23 20:04:11.004697\n",
      "38000 2.8568127 [5.9228067, 2.9849126, 2.9378939] 2018-05-23 20:04:45.870350\n",
      "38100 2.7173455 [3.6594014, 1.8166649, 1.8427366] 2018-05-23 20:05:24.505052\n",
      "38200 2.7298117 [3.049414, 1.5179156, 1.5314983] 2018-05-23 20:05:48.819523\n",
      "38300 2.7426536 [2.8918688, 1.4244645, 1.4674044] 2018-05-23 20:06:24.484393\n",
      "38400 2.7984986 [2.2001374, 1.0870128, 1.1131246] 2018-05-23 20:06:55.776210\n",
      "38500 2.8189123 [2.5554223, 1.4207779, 1.1346444] 2018-05-23 20:07:32.792405\n",
      "38600 2.753494 [2.63689, 1.3136677, 1.3232224] 2018-05-23 20:08:08.005175\n",
      "38700 2.678292 [2.8778408, 1.5262835, 1.3515573] 2018-05-23 20:08:34.978958\n",
      "38800 2.7859867 [2.4891791, 1.2328799, 1.2562993] 2018-05-23 20:09:07.413794\n",
      "38900 2.7931843 [3.2685285, 1.6300838, 1.6384445] 2018-05-23 20:09:46.996895\n",
      "39000 2.7811356 [2.942747, 1.4525429, 1.4902041] 2018-05-23 20:10:20.131007\n",
      "39100 2.6421041 [2.603238, 1.302716, 1.3005222] 2018-05-23 20:10:52.630448\n",
      "39200 2.7384582 [3.1108532, 1.5441376, 1.5667157] 2018-05-23 20:11:20.482927\n",
      "39300 2.7178674 [2.5664308, 1.2934759, 1.272955] 2018-05-23 20:11:58.306522\n",
      "39400 2.761113 [2.808547, 1.375072, 1.433475] 2018-05-23 20:12:30.749287\n",
      "39500 2.7133746 [3.1070461, 1.5431607, 1.5638855] 2018-05-23 20:13:01.316917\n",
      "39600 2.7755942 [1.0620855, 0.65261483, 0.40947068] 2018-05-23 20:13:34.852351\n",
      "39700 2.7590857 [2.78554, 1.3740293, 1.411511] 2018-05-23 20:14:09.796967\n",
      "39800 2.76375 [1.6857538, 0.9554279, 0.73032594] 2018-05-23 20:14:37.850726\n",
      "39900 2.8177848 [2.9410164, 1.461086, 1.4799304] 2018-05-23 20:15:13.840480\n",
      "3931 batches of second unsupervised\n",
      "0 2.8259053 [2.8259053, 1.4156349, 1.4102706] 2018-05-23 20:15:48.300157\n",
      "100 2.630759 [2.8861353, 1.4238307, 1.4623046] 2018-05-23 20:16:11.566130\n",
      "200 2.604561 [2.3784156, 1.1574941, 1.2209216] 2018-05-23 20:16:32.579118\n",
      "300 2.5404027 [2.5882835, 1.274458, 1.3138254] 2018-05-23 20:16:53.274542\n",
      "400 2.5454853 [2.688466, 1.3411928, 1.3472731] 2018-05-23 20:17:14.396057\n",
      "500 2.5418677 [2.7155626, 1.347579, 1.3679836] 2018-05-23 20:17:35.748929\n",
      "600 2.506353 [2.694966, 1.3432384, 1.3517277] 2018-05-23 20:18:05.551625\n",
      "700 2.475358 [2.4465816, 1.2293487, 1.217233] 2018-05-23 20:18:31.325701\n",
      "800 2.490106 [2.3479857, 1.1622111, 1.1857748] 2018-05-23 20:18:57.924430\n",
      "900 2.4701295 [2.4977531, 1.2416607, 1.2560923] 2018-05-23 20:19:19.627186\n",
      "1000 2.4022732 [2.739415, 1.3596578, 1.3797572] 2018-05-23 20:19:42.376927\n",
      "1100 2.427312 [2.748271, 1.3404367, 1.4078342] 2018-05-23 20:20:01.457213\n",
      "1200 2.3908963 [2.636506, 1.31351, 1.3229961] 2018-05-23 20:20:22.333430\n",
      "1300 2.4013348 [2.6438096, 1.3095613, 1.3342483] 2018-05-23 20:20:51.560368\n",
      "1400 2.430994 [2.4761596, 1.2304395, 1.2457199] 2018-05-23 20:21:17.426034\n",
      "1500 2.3681855 [2.4047205, 1.1821213, 1.2225993] 2018-05-23 20:21:38.758200\n",
      "1600 2.371895 [2.3236623, 1.1579323, 1.16573] 2018-05-23 20:22:01.505714\n",
      "1700 2.369657 [2.406399, 1.1896372, 1.2167618] 2018-05-23 20:22:23.528568\n",
      "1800 2.399126 [2.9192286, 1.4065223, 1.5127063] 2018-05-23 20:22:47.202266\n",
      "1900 2.362816 [2.1268313, 1.0503404, 1.0764909] 2018-05-23 20:23:09.221256\n",
      "2000 2.3548768 [2.2242923, 1.1093459, 1.1149462] 2018-05-23 20:23:31.640592\n",
      "2100 2.3823943 [2.4665828, 1.2152348, 1.251348] 2018-05-23 20:23:54.865935\n",
      "2200 2.391808 [2.491291, 1.2476714, 1.2436198] 2018-05-23 20:24:24.957338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2300 2.3302495 [1.9898002, 0.97406375, 1.0157365] 2018-05-23 20:24:46.033069\n",
      "2400 2.3631654 [2.2711358, 1.1096532, 1.1614825] 2018-05-23 20:25:07.630321\n",
      "2500 2.3389068 [2.5677822, 1.2809079, 1.2868743] 2018-05-23 20:25:28.653351\n",
      "2600 2.3669028 [2.0512645, 1.0222855, 1.0289791] 2018-05-23 20:25:50.276639\n",
      "2700 2.3713045 [2.2245388, 1.100406, 1.1241326] 2018-05-23 20:26:15.920936\n",
      "2800 2.3483474 [2.2540534, 1.1129279, 1.1411254] 2018-05-23 20:26:43.167975\n",
      "2900 2.3311925 [2.5484247, 1.2723987, 1.2760259] 2018-05-23 20:27:07.681475\n",
      "3000 2.3197417 [2.3405461, 1.1672847, 1.1732614] 2018-05-23 20:27:28.676076\n",
      "3100 2.35876 [2.392203, 1.1305058, 1.2616973] 2018-05-23 20:27:53.573149\n",
      "3200 2.2854276 [2.2717261, 1.1314754, 1.1402507] 2018-05-23 20:28:14.995729\n",
      "3300 2.3042421 [2.1562772, 1.0799166, 1.0763605] 2018-05-23 20:28:37.761062\n",
      "3400 2.3089182 [2.096661, 1.0543878, 1.0422733] 2018-05-23 20:29:01.581472\n",
      "3500 2.3097663 [2.368877, 1.1770291, 1.1918478] 2018-05-23 20:29:23.828976\n",
      "3600 2.302749 [2.3572457, 1.1677295, 1.1895162] 2018-05-23 20:29:45.785352\n",
      "3700 2.3123386 [2.0697803, 1.0321766, 1.0376039] 2018-05-23 20:30:09.680384\n",
      "3800 2.29814 [2.2273786, 1.1120547, 1.1153239] 2018-05-23 20:30:31.184232\n",
      "3900 2.3409748 [2.170025, 1.0751777, 1.0948474] 2018-05-23 20:30:54.490833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-23 20:31:00.388315\n",
      "Trained at 2018-05-23 20:48:44.886303\n",
      "TP 14602 FP 2432 FN 4727 F 0.8031240546709567 Precision 0.8572267230245392 Recall 0.7554451859899632\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-23 20:53:44.228132\n",
      "Trained at 2018-05-23 21:11:10.311192\n",
      "TP 15396 FP 2440 FN 3933 F 0.8285214583613615 Precision 0.8631980264633325 Recall 0.7965233586838429\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-23 21:16:10.459783\n",
      "Trained at 2018-05-23 21:33:38.191344\n",
      "TP 15778 FP 2427 FN 3551 F 0.8407310704960835 Precision 0.8666849766547652 Recall 0.816286409022712\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-23 21:38:39.104361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Dict 2018-05-23 21:47:38.500997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-23 21:55:37.558128\n",
      "TP 16207 FP 2632 FN 3122 F 0.8492454412072941 Precision 0.8602898243006529 Recall 0.8384810388535361\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-23 22:00:39.742429\n",
      "Trained at 2018-05-23 22:16:52.628571\n",
      "TP 16439 FP 2549 FN 2890 F 0.8580525615262156 Precision 0.8657573204128923 Recall 0.8504837291116975\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-23 22:21:54.739033\n",
      "Trained at 2018-05-23 22:38:06.739211\n",
      "TP 16031 FP 2115 FN 3298 F 0.8555570380253502 Precision 0.883445387413204 Recall 0.8293755496921724\n",
      "Epoch 6 start at 2018-05-23 22:43:12.825138\n",
      "Trained at 2018-05-23 22:59:25.187787\n",
      "TP 17282 FP 3061 FN 2047 F 0.8712442024601734 Precision 0.849530551049501 Recall 0.894096952765275\n",
      "Best so far\n",
      "Epoch 7 start at 2018-05-23 23:04:29.429883\n",
      "Trained at 2018-05-23 23:20:41.793289\n",
      "TP 17267 FP 3122 FN 2062 F 0.8694798328213909 Precision 0.8468782186473098 Recall 0.8933209167572042\n",
      "Epoch 8 start at 2018-05-23 23:25:45.976906\n",
      "Trained at 2018-05-23 23:41:58.950370\n",
      "TP 17230 FP 2976 FN 2099 F 0.8716327304919691 Precision 0.8527170147480946 Recall 0.8914066946039629\n",
      "Best so far\n",
      "Epoch 9 start at 2018-05-23 23:47:02.779378\n",
      "Trained at 2018-05-24 00:03:15.748250\n",
      "TP 17169 FP 3070 FN 2160 F 0.8678224828143954 Precision 0.8483126636691536 Recall 0.8882508148378084\n",
      "Epoch 10 start at 2018-05-24 00:08:22.588463\n",
      "Trained at 2018-05-24 00:24:35.393550\n",
      "TP 17208 FP 2990 FN 2121 F 0.8706959799630632 Precision 0.8519655411426874 Recall 0.8902685084587925\n",
      "Epoch 11 start at 2018-05-24 00:29:40.490242\n",
      "Trained at 2018-05-24 00:45:53.130124\n",
      "TP 17182 FP 2886 FN 2147 F 0.8722491560271086 Precision 0.8561889575443492 Recall 0.8889233793781365\n",
      "Best so far\n",
      "Epoch 12 start at 2018-05-24 00:50:59.255837\n",
      "Trained at 2018-05-24 01:07:12.737144\n",
      "TP 17205 FP 3113 FN 2124 F 0.8679092995686937 Precision 0.8467861009941924 Recall 0.8901133012571784\n",
      "Epoch 13 start at 2018-05-24 01:12:20.038037\n",
      "Trained at 2018-05-24 01:28:31.666155\n",
      "TP 17158 FP 2963 FN 2171 F 0.8698605830164765 Precision 0.8527409174494309 Recall 0.8876817217652232\n",
      "Epoch 14 start at 2018-05-24 01:33:39.108433\n",
      "Trained at 2018-05-24 01:49:52.292379\n",
      "TP 17185 FP 2969 FN 2144 F 0.8705012283767698 Precision 0.8526843306539644 Recall 0.8890785865797506\n",
      "Epoch 15 start at 2018-05-24 01:54:59.087415\n",
      "Trained at 2018-05-24 02:11:12.830368\n",
      "TP 17100 FP 2911 FN 2229 F 0.8693441789527199 Precision 0.8545300084953276 Recall 0.8846810492006829\n",
      "Epoch 16 start at 2018-05-24 02:16:18.598480\n",
      "Trained at 2018-05-24 02:32:31.639084\n",
      "TP 16854 FP 2588 FN 2475 F 0.8694127053725723 Precision 0.8668861228268696 Recall 0.8719540586683222\n",
      "Epoch 17 start at 2018-05-24 02:37:35.663266\n",
      "Trained at 2018-05-24 02:53:49.262716\n",
      "TP 16786 FP 2564 FN 2543 F 0.867964528555547 Precision 0.8674935400516796 Recall 0.868436028765068\n",
      "Epoch 18 start at 2018-05-24 02:58:54.482955\n",
      "Trained at 2018-05-24 03:15:07.323377\n",
      "TP 17212 FP 3034 FN 2117 F 0.8698420720151611 Precision 0.8501432381705029 Recall 0.890475451394278\n",
      "Epoch 19 start at 2018-05-24 03:20:12.856543\n",
      "Trained at 2018-05-24 03:36:24.988325\n",
      "TP 16994 FP 2729 FN 2335 F 0.8703267438287412 Precision 0.861633625716169 Recall 0.8791970614103161\n",
      "Epoch 20 start at 2018-05-24 03:41:30.972272\n",
      "Trained at 2018-05-24 03:57:43.646374\n",
      "TP 17236 FP 2900 FN 2093 F 0.8734828328899025 Precision 0.855979340484704 Recall 0.8917171090071913\n",
      "Best so far\n",
      "Epoch 21 start at 2018-05-24 04:02:48.664147\n",
      "Trained at 2018-05-24 04:19:01.003433\n",
      "TP 17206 FP 3010 FN 2123 F 0.8701985080288279 Precision 0.8511080332409973 Recall 0.8901650369910498\n",
      "Epoch 22 start at 2018-05-24 04:24:05.284784\n",
      "Trained at 2018-05-24 04:40:17.644203\n",
      "TP 17122 FP 2931 FN 2207 F 0.8695343050124422 Precision 0.8538373310726575 Recall 0.8858192353458534\n",
      "Epoch 23 start at 2018-05-24 04:45:25.516776\n",
      "Trained at 2018-05-24 05:01:37.253949\n",
      "TP 17181 FP 2934 FN 2148 F 0.8711591116519622 Precision 0.8541387024608501 Recall 0.8888716436442651\n",
      "Epoch 24 start at 2018-05-24 05:06:42.872494\n",
      "Trained at 2018-05-24 05:22:55.534601\n",
      "TP 17120 FP 3034 FN 2209 F 0.8672086720867209 Precision 0.8494591644338593 Recall 0.8857157638781106\n",
      "Epoch 25 start at 2018-05-24 05:28:04.663769\n",
      "Trained at 2018-05-24 05:44:16.511070\n",
      "TP 17049 FP 2853 FN 2280 F 0.8691595931788636 Precision 0.8566475731082304 Recall 0.8820425267732422\n",
      "Epoch 26 start at 2018-05-24 05:49:25.163267\n",
      "Trained at 2018-05-24 06:05:37.075394\n",
      "TP 17027 FP 2870 FN 2302 F 0.8681486768979758 Precision 0.8557571493189928 Recall 0.8809043406280718\n",
      "Epoch 27 start at 2018-05-24 06:10:42.994207\n",
      "Trained at 2018-05-24 06:26:54.587338\n",
      "TP 17109 FP 3099 FN 2220 F 0.8654677896653767 Precision 0.8466448931116389 Recall 0.8851466708055253\n",
      "Epoch 28 start at 2018-05-24 06:32:00.748810\n",
      "Trained at 2018-05-24 06:48:13.289549\n",
      "TP 17329 FP 3425 FN 2000 F 0.8646558391337974 Precision 0.8349715717452058 Recall 0.8965285322572301\n",
      "Epoch 29 start at 2018-05-24 06:53:21.178543\n",
      "Trained at 2018-05-24 07:09:33.685790\n",
      "TP 17041 FP 2980 FN 2288 F 0.8661245235069885 Precision 0.8511562858998052 Recall 0.8816286409022712\n"
     ]
    }
   ],
   "source": [
    "# as close as possible to original plus unsup plus dict\n",
    "from chemlistem import minimodel_cudnn\n",
    "nmm = minimodel_cudnn.MiniModelCuDNN()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patful_a61k31a61_noct_s.txt\", 40000, \"../catted_patabs_a61k31a61_noct_s.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-24 09:15:10.182486\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-24 12:18:46.763560\n",
      "Shufflesplit at 2018-05-24 12:18:46.764030\n",
      "Read annots at 2018-05-24 12:18:46.821007\n",
      "Read train seqs at 2018-05-24 12:18:47.109427\n",
      "Read test seqs at 2018-05-24 12:19:30.348367\n",
      "Corpus read at 2018-05-24 12:19:43.084714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make batches at 2018-05-24 12:19:47.296531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "../glove.6B.300d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-24 12:20:10.848654\n",
      "Trained at 2018-05-24 12:27:12.458268\n",
      "TP 9827 FP 3846 FN 9502 F 0.5955396642627719 Precision 0.7187157171067067 Recall 0.5084070567541\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-24 12:29:21.733366\n",
      "Trained at 2018-05-24 12:36:10.527121\n",
      "TP 13865 FP 3718 FN 5464 F 0.751246207195492 Precision 0.7885457544218848 Recall 0.7173159501267525\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-24 12:38:18.684986\n",
      "Trained at 2018-05-24 12:45:04.932661\n",
      "TP 15199 FP 3268 FN 4130 F 0.8042650015874696 Precision 0.823035685276439 Recall 0.7863314191111801\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-24 12:47:12.419680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Glove 2018-05-24 12:52:43.565637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-24 12:53:56.226037\n",
      "TP 15838 FP 3257 FN 3491 F 0.8243805954611702 Precision 0.8294317884262896 Recall 0.8193905530549951\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-24 12:56:05.216369\n",
      "Trained at 2018-05-24 13:02:31.954163\n",
      "TP 16378 FP 3418 FN 2951 F 0.8372140575079873 Precision 0.827338856334613 Recall 0.847327849345543\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-24 13:04:38.338865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Dict 2018-05-24 13:08:32.566526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-24 13:10:46.929803\n",
      "TP 16391 FP 3022 FN 2938 F 0.8461617882401528 Precision 0.8443311183227734 Recall 0.848000413885871\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-24 13:12:53.711662\n",
      "Trained at 2018-05-24 13:18:30.810213\n",
      "TP 16508 FP 3021 FN 2821 F 0.8496577281383499 Precision 0.8453069793640228 Recall 0.854053494748823\n",
      "Best so far\n",
      "Epoch 7 start at 2018-05-24 13:20:37.695605\n",
      "Trained at 2018-05-24 13:26:15.310241\n",
      "TP 16613 FP 3078 FN 2716 F 0.8515120451050743 Precision 0.843684932202529 Recall 0.8594857468053184\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-24 13:28:22.533658\n",
      "Trained at 2018-05-24 13:34:00.028523\n",
      "TP 16708 FP 3119 FN 2621 F 0.8534068852793952 Precision 0.8426892621173148 Recall 0.8644006415231\n",
      "Best so far\n",
      "Epoch 9 start at 2018-05-24 13:36:08.508930\n",
      "Trained at 2018-05-24 13:41:46.133108\n",
      "TP 16752 FP 3003 FN 2577 F 0.8572305802886091 Precision 0.8479878511769172 Recall 0.866677013813441\n",
      "Best so far\n",
      "Epoch 10 start at 2018-05-24 13:43:56.298020\n",
      "Trained at 2018-05-24 13:49:34.693825\n",
      "TP 16655 FP 3027 FN 2674 F 0.8538617313065545 Precision 0.8462046539985774 Recall 0.8616586476279166\n",
      "Epoch 11 start at 2018-05-24 13:51:42.313714\n",
      "Trained at 2018-05-24 13:57:19.670608\n",
      "TP 16582 FP 2871 FN 2747 F 0.8551389820019597 Precision 0.8524135094843983 Recall 0.8578819390553055\n",
      "Epoch 12 start at 2018-05-24 13:59:28.923421\n",
      "Trained at 2018-05-24 14:05:07.075305\n",
      "TP 16761 FP 3161 FN 2568 F 0.8540419352373188 Precision 0.841331191647425 Recall 0.8671426354182834\n",
      "Epoch 13 start at 2018-05-24 14:07:18.325528\n",
      "Trained at 2018-05-24 14:12:56.130374\n",
      "TP 16657 FP 3090 FN 2672 F 0.8525437608762412 Precision 0.8435205347647744 Recall 0.8617621190956594\n",
      "Epoch 14 start at 2018-05-24 14:15:07.094503\n",
      "Trained at 2018-05-24 14:20:44.118515\n",
      "TP 16805 FP 3146 FN 2524 F 0.8556517311608961 Precision 0.8423136684877951 Recall 0.8694190077086243\n",
      "Epoch 15 start at 2018-05-24 14:22:51.460346\n",
      "Trained at 2018-05-24 14:28:28.787439\n",
      "TP 16880 FP 3484 FN 2449 F 0.8505278008716902 Precision 0.828913769396975 Recall 0.8732991877489782\n",
      "Epoch 16 start at 2018-05-24 14:30:39.053246\n",
      "Trained at 2018-05-24 14:36:16.729783\n",
      "TP 16681 FP 3009 FN 2648 F 0.8550193495476562 Precision 0.8471813103098019 Recall 0.8630037767085726\n",
      "Epoch 17 start at 2018-05-24 14:38:26.107453\n",
      "Trained at 2018-05-24 14:44:04.295394\n",
      "TP 16666 FP 3026 FN 2663 F 0.8542067092078625 Precision 0.8463335364615072 Recall 0.8622277407005018\n",
      "Epoch 18 start at 2018-05-24 14:46:12.053584\n",
      "Trained at 2018-05-24 14:51:49.348520\n",
      "TP 16825 FP 3046 FN 2504 F 0.8584183673469388 Precision 0.846711287806351 Recall 0.870453722386052\n",
      "Best so far\n",
      "Epoch 19 start at 2018-05-24 14:53:57.234894\n",
      "Trained at 2018-05-24 14:59:35.449161\n",
      "TP 16762 FP 3115 FN 2567 F 0.8550732030811611 Precision 0.843286210192685 Recall 0.8671943711521548\n",
      "Epoch 20 start at 2018-05-24 15:01:44.089423\n",
      "Trained at 2018-05-24 15:07:21.532255\n",
      "TP 16877 FP 3239 FN 2452 F 0.8557231588287489 Precision 0.8389838934181746 Recall 0.8731439805473641\n",
      "Epoch 21 start at 2018-05-24 15:09:29.548407\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3a03530802c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneominimodel_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneominimodel_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPredNeoMiniModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../catted_patabs_a61k31a61_noct_s.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../glove.6B.300d.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/neominimodel_pred.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname, unsupfile, nunsup, bsize, glovefile, ngloveb)\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0;31m#model.fit(tx, ty, verbose=0, epochs=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#interleave dict training, with pretraining, 1 layer pretraining, also glove\n",
    "from chemlistem import neominimodel_pred\n",
    "nmm = neominimodel_pred.PredNeoMiniModel()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patabs_a61k31a61_noct_s.txt\", 0, 32, \"../glove.6B.300d.txt\", 4000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-24 15:12:05.538856\n",
      "Shufflesplit at 2018-05-24 15:12:05.539524\n",
      "Read annots at 2018-05-24 15:12:05.596502\n",
      "Read train seqs at 2018-05-24 15:12:05.886710\n",
      "Read test seqs at 2018-05-24 15:12:48.927312\n",
      "Corpus read at 2018-05-24 15:13:01.576709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make train dict at 2018-05-24 15:13:05.697171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "3931 batches of unsupervised\n",
      "0 9.022314 [9.022314, 4.5116525, 4.510661] 2018-05-24 15:13:18.183926\n",
      "100 5.9739003 [5.063637, 2.5086954, 2.5549412] 2018-05-24 15:13:40.055825\n",
      "200 4.8394785 [4.436985, 2.2281418, 2.2088432] 2018-05-24 15:14:00.879815\n",
      "300 4.241254 [3.522212, 1.7894292, 1.7327828] 2018-05-24 15:14:26.909977\n",
      "400 3.7323675 [4.063581, 2.0476072, 2.0159738] 2018-05-24 15:14:54.633439\n",
      "500 3.36322 [3.7437139, 1.8793659, 1.864348] 2018-05-24 15:15:24.960559\n",
      "600 3.0330844 [3.0528448, 1.5217023, 1.5311425] 2018-05-24 15:15:49.159008\n",
      "700 2.8531005 [3.193767, 1.5987226, 1.5950444] 2018-05-24 15:16:15.696740\n",
      "800 2.7451317 [2.4488754, 1.1899346, 1.2589409] 2018-05-24 15:16:46.660895\n",
      "900 2.6279297 [2.3731024, 1.1750951, 1.1980073] 2018-05-24 15:17:17.673924\n",
      "1000 2.5092788 [2.2637863, 1.1264038, 1.1373825] 2018-05-24 15:17:45.143665\n",
      "1100 2.4053123 [3.062097, 1.5195699, 1.5425273] 2018-05-24 15:18:11.809485\n",
      "1200 2.356109 [3.036242, 1.51657, 1.5196719] 2018-05-24 15:18:38.937239\n",
      "1300 2.311685 [1.911825, 0.92274284, 0.98908204] 2018-05-24 15:19:09.504839\n",
      "1400 2.266682 [2.1863203, 1.0808165, 1.1055037] 2018-05-24 15:19:33.760474\n",
      "1500 2.185746 [2.0577202, 0.9998095, 1.0579108] 2018-05-24 15:19:58.541197\n",
      "1600 2.1872833 [2.650959, 1.3172148, 1.333744] 2018-05-24 15:20:29.459361\n",
      "1700 2.1055813 [2.4506843, 1.2109106, 1.2397738] 2018-05-24 15:20:51.649441\n",
      "1800 2.083954 [1.8095719, 0.8949865, 0.91458535] 2018-05-24 15:21:15.674016\n",
      "1900 2.0564737 [2.7557893, 1.3757659, 1.3800232] 2018-05-24 15:21:42.651835\n",
      "2000 2.0289214 [1.5407916, 0.7404337, 0.80035794] 2018-05-24 15:22:10.144877\n",
      "2100 2.0082011 [2.05231, 1.011147, 1.041163] 2018-05-24 15:22:36.496289\n",
      "2200 1.9217584 [2.6939435, 1.3436108, 1.3503327] 2018-05-24 15:22:59.835275\n",
      "2300 1.9360604 [1.9483135, 0.96571875, 0.98259467] 2018-05-24 15:23:24.952143\n",
      "2400 1.927647 [1.7819397, 0.87618446, 0.9057553] 2018-05-24 15:23:52.319527\n",
      "2500 1.9558831 [1.7003748, 0.84377587, 0.85659903] 2018-05-24 15:24:20.853165\n",
      "2600 1.9251443 [1.897536, 0.9466026, 0.9509334] 2018-05-24 15:24:46.040025\n",
      "2700 1.85491 [1.9023991, 0.9435651, 0.95883405] 2018-05-24 15:25:10.304105\n",
      "2800 1.8995566 [1.9416926, 0.95512414, 0.98656845] 2018-05-24 15:25:42.115698\n",
      "2900 1.8483527 [2.102302, 1.0436434, 1.0586588] 2018-05-24 15:26:07.254298\n",
      "3000 1.8829265 [1.903091, 0.9290371, 0.9740538] 2018-05-24 15:26:33.167402\n",
      "3100 1.8162082 [2.0428593, 1.0187118, 1.0241475] 2018-05-24 15:27:01.293272\n",
      "3200 1.8273876 [1.862987, 0.9087278, 0.95425916] 2018-05-24 15:27:26.833983\n",
      "3300 1.8435397 [1.674948, 0.8351055, 0.83984256] 2018-05-24 15:27:59.279954\n",
      "3400 1.82536 [1.88033, 0.91650116, 0.9638288] 2018-05-24 15:28:23.823626\n",
      "3500 1.7604426 [1.7220068, 0.8524621, 0.8695447] 2018-05-24 15:28:49.748202\n",
      "3600 1.793345 [1.5083452, 0.74836075, 0.7599845] 2018-05-24 15:29:19.844928\n",
      "3700 1.7875798 [1.6757483, 0.8289699, 0.8467785] 2018-05-24 15:29:48.582203\n",
      "3800 1.8025604 [1.6860256, 0.81697285, 0.86905277] 2018-05-24 15:30:17.617722\n",
      "3900 1.7674948 [2.117793, 1.0646113, 1.0531816] 2018-05-24 15:30:45.968798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-24 15:30:51.812146\n",
      "Trained at 2018-05-24 15:51:55.881616\n",
      "TP 15251 FP 2444 FN 4078 F 0.8238439930855661 Precision 0.8618818875388528 Recall 0.7890216772724921\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-24 15:57:34.778425\n",
      "Trained at 2018-05-24 16:18:24.851543\n",
      "TP 15693 FP 2436 FN 3636 F 0.837898446259811 Precision 0.865629654145292 Recall 0.8118888716436443\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-24 16:24:06.635033\n",
      "Trained at 2018-05-24 16:44:56.632546\n",
      "TP 16238 FP 2493 FN 3091 F 0.8532842879663689 Precision 0.8669051305322727 Recall 0.8400848466035491\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-24 16:50:40.464392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Dict 2018-05-24 17:01:28.457276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-24 17:10:55.425383\n",
      "TP 16115 FP 2396 FN 3214 F 0.8517441860465116 Precision 0.8705634487601966 Recall 0.8337213513373687\n",
      "Epoch 4 start at 2018-05-24 17:16:36.290873\n",
      "Trained at 2018-05-24 17:35:56.867772\n",
      "TP 16530 FP 2523 FN 2799 F 0.8613412537126778 Precision 0.867579908675799 Recall 0.8551916808939934\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-24 17:41:40.255163\n",
      "Trained at 2018-05-24 18:01:01.763345\n",
      "TP 17229 FP 2983 FN 2100 F 0.8714498874585873 Precision 0.8524144072828023 Recall 0.8913549588700915\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-24 18:06:48.314173\n",
      "Trained at 2018-05-24 18:26:07.691319\n",
      "TP 17212 FP 3063 FN 2117 F 0.8692051307948692 Precision 0.8489272503082614 Recall 0.890475451394278\n",
      "Epoch 7 start at 2018-05-24 18:31:54.648738\n",
      "Trained at 2018-05-24 18:51:15.452424\n",
      "TP 17087 FP 2722 FN 2242 F 0.873166743318514 Precision 0.8625877126558635 Recall 0.8840084846603549\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-24 18:57:04.947810\n",
      "Trained at 2018-05-24 19:16:25.510079\n",
      "TP 16887 FP 2811 FN 2442 F 0.8654008763163964 Precision 0.8572951568687176 Recall 0.8736613378860779\n",
      "Epoch 9 start at 2018-05-24 19:22:10.592512\n",
      "Trained at 2018-05-24 19:41:30.714403\n",
      "TP 16953 FP 2672 FN 2376 F 0.8704112542999435 Precision 0.8638471337579617 Recall 0.8770758963215893\n",
      "Epoch 10 start at 2018-05-24 19:47:17.657277\n",
      "Trained at 2018-05-24 20:06:37.184610\n",
      "TP 17306 FP 3106 FN 2023 F 0.8709393321758385 Precision 0.8478346070938664 Recall 0.8953386103781882\n",
      "Epoch 11 start at 2018-05-24 20:12:25.505910\n",
      "Trained at 2018-05-24 20:31:45.681912\n",
      "TP 17184 FP 3020 FN 2145 F 0.8693496572483748 Precision 0.8505246485844388 Recall 0.8890268508458793\n",
      "Epoch 12 start at 2018-05-24 20:37:32.902564\n",
      "Trained at 2018-05-24 20:56:53.311168\n",
      "TP 17034 FP 2711 FN 2295 F 0.8718841173158622 Precision 0.8626994175740694 Recall 0.8812664907651715\n",
      "Epoch 13 start at 2018-05-24 21:02:38.487456\n",
      "Trained at 2018-05-24 21:21:58.589922\n",
      "TP 17112 FP 3004 FN 2217 F 0.8676384839650145 Precision 0.8506661364088288 Recall 0.8853018780071396\n",
      "Epoch 14 start at 2018-05-24 21:27:48.739171\n",
      "Trained at 2018-05-24 21:47:09.687801\n",
      "TP 17012 FP 2848 FN 2317 F 0.8682028120135752 Precision 0.8565961732124874 Recall 0.880128304620001\n",
      "Epoch 15 start at 2018-05-24 21:52:57.548142\n",
      "Trained at 2018-05-24 22:12:20.104497\n",
      "TP 17275 FP 3172 FN 2054 F 0.8686142397425584 Precision 0.8448672176847459 Recall 0.8937348026281753\n",
      "Epoch 16 start at 2018-05-24 22:18:09.494073\n",
      "Trained at 2018-05-24 22:37:32.763996\n",
      "TP 17060 FP 2821 FN 2269 F 0.8701861769956644 Precision 0.858105729088074 Recall 0.8826116198458275\n",
      "Epoch 17 start at 2018-05-24 22:43:23.123050\n",
      "Trained at 2018-05-24 23:02:46.557482\n",
      "TP 17367 FP 3213 FN 1962 F 0.8703300007517102 Precision 0.8438775510204082 Recall 0.8984944901443427\n",
      "Epoch 18 start at 2018-05-24 23:08:36.463128\n",
      "Trained at 2018-05-24 23:28:02.404278\n",
      "TP 17018 FP 2926 FN 2311 F 0.8666513889949838 Precision 0.8532892097874047 Recall 0.8804387190232293\n",
      "Epoch 19 start at 2018-05-24 23:33:50.955414\n",
      "Trained at 2018-05-24 23:53:13.262927\n",
      "TP 17117 FP 2980 FN 2212 F 0.8683102521178917 Precision 0.8517191620639897 Recall 0.8855605566764965\n",
      "Epoch 20 start at 2018-05-24 23:59:01.153943\n",
      "Trained at 2018-05-25 00:18:23.136192\n",
      "TP 17287 FP 3422 FN 2042 F 0.8635296468355063 Precision 0.8347578347578347 Recall 0.8943556314346319\n",
      "Epoch 21 start at 2018-05-25 00:24:12.978053\n",
      "Trained at 2018-05-25 00:43:32.597707\n",
      "TP 16670 FP 2729 FN 2659 F 0.8608758520966743 Precision 0.8593226454971906 Recall 0.8624346836359874\n",
      "Epoch 22 start at 2018-05-25 00:49:19.014444\n",
      "Trained at 2018-05-25 01:08:38.756077\n",
      "TP 16727 FP 2835 FN 2602 F 0.8601990177676069 Precision 0.8550761680809733 Recall 0.8653836204666563\n",
      "Epoch 23 start at 2018-05-25 01:14:26.271752\n",
      "Trained at 2018-05-25 01:33:45.148419\n",
      "TP 16501 FP 2481 FN 2828 F 0.8614236120174362 Precision 0.8692972289537456 Recall 0.8536913446117234\n",
      "Epoch 24 start at 2018-05-25 01:39:32.652940\n",
      "Trained at 2018-05-25 01:58:52.145306\n",
      "TP 16925 FP 2924 FN 2404 F 0.8640053091020471 Precision 0.8526877928359111 Recall 0.8756272957731905\n",
      "Epoch 25 start at 2018-05-25 02:04:38.364370\n",
      "Trained at 2018-05-25 02:23:57.895383\n",
      "TP 16852 FP 2977 FN 2477 F 0.8607181163491496 Precision 0.8498663573553886 Recall 0.8718505872005794\n",
      "Epoch 26 start at 2018-05-25 02:29:45.727370\n",
      "Trained at 2018-05-25 02:49:05.914895\n",
      "TP 16787 FP 2876 FN 2542 F 0.8610484201887567 Precision 0.8537354422010883 Recall 0.8684877644989394\n",
      "Epoch 27 start at 2018-05-25 02:54:54.431256\n",
      "Trained at 2018-05-25 03:14:12.566093\n",
      "TP 16869 FP 2878 FN 2460 F 0.8633944108915959 Precision 0.8542563427356054 Recall 0.872730094676393\n",
      "Epoch 28 start at 2018-05-25 03:20:01.705779\n",
      "Trained at 2018-05-25 03:39:21.975484\n",
      "TP 16904 FP 3029 FN 2425 F 0.8610870561866436 Precision 0.8480409371394171 Recall 0.8745408453618915\n",
      "Epoch 29 start at 2018-05-25 03:45:09.760571\n",
      "Trained at 2018-05-25 04:04:28.623974\n",
      "TP 16830 FP 2921 FN 2499 F 0.861310133060389 Precision 0.8521087539871399 Recall 0.8707124010554089\n"
     ]
    }
   ],
   "source": [
    "# base layer has 256 per direction\n",
    "from chemlistem import minimodel_cudnn\n",
    "nmm = minimodel_cudnn.MiniModelCuDNN()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patabs_a61k31a61_noct_s.txt\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-25 09:09:57.076742\n",
      "Shufflesplit at 2018-05-25 09:09:57.077140\n",
      "Read annots at 2018-05-25 09:09:57.133400\n",
      "Read train seqs at 2018-05-25 09:09:57.410667\n",
      "Read test seqs at 2018-05-25 09:10:40.284068\n",
      "Corpus read at 2018-05-25 09:10:52.807623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make train dict at 2018-05-25 09:10:57.047987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "3931 batches of unsupervised\n",
      "0 9.0223675 [9.0223675, 4.5108957, 4.5114717] 2018-05-25 09:11:09.481153\n",
      "100 5.990538 [5.0799747, 2.5271153, 2.5528593] 2018-05-25 09:11:31.271024\n",
      "200 4.8723216 [4.4760237, 2.2660172, 2.2100062] 2018-05-25 09:11:51.938701\n",
      "300 4.279651 [3.5522728, 1.8131292, 1.7391436] 2018-05-25 09:12:17.998711\n",
      "400 3.7721655 [4.0932465, 2.070378, 2.0228682] 2018-05-25 09:12:46.517858\n",
      "500 3.396821 [3.802576, 1.913567, 1.8890092] 2018-05-25 09:13:15.979051\n",
      "600 3.0593827 [3.0843463, 1.5476112, 1.536735] 2018-05-25 09:13:40.144165\n",
      "700 2.8751614 [3.2218738, 1.6241654, 1.5977085] 2018-05-25 09:14:06.604354\n",
      "800 2.7627592 [2.4817529, 1.2214818, 1.260271] 2018-05-25 09:14:37.607649\n",
      "900 2.6438031 [2.3745987, 1.192092, 1.1825068] 2018-05-25 09:15:08.667338\n",
      "1000 2.5216007 [2.2828279, 1.1367763, 1.1460516] 2018-05-25 09:15:36.128932\n",
      "1100 2.4142258 [3.0519242, 1.5169368, 1.5349873] 2018-05-25 09:16:02.880473\n",
      "1200 2.3663943 [3.0519829, 1.5289794, 1.5230035] 2018-05-25 09:16:30.966548\n",
      "1300 2.3203657 [1.9259682, 0.97070605, 0.95526206] 2018-05-25 09:17:00.802074\n",
      "1400 2.2743948 [2.1911867, 1.088012, 1.1031747] 2018-05-25 09:17:25.120021\n",
      "1500 2.1947646 [2.0645094, 1.0222591, 1.0422504] 2018-05-25 09:17:49.939090\n",
      "1600 2.1936345 [2.6447396, 1.3169495, 1.32779] 2018-05-25 09:18:21.693918\n",
      "1700 2.1087198 [2.463317, 1.2343434, 1.2289734] 2018-05-25 09:18:43.188130\n",
      "1800 2.0884016 [1.8047388, 0.8985644, 0.9061744] 2018-05-25 09:19:07.457195\n",
      "1900 2.0584354 [2.7801387, 1.3951523, 1.3849864] 2018-05-25 09:19:34.464968\n",
      "2000 2.0305333 [1.529857, 0.7335953, 0.7962617] 2018-05-25 09:20:02.962804\n",
      "2100 2.0115435 [2.0638325, 1.0299034, 1.0339291] 2018-05-25 09:20:29.298659\n",
      "2200 1.9228519 [2.687542, 1.344307, 1.3432351] 2018-05-25 09:20:51.823906\n",
      "2300 1.9384317 [1.9717011, 0.9769576, 0.9947435] 2018-05-25 09:21:16.845766\n",
      "2400 1.9320157 [1.770517, 0.8795542, 0.8909628] 2018-05-25 09:21:44.203137\n",
      "2500 1.9575194 [1.6950336, 0.84769547, 0.847338] 2018-05-25 09:22:13.677451\n",
      "2600 1.9287162 [1.9007326, 0.9499645, 0.9507681] 2018-05-25 09:22:39.103506\n",
      "2700 1.8551416 [1.9072752, 0.9499198, 0.95735544] 2018-05-25 09:23:02.532453\n",
      "2800 1.8983868 [1.9496579, 0.962394, 0.98726386] 2018-05-25 09:23:34.378199\n",
      "2900 1.8447182 [2.123467, 1.0598835, 1.0635834] 2018-05-25 09:23:59.520037\n",
      "3000 1.8808427 [1.8429573, 0.90139365, 0.9415636] 2018-05-25 09:24:25.459190\n",
      "3100 1.815723 [2.0617745, 1.0310972, 1.0306773] 2018-05-25 09:24:53.497711\n",
      "3200 1.8217682 [1.8676965, 0.906912, 0.96078444] 2018-05-25 09:25:19.045261\n",
      "3300 1.8417523 [1.6690629, 0.8326716, 0.83639127] 2018-05-25 09:25:51.577883\n",
      "3400 1.8265316 [1.9180026, 0.93067086, 0.9873317] 2018-05-25 09:26:16.113013\n",
      "3500 1.756438 [1.7131407, 0.8494258, 0.86371493] 2018-05-25 09:26:42.153281\n",
      "3600 1.790513 [1.50516, 0.744275, 0.760885] 2018-05-25 09:27:11.388548\n",
      "3700 1.7838475 [1.6683068, 0.80791944, 0.86038744] 2018-05-25 09:27:40.925397\n",
      "3800 1.7996492 [1.6650628, 0.81290233, 0.85216045] 2018-05-25 09:28:10.009471\n",
      "3900 1.7629881 [2.0969741, 1.0507483, 1.0462258] 2018-05-25 09:28:38.401333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-25 09:28:44.222223\n",
      "Trained at 2018-05-25 09:51:36.017526\n",
      "TP 15006 FP 2448 FN 4323 F 0.8159203980099502 Precision 0.8597456170505329 Recall 0.7763464224740028\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-25 09:57:32.138778\n",
      "Trained at 2018-05-25 10:20:07.821102\n",
      "TP 14883 FP 1994 FN 4446 F 0.822128928906811 Precision 0.8818510398767554 Recall 0.7699829272078225\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-25 10:26:05.145877\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1c5537279a0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminimodel_cudnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimodel_cudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMiniModelCuDNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../catted_patabs_a61k31a61_noct_s.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/minimodel_cudnn.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname, unsupfile, nunsup, unsupfile2)\u001b[0m\n\u001b[1;32m    353\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dictmodel_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictbatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_l_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wordn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# base layer has 256 per direction - 256/128/128\n",
    "from chemlistem import minimodel_cudnn\n",
    "nmm = minimodel_cudnn.MiniModelCuDNN()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patabs_a61k31a61_noct_s.txt\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-25 10:29:51.799283\n",
      "Shufflesplit at 2018-05-25 10:29:51.799409\n",
      "Read annots at 2018-05-25 10:29:51.864569\n",
      "Read train seqs at 2018-05-25 10:29:52.149115\n",
      "Read test seqs at 2018-05-25 10:30:35.116872\n",
      "Corpus read at 2018-05-25 10:30:47.687124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make train dict at 2018-05-25 10:30:51.835977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "3931 batches of unsupervised\n",
      "0 9.022513 [9.022513, 4.5099792, 4.512534] 2018-05-25 10:31:03.698680\n",
      "100 5.854102 [4.7806315, 2.3868942, 2.3937373] 2018-05-25 10:31:22.052540\n",
      "200 4.564499 [4.1771183, 2.083148, 2.0939703] 2018-05-25 10:31:39.032076\n",
      "300 4.0479245 [3.35091, 1.668713, 1.682197] 2018-05-25 10:32:00.526805\n",
      "400 3.6209023 [4.0039783, 1.9894886, 2.0144897] 2018-05-25 10:32:24.379612\n",
      "500 3.3208544 [3.7163832, 1.833749, 1.8826342] 2018-05-25 10:32:48.809445\n",
      "600 3.0426908 [3.08386, 1.5267739, 1.557086] 2018-05-25 10:33:08.857012\n",
      "700 2.9011233 [3.2525587, 1.6091533, 1.6434054] 2018-05-25 10:33:30.787114\n",
      "800 2.819614 [2.592159, 1.2439888, 1.3481703] 2018-05-25 10:33:57.236520\n",
      "900 2.7241077 [2.486425, 1.227068, 1.259357] 2018-05-25 10:34:22.130334\n",
      "1000 2.6279757 [2.413839, 1.181185, 1.2326541] 2018-05-25 10:34:45.837991\n",
      "1100 2.5397568 [3.1948528, 1.5837364, 1.6111164] 2018-05-25 10:35:07.873083\n",
      "1200 2.503632 [3.212305, 1.5829077, 1.6293975] 2018-05-25 10:35:30.173868\n",
      "1300 2.467195 [2.0646107, 0.99737525, 1.0672355] 2018-05-25 10:35:55.558844\n",
      "1400 2.4367003 [2.359503, 1.1648517, 1.1946514] 2018-05-25 10:36:15.671606\n",
      "1500 2.3592386 [2.2032418, 1.0881715, 1.1150703] 2018-05-25 10:36:36.479939\n",
      "1600 2.3801963 [2.8397045, 1.4073788, 1.4323258] 2018-05-25 10:37:02.086023\n",
      "1700 2.2945902 [2.6706595, 1.3248624, 1.3457971] 2018-05-25 10:37:20.741597\n",
      "1800 2.2846801 [1.9712939, 0.97407496, 0.99721897] 2018-05-25 10:37:40.761285\n",
      "1900 2.2574441 [3.0175655, 1.4998407, 1.5177248] 2018-05-25 10:38:03.076046\n",
      "2000 2.2381642 [1.7292001, 0.8459474, 0.8832528] 2018-05-25 10:38:26.724610\n",
      "2100 2.2148068 [2.2501173, 1.1224389, 1.1276785] 2018-05-25 10:38:48.559353\n",
      "2200 2.1341207 [3.0016537, 1.4875679, 1.5140858] 2018-05-25 10:39:07.094291\n",
      "2300 2.1531415 [2.1526465, 1.0591872, 1.0934594] 2018-05-25 10:39:27.801377\n",
      "2400 2.1477668 [2.0165138, 0.9972517, 1.0192622] 2018-05-25 10:39:51.325983\n",
      "2500 2.1855462 [1.9139868, 0.953269, 0.9607178] 2018-05-25 10:40:15.084867\n",
      "2600 2.1753542 [2.127968, 1.0624433, 1.0655248] 2018-05-25 10:40:36.095063\n",
      "2700 2.0839994 [2.1066844, 1.0415249, 1.0651596] 2018-05-25 10:40:56.823962\n",
      "2800 2.1387289 [2.2002616, 1.0778033, 1.1224585] 2018-05-25 10:41:23.215823\n",
      "2900 2.0826802 [2.3656154, 1.1713924, 1.1942228] 2018-05-25 10:41:44.037096\n",
      "3000 2.1284914 [2.1475654, 1.0604782, 1.0870873] 2018-05-25 10:42:05.400233\n",
      "3100 2.0628982 [2.3125386, 1.1467576, 1.165781] 2018-05-25 10:42:28.578058\n",
      "3200 2.0720315 [2.1466112, 1.040366, 1.1062453] 2018-05-25 10:42:49.621374\n",
      "3300 2.0972695 [1.8994964, 0.93611836, 0.9633781] 2018-05-25 10:43:16.593898\n",
      "3400 2.0860043 [2.2130103, 1.010734, 1.2022765] 2018-05-25 10:43:36.937061\n",
      "3500 2.0070531 [1.954044, 0.9657178, 0.9883262] 2018-05-25 10:43:58.586998\n",
      "3600 2.0460892 [1.715579, 0.8506609, 0.8649181] 2018-05-25 10:44:22.836001\n",
      "3700 2.0445273 [1.884465, 0.9116654, 0.97279966] 2018-05-25 10:44:47.400797\n",
      "3800 2.0655215 [2.0170097, 0.9588906, 1.058119] 2018-05-25 10:45:11.466677\n",
      "3900 2.025054 [2.3733025, 1.1743096, 1.1989928] 2018-05-25 10:45:35.124839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-25 10:45:39.892372\n",
      "Trained at 2018-05-25 11:02:23.598721\n",
      "TP 14367 FP 2278 FN 4962 F 0.7987435369989436 Precision 0.8631420847101232 Recall 0.7432872885301878\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-25 11:07:12.228953\n",
      "Trained at 2018-05-25 11:23:44.055641\n",
      "TP 15992 FP 3197 FN 3337 F 0.8303650241445558 Precision 0.833394132054823 Recall 0.8273578560711884\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-25 11:28:38.848064\n",
      "Trained at 2018-05-25 11:45:12.830007\n",
      "TP 16029 FP 2737 FN 3300 F 0.8415277595484971 Precision 0.8541511243738676 Recall 0.8292720782244296\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-25 11:50:08.794504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Dict 2018-05-25 11:58:49.339340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-25 12:06:11.204634\n",
      "TP 16483 FP 3003 FN 2846 F 0.8493108334406801 Precision 0.845889356461049 Recall 0.8527601014020384\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-25 12:11:04.512199\n",
      "Trained at 2018-05-25 12:26:25.191934\n",
      "TP 16586 FP 2904 FN 2743 F 0.8545299981967593 Precision 0.8510005130836327 Recall 0.858088881990791\n",
      "Best so far\n",
      "Epoch 5 start at 2018-05-25 12:31:20.347293\n",
      "Trained at 2018-05-25 12:46:42.819350\n",
      "TP 16940 FP 3103 FN 2389 F 0.8605100071116529 Precision 0.8451828568577558 Recall 0.8764033317812613\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-25 12:51:38.653677\n",
      "Trained at 2018-05-25 13:06:58.782780\n",
      "TP 17193 FP 3713 FN 2136 F 0.8546290543059525 Precision 0.82239548454989 Recall 0.8894924724507217\n",
      "Epoch 7 start at 2018-05-25 13:11:55.525105\n",
      "Trained at 2018-05-25 13:27:16.457791\n",
      "TP 17226 FP 3244 FN 2103 F 0.8656498907007714 Precision 0.84152418172936 Recall 0.8911997516684774\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-25 13:32:15.006903\n",
      "Trained at 2018-05-25 13:47:36.349189\n",
      "TP 17001 FP 3101 FN 2328 F 0.8623164515229135 Precision 0.8457367426126754 Recall 0.8795592115474158\n",
      "Epoch 9 start at 2018-05-25 13:52:34.357400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b25dc62f0451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminimodel_cudnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimodel_cudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMiniModelCuDNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../catted_patabs_a61k31a61_noct_s.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/minimodel_cudnn.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname, unsupfile, nunsup, unsupfile2)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trained at\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch_%s_%s.h5\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 128/64/64, cudnngru instead of cudnnlstm\n",
    "from chemlistem import minimodel_cudnn\n",
    "nmm = minimodel_cudnn.MiniModelCuDNN()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patabs_a61k31a61_noct_s.txt\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus at 2018-05-25 13:53:56.963014\n",
      "Shufflesplit at 2018-05-25 13:53:56.963131\n",
      "Read annots at 2018-05-25 13:53:57.030353\n",
      "Read train seqs at 2018-05-25 13:53:57.311792\n",
      "Read test seqs at 2018-05-25 13:54:40.479246\n",
      "Corpus read at 2018-05-25 13:54:53.086087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make train dict at 2018-05-25 13:54:57.336720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "3931 batches of unsupervised\n",
      "0 9.026678 [9.026678, 4.512102, 4.514576] 2018-05-25 13:55:09.307551\n",
      "100 5.8401628 [4.7881055, 2.3714156, 2.4166899] 2018-05-25 13:55:28.136138\n",
      "200 4.5747256 [4.1630206, 2.0732234, 2.0897973] 2018-05-25 13:55:45.091403\n",
      "300 4.032432 [3.3247042, 1.6547678, 1.6699364] 2018-05-25 13:56:06.667280\n",
      "400 3.5938506 [3.983118, 1.9790188, 2.0040994] 2018-05-25 13:56:29.548312\n",
      "500 3.2998137 [3.7003784, 1.8340065, 1.8663719] 2018-05-25 13:56:54.648436\n",
      "600 3.0287213 [3.0540977, 1.5090694, 1.5450281] 2018-05-25 13:57:13.809083\n",
      "700 2.888102 [3.2300813, 1.604351, 1.6257303] 2018-05-25 13:57:35.624636\n",
      "800 2.8128533 [2.5643768, 1.2411094, 1.3232676] 2018-05-25 13:58:01.948261\n",
      "900 2.718237 [2.4859264, 1.2397854, 1.246141] 2018-05-25 13:58:27.710689\n",
      "1000 2.6229208 [2.3958602, 1.1773677, 1.2184924] 2018-05-25 13:58:50.848751\n",
      "1100 2.533539 [3.1842794, 1.5802712, 1.6040081] 2018-05-25 13:59:13.028415\n",
      "1200 2.5008187 [3.2140055, 1.6024466, 1.6115589] 2018-05-25 13:59:35.445450\n",
      "1300 2.4660773 [2.111806, 1.0244522, 1.0873536] 2018-05-25 14:00:01.026711\n",
      "1400 2.4327438 [2.3709965, 1.1801399, 1.1908565] 2018-05-25 14:00:21.149072\n",
      "1500 2.3538003 [2.2194586, 1.1035192, 1.1159395] 2018-05-25 14:00:41.831663\n",
      "1600 2.3750577 [2.8262908, 1.4078491, 1.4184418] 2018-05-25 14:01:07.464216\n",
      "1700 2.2931151 [2.6755304, 1.3338048, 1.3417256] 2018-05-25 14:01:26.064504\n",
      "1800 2.2765539 [1.9673527, 0.9707491, 0.99660367] 2018-05-25 14:01:46.151323\n",
      "1900 2.254572 [3.002591, 1.4955671, 1.5070238] 2018-05-25 14:02:08.389340\n",
      "2000 2.2343783 [1.711051, 0.8375039, 0.8735471] 2018-05-25 14:02:31.184790\n",
      "2100 2.2135804 [2.2654076, 1.13454, 1.1308675] 2018-05-25 14:02:53.710111\n",
      "2200 2.1317513 [2.9827178, 1.4865289, 1.4961889] 2018-05-25 14:03:12.258842\n",
      "2300 2.1516848 [2.1385283, 1.059747, 1.0787815] 2018-05-25 14:03:32.995700\n",
      "2400 2.1446848 [1.9928541, 0.99716705, 0.9956871] 2018-05-25 14:03:55.590412\n",
      "2500 2.1829684 [1.9003828, 0.9461725, 0.9542103] 2018-05-25 14:04:20.056875\n",
      "2600 2.1739044 [2.141588, 1.0725973, 1.0689907] 2018-05-25 14:04:40.854008\n",
      "2700 2.0801964 [2.0863254, 1.0351765, 1.0511489] 2018-05-25 14:05:00.907084\n",
      "2800 2.1356347 [2.1858048, 1.0891211, 1.0966837] 2018-05-25 14:05:26.595359\n",
      "2900 2.0815797 [2.3616788, 1.1724334, 1.1892455] 2018-05-25 14:05:47.330218\n",
      "3000 2.1242313 [2.130454, 1.03588, 1.094574] 2018-05-25 14:06:08.760307\n",
      "3100 2.0627656 [2.308253, 1.1515925, 1.1566606] 2018-05-25 14:06:32.841575\n",
      "3200 2.0697155 [2.1588705, 1.056665, 1.1022055] 2018-05-25 14:06:54.062476\n",
      "3300 2.0957005 [1.8894558, 0.94576144, 0.94369435] 2018-05-25 14:07:20.109473\n",
      "3400 2.0805285 [2.2013087, 1.0256405, 1.1756682] 2018-05-25 14:07:40.474729\n",
      "3500 2.0048916 [1.9515457, 0.9706151, 0.9809306] 2018-05-25 14:08:02.744694\n",
      "3600 2.0479267 [1.7166059, 0.86155415, 0.8550517] 2018-05-25 14:08:26.938941\n",
      "3700 2.0405164 [1.882355, 0.93083704, 0.95151794] 2018-05-25 14:08:51.493478\n",
      "3800 2.0627851 [2.03342, 0.99726546, 1.0361546] 2018-05-25 14:09:15.552841\n",
      "3900 2.0234137 [2.3833723, 1.1896473, 1.1937249] 2018-05-25 14:09:38.306402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at 2018-05-25 14:09:43.863499\n",
      "Trained at 2018-05-25 14:27:06.407393\n",
      "TP 13916 FP 2052 FN 5413 F 0.7885089384366943 Precision 0.8714929859719439 Recall 0.7199544725541932\n",
      "Best so far\n",
      "Epoch 1 start at 2018-05-25 14:31:53.559894\n",
      "Trained at 2018-05-25 14:49:01.750978\n",
      "TP 14726 FP 2205 FN 4603 F 0.8122448979591836 Precision 0.8697655188707105 Recall 0.761860416990015\n",
      "Best so far\n",
      "Epoch 2 start at 2018-05-25 14:53:47.076947\n",
      "Trained at 2018-05-25 15:10:57.503801\n",
      "TP 15578 FP 2447 FN 3751 F 0.8340739947529047 Precision 0.864244105409154 Recall 0.805939262248435\n",
      "Best so far\n",
      "Epoch 3 start at 2018-05-25 15:15:44.962685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Dict 2018-05-25 15:24:41.101416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trained at 2018-05-25 15:32:24.437138\n",
      "TP 16176 FP 2572 FN 3153 F 0.8496467683903669 Precision 0.8628120332835503 Recall 0.8368772311035232\n",
      "Best so far\n",
      "Epoch 4 start at 2018-05-25 15:37:16.777033\n",
      "Trained at 2018-05-25 15:53:11.300432\n",
      "TP 15941 FP 2673 FN 3388 F 0.8402603905858789 Precision 0.856398409799076 Recall 0.8247193336437477\n",
      "Epoch 5 start at 2018-05-25 15:57:58.226355\n",
      "Trained at 2018-05-25 16:13:52.977852\n",
      "TP 17186 FP 3300 FN 2143 F 0.8632927288710285 Precision 0.8389143805525725 Recall 0.889130322313622\n",
      "Best so far\n",
      "Epoch 6 start at 2018-05-25 16:18:46.483097\n",
      "Trained at 2018-05-25 16:34:40.845652\n",
      "TP 17042 FP 3110 FN 2287 F 0.8633013348192802 Precision 0.8456728860658992 Recall 0.8816803766361426\n",
      "Best so far\n",
      "Epoch 7 start at 2018-05-25 16:39:33.370942\n",
      "Trained at 2018-05-25 16:55:29.596015\n",
      "TP 17151 FP 3090 FN 2178 F 0.866868840030326 Precision 0.8473395583222173 Recall 0.8873195716281236\n",
      "Best so far\n",
      "Epoch 8 start at 2018-05-25 17:00:24.827414\n",
      "Trained at 2018-05-25 17:16:19.321120\n",
      "TP 17102 FP 3050 FN 2227 F 0.8663407715103467 Precision 0.8486502580389044 Recall 0.8847845206684257\n",
      "Epoch 9 start at 2018-05-25 17:21:11.939800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f544d8fa43f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchemlistem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminimodel_cudnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimodel_cudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMiniModelCuDNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../BioCreative V.5 training set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../CEMP_BioCreative V.5 training set annot.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../catted_patabs_a61k31a61_noct_s.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chemlistem/chemlistem/minimodel_cudnn.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, textfile, annotfile, runname, unsupfile, nunsup, unsupfile2)\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0mty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtobits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlablist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;31m# This trains in mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trained at\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch_%s_%s.h5\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 128/64/64, gru/lstm/lstm\n",
    "from chemlistem import minimodel_cudnn\n",
    "nmm = minimodel_cudnn.MiniModelCuDNN()\n",
    "nmm.train(\"../BioCreative V.5 training set.txt\", \"../CEMP_BioCreative V.5 training set annot.tsv\", \"foo\", \"../catted_patabs_a61k31a61_noct_s.txt\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
